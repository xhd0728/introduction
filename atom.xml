<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Haidong Xin</title>
  
  <subtitle>HomePage of Haidong Xin</subtitle>
  <link href="https://resume.kokomi0728.eu.org/atom.xml" rel="self"/>
  
  <link href="https://resume.kokomi0728.eu.org/"/>
  <updated>2023-11-23T05:54:05.532Z</updated>
  <id>https://resume.kokomi0728.eu.org/</id>
  
  <author>
    <name>Haidong Xin</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>字节对编码(BPE)</title>
    <link href="https://resume.kokomi0728.eu.org/posts/e65f4108.html"/>
    <id>https://resume.kokomi0728.eu.org/posts/e65f4108.html</id>
    <published>2023-11-22T16:00:00.000Z</published>
    <updated>2023-11-23T05:54:05.532Z</updated>
    
    <content type="html"><![CDATA[<p>字节对编码（Byte Pair Encoding，BPE）是一种用于数据压缩和文本编码的技术，同时也在自然语言处理（NLP）中广泛应用于分词和子词分割。这种编码方法由Philip Gage在1994年提出，后来由Sennrich等人引入到机器翻译领域。</p><span id="more"></span><h3 id="BPE的基本思想："><a href="#BPE的基本思想：" class="headerlink" title="BPE的基本思想："></a>BPE的基本思想：</h3><p>BPE的主要思想是从输入文本的最小单位开始，通过不断合并出现频率最高的相邻单位，逐渐构建出更大的单位，直到达到预定的词汇大小或达到停止条件。在NLP中，这些单位可以是字符、子词，或者更大的单词。</p><h3 id="BPE算法步骤："><a href="#BPE算法步骤：" class="headerlink" title="BPE算法步骤："></a>BPE算法步骤：</h3><ol><li><p><strong>初始化：</strong> 将输入文本中的每个字符或子词视为一个初始的符号。</p></li><li><p><strong>计算频率：</strong> 统计相邻符号对的频率。</p></li><li><p><strong>合并：</strong> 合并频率最高的相邻符号对，形成一个新的符号，将其添加到符号表中。</p></li><li><p><strong>更新文本：</strong> 在输入文本中，用新的符号替换被合并的相邻符号。</p></li><li><p><strong>重复：</strong> 重复步骤2-4，直到符号表的大小达到预定的词汇大小或者达到停止条件。</p></li></ol><h3 id="例子："><a href="#例子：" class="headerlink" title="例子："></a>例子：</h3><p>假设有以下输入文本：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">aabacabadabacabae</span><br></pre></td></tr></table></figure><ol><li>初始化：</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a, a, b, a, c, a, b, a, d, a, b, a, c, a, b, a, e</span><br></pre></td></tr></table></figure><ol start="2"><li>计算频率：</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(a, a): 6, (a, b): 6, (b, a): 6, (a, c): 4, (c, a): 4, (b, d): 2, (d, a): 2, (d, a): 2, (b, a, c): 2, (a, c, a): 2, (c, a, b): 2, (a, b, a): 2</span><br></pre></td></tr></table></figure><ol start="3"><li>合并：</li></ol><p>合并频率最高的相邻符号对 (a, b)：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a, ab, a, c, a, ab, a, d, a, ab, a, c, a, ab, a, e</span><br></pre></td></tr></table></figure><ol start="4"><li>重复：</li></ol><p>继续迭代，重复计算频率、合并的步骤，直到达到预定的词汇大小。</p><p>BPE最终生成的合并的符号可以视为子词，用于表示文本中的重要词汇和模式。在NLP任务中，这种子词分割方法有助于解决未登录词（Out-of-Vocabulary，OOV）问题，同时提高对复杂词汇和短语的建模能力。 BPE被广泛用于机器翻译、文本生成等领域。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;字节对编码（Byte Pair Encoding，BPE）是一种用于数据压缩和文本编码的技术，同时也在自然语言处理（NLP）中广泛应用于分词和子词分割。这种编码方法由Philip Gage在1994年提出，后来由Sennrich等人引入到机器翻译领域。&lt;/p&gt;</summary>
    
    
    
    <category term="论文学习" scheme="https://resume.kokomi0728.eu.org/categories/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="NLP" scheme="https://resume.kokomi0728.eu.org/tags/NLP/"/>
    
    <category term="机器翻译" scheme="https://resume.kokomi0728.eu.org/tags/%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/"/>
    
  </entry>
  
  <entry>
    <title>Coreference Resolution</title>
    <link href="https://resume.kokomi0728.eu.org/posts/f0bcdccd.html"/>
    <id>https://resume.kokomi0728.eu.org/posts/f0bcdccd.html</id>
    <published>2023-11-20T16:00:00.000Z</published>
    <updated>2023-11-21T04:50:46.519Z</updated>
    
    <content type="html"><![CDATA[<p>Coreference Resolution（指代消解）是自然语言处理（NLP）领域的一个任务，其目标是识别一段文本中的代词（如”he”、”she”、”it”）或名词短语是否指代同一实体。这是一个重要的语言理解任务，因为它涉及到理解文本中实体之间的关系，使得计算机可以更好地理解上下文，并推断谁在说什么、指代什么。</p><span id="more"></span><p>考虑以下的例子：</p><p>“John went to the store. He bought a new laptop.”</p><p>在这个例子中，”He” 指代的是 “John”，这种关系就是一个指代关系。Coreference Resolution 的任务就是在文本中找到这种关系，将 “He” 和 “John” 连接起来。</p><p>Coreference Resolution 面临的挑战包括：</p><ol><li><strong>代词消解：</strong> 识别文本中的代词，并确定它们指代的实体。</li><li><strong>命名实体消解：</strong> 识别文本中的命名实体（如人名、地名）是否指代同一实体。</li><li><strong>解决跨句指代：</strong> 处理文本中跨句的指代关系，即一个实体在文本的不同句子中出现。</li></ol><p>Coreference Resolution 的应用广泛，包括文本理解、问答系统、机器翻译等。在这些任务中，正确处理代词消解和实体指代关系可以提高系统对文本的理解和生成的质量。</p><p>一些先进的NLP模型，尤其是基于深度学习的模型，已经在 Coreference Resolution 任务上取得了一些进展。这包括使用注意力机制、神经网络和大规模预训练模型等技术。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;Coreference Resolution（指代消解）是自然语言处理（NLP）领域的一个任务，其目标是识别一段文本中的代词（如”he”、”she”、”it”）或名词短语是否指代同一实体。这是一个重要的语言理解任务，因为它涉及到理解文本中实体之间的关系，使得计算机可以更好地理解上下文，并推断谁在说什么、指代什么。&lt;/p&gt;</summary>
    
    
    
    <category term="论文学习" scheme="https://resume.kokomi0728.eu.org/categories/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="NLP" scheme="https://resume.kokomi0728.eu.org/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Masked Language Modeling</title>
    <link href="https://resume.kokomi0728.eu.org/posts/c581aeed.html"/>
    <id>https://resume.kokomi0728.eu.org/posts/c581aeed.html</id>
    <published>2023-11-20T16:00:00.000Z</published>
    <updated>2023-11-21T05:40:42.495Z</updated>
    
    <content type="html"><![CDATA[<p>Masked Language Modeling（MLM，遮蔽语言建模）是一种自然语言处理（NLP）中的预训练任务，其主要目标是通过在输入文本中遮蔽（掩盖）一些单词并尝试预测这些遮蔽单词的身份来学习词汇和语言表示。这一任务通常在Transformer等深度学习模型上进行。</p><span id="more"></span><p>以下是Masked Language Modeling 的基本步骤：</p><ol><li><p><strong>遮蔽输入文本：</strong> 在输入文本中，随机选择一些单词，并将它们用特殊的标记（通常是”[MASK]”标记）替换。这使得模型在训练时需要预测这些被遮蔽的单词。</p></li><li><p><strong>模型预测：</strong> 将经过遮蔽的文本输入模型，并让模型预测被遮蔽的位置上的单词。在Transformer模型中，这通常涉及到在每个位置上进行softmax分类，使得模型预测每个词汇表中的单词的概率。</p></li><li><p><strong>计算损失：</strong> 通过比较模型的预测和真实的被遮蔽单词，计算一个损失值。损失函数通常是交叉熵损失。</p></li></ol><p>通过这个过程，模型被迫学习对上下文的理解，因为它需要根据上下文来预测被遮蔽的单词。这种预训练任务可以为后续特定任务（如文本分类、命名实体识别等）提供有用的语言表示。</p><p>BERT（Bidirectional Encoder Representations from Transformers）是一个采用MLM作为预训练任务的例子。BERT通过预测被遮蔽的单词，以及利用双向上下文信息，获得了在多种NLP任务上出色表现的通用语言表示。其他一些模型，如GPT-2（Generative Pre-trained Transformer 2）也使用了类似的预训练任务，尽管采用的是单向语言模型。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;Masked Language Modeling（MLM，遮蔽语言建模）是一种自然语言处理（NLP）中的预训练任务，其主要目标是通过在输入文本中遮蔽（掩盖）一些单词并尝试预测这些遮蔽单词的身份来学习词汇和语言表示。这一任务通常在Transformer等深度学习模型上进行。&lt;/p&gt;</summary>
    
    
    
    <category term="论文学习" scheme="https://resume.kokomi0728.eu.org/categories/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="NLP" scheme="https://resume.kokomi0728.eu.org/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>预热</title>
    <link href="https://resume.kokomi0728.eu.org/posts/fc31d48b.html"/>
    <id>https://resume.kokomi0728.eu.org/posts/fc31d48b.html</id>
    <published>2023-11-20T16:00:00.000Z</published>
    <updated>2023-11-21T04:41:06.390Z</updated>
    
    <content type="html"><![CDATA[<p>在深度学习中，”预热”（warm-up）通常指的是在训练的初期阶段逐渐增加学习率或其他训练参数的过程。这旨在帮助模型更好地适应数据，减少训练初期的不稳定性，并提高模型的性能。预热在训练神经网络时是一种常见的优化策略，具有以下好处：</p><span id="more"></span><ol><li><p><strong>稳定训练初期：</strong> 在训练的开始阶段，模型的权重可能处于一个相对不稳定的状态。通过逐渐增加学习率，模型可以更缓慢地更新权重，避免因为过大的梯度导致的不稳定性。</p></li><li><p><strong>避免陷入局部最优解：</strong> 如果学习率过大，模型可能会跳过或振荡在全局最优解附近，无法精确找到最优解。通过预热，可以减小学习率，使模型更容易收敛到合适的区域，然后再逐渐增加学习率。</p></li><li><p><strong>提高泛化性能：</strong> 预热可以帮助模型更好地捕捉数据的特征，从而提高在未见过的数据上的泛化性能。</p></li><li><p><strong>减小梯度爆炸和梯度消失问题：</strong> 在训练初期，由于初始权重和梯度的原因，可能出现梯度爆炸或梯度消失的问题。通过逐渐增加学习率，可以缓解这些问题。</p></li><li><p><strong>更好的权重初始化：</strong> 逐渐增加学习率可以帮助模型更好地选择适当的权重初始化，提高训练效果。</p></li><li><p><strong>更快的收敛：</strong> 预热可以使模型更快地收敛到一个相对较好的解，从而缩短整体训练时间。</p></li></ol><p>总体来说，预热是一种通过逐渐升高学习率来引导模型进入训练状态的策略，有助于训练过程更加平滑、稳定，提高模型性能。然而，具体的预热策略可能取决于任务、模型架构和数据集的特性。</p><p>在训练神经网络时，学习率的调度策略对于模型的性能和收敛速度至关重要。先预热后线性减小学习率和直接从最大学习率线性减小两种方式都有其优缺点，而更好的方式可能取决于具体的任务和数据。</p><h3 id="先预热，再线性减小学习率："><a href="#先预热，再线性减小学习率：" class="headerlink" title="先预热，再线性减小学习率："></a>先预热，再线性减小学习率：</h3><p><strong>优点：</strong></p><ol><li><strong>更稳定的训练初期：</strong> 预热阶段可以帮助模型更好地适应数据，减少训练初期的不稳定性。</li><li><strong>更好的泛化性能：</strong> 通过预热，模型可能更容易收敛到一个良好的初始点，有助于提高泛化性能。</li></ol><p><strong>缺点：</strong></p><ol><li><strong>训练时间增加：</strong> 预热阶段会增加整体训练时间。</li><li><strong>对预热参数敏感：</strong> 预热的效果可能对预热时选择的参数（如预热步数、预热学习率等）敏感。</li></ol><h3 id="直接从最大学习率线性减小："><a href="#直接从最大学习率线性减小：" class="headerlink" title="直接从最大学习率线性减小："></a>直接从最大学习率线性减小：</h3><p><strong>优点：</strong></p><ol><li><strong>更高的训练效率：</strong> 直接从最大学习率开始训练，节省了预热的时间，可能更高效。</li><li><strong>更少的超参数：</strong> 不需要设置预热相关的超参数。</li></ol><p><strong>缺点：</strong></p><ol><li><strong>训练初期不稳定：</strong> 直接从较大的学习率开始训练，可能导致训练初期的不稳定性。</li><li><strong>泛化性能可能较差：</strong> 初始时较大的学习率可能使得模型难以找到稳定的优化路径，影响泛化性能。</li></ol><h3 id="更好的训练方式："><a href="#更好的训练方式：" class="headerlink" title="更好的训练方式："></a>更好的训练方式：</h3><p>目前，没有一种单一的训练方式适用于所有情况，最佳的训练方式可能取决于具体的任务、数据和模型。一些其他可能的学习率调度策略包括余弦退火、学习率多项式衰减等。调整学习率调度策略时，可以通过验证集性能进行实验和调试，以找到最适合特定任务的策略。</p><p>在实践中，还有一些先进的学习率调度算法，如使用学习率预测器（learning rate schedulers with warmup and cooldown phases）或根据训练曲线动态调整学习率。这些方法可以更灵活地适应不同的训练阶段和数据特性。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;在深度学习中，”预热”（warm-up）通常指的是在训练的初期阶段逐渐增加学习率或其他训练参数的过程。这旨在帮助模型更好地适应数据，减少训练初期的不稳定性，并提高模型的性能。预热在训练神经网络时是一种常见的优化策略，具有以下好处：&lt;/p&gt;</summary>
    
    
    
    <category term="论文学习" scheme="https://resume.kokomi0728.eu.org/categories/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="机器学习" scheme="https://resume.kokomi0728.eu.org/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>GeLU</title>
    <link href="https://resume.kokomi0728.eu.org/posts/2e561321.html"/>
    <id>https://resume.kokomi0728.eu.org/posts/2e561321.html</id>
    <published>2023-11-19T16:00:00.000Z</published>
    <updated>2023-11-20T15:10:52.428Z</updated>
    
    <content type="html"><![CDATA[<p>GeLU（Gaussian Error Linear Unit）是一种激活函数，通常用于神经网络的隐藏层。它被设计用来克服一些传统激活函数（如ReLU）的一些限制，尤其是对于包含负值的输入数据。</p><span id="more"></span><p>GeLU的定义是：</p><p>$\text{GeLU}(x) &#x3D; 0.5x \left(1 + \tanh\left(\sqrt{\frac{2}{\pi}}\left(x + 0.044715x^3\right)\right)\right)$</p><p>其中，$\tanh$ 是双曲正切函数。</p><p>GeLU的性质包括：</p><ol><li><p><strong>平滑性：</strong> GeLU是平滑的函数，这意味着在整个实数域内都是可导的。这与一些其他激活函数，如ReLU，相比是一个优势，因为可导性对于训练过程中的梯度计算是重要的。</p></li><li><p><strong>近似线性：</strong> 当 $x$ 的值远离零点时，GeLU的形状接近于线性，这有助于减缓激活函数的饱和效应，使得网络更容易训练。</p></li><li><p><strong>零均值：</strong> 对于足够大的输入 $x$，GeLU的输出趋近于零均值。这对一些神经网络架构可能是有用的。</p></li></ol><p>GeLU的名字来源于它的形状类似于高斯误差函数（Gaussian Error Function）。GeLU被广泛应用于一些神经网络架构，尤其是在语言模型和自然语言处理任务中，但并非适用于所有情况。在实践中，研究人员会根据任务和模型的特性选择合适的激活函数。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;GeLU（Gaussian Error Linear Unit）是一种激活函数，通常用于神经网络的隐藏层。它被设计用来克服一些传统激活函数（如ReLU）的一些限制，尤其是对于包含负值的输入数据。&lt;/p&gt;</summary>
    
    
    
    <category term="论文学习" scheme="https://resume.kokomi0728.eu.org/categories/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="机器学习" scheme="https://resume.kokomi0728.eu.org/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="数学" scheme="https://resume.kokomi0728.eu.org/tags/%E6%95%B0%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>双序列采样</title>
    <link href="https://resume.kokomi0728.eu.org/posts/b3b84358.html"/>
    <id>https://resume.kokomi0728.eu.org/posts/b3b84358.html</id>
    <published>2023-11-19T16:00:00.000Z</published>
    <updated>2023-11-20T14:37:02.842Z</updated>
    
    <content type="html"><![CDATA[<p>BERT（Bidirectional Encoder Representations from Transformers）模型在预训练时采用了双序列（或双句子）采样的方法，这是为了帮助模型学习句子之间的关系和上下文信息。</p><span id="more"></span><p>在BERT的预训练过程中，模型的输入是一对句子，通常是一个文本中相邻的两个句子。这一对句子会被连接成一个输入序列，并使用特殊的分隔符标记（[SEP]）进行分隔。在原始BERT的预训练任务中，有一个叫做”Next Sentence Prediction”（NSP）的任务，目标是判断两个句子是否是相邻的。</p><p>双序列采样的步骤如下：</p><ol><li><p><strong>选择一篇文档：</strong> 从语料库中选择一篇文档。</p></li><li><p><strong>选择一个句子：</strong> 随机选择文档中的一个句子作为第一个句子（A）。</p></li><li><p><strong>选择下一个句子：</strong> 有50%的概率选择与第一个句子相邻的句子作为第二个句子（B），有50%的概率从语料库中随机选择一个不与第一个句子相邻的句子。</p></li><li><p><strong>构建输入序列：</strong> 将句子A和句子B连接成一个输入序列，并在它们之间插入一个特殊的分隔符标记 [SEP]。</p></li><li><p><strong>添加分类标签：</strong> 在输入序列的开头添加一个特殊的分类标签（[CLS]），用于预测下游任务的输出。</p></li></ol><p>这样，模型在预训练中不仅需要理解单个句子的上下文信息，还需要理解两个句子之间的关系。这有助于提高模型对文本理解的能力，使其能够更好地捕捉句子之间的语义关系。需要注意的是，一些后续的模型，如RoBERTa，已经移除了NSP任务，而专注于更好地优化单个句子的表示。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;BERT（Bidirectional Encoder Representations from Transformers）模型在预训练时采用了双序列（或双句子）采样的方法，这是为了帮助模型学习句子之间的关系和上下文信息。&lt;/p&gt;</summary>
    
    
    
    <category term="论文学习" scheme="https://resume.kokomi0728.eu.org/categories/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="NLP" scheme="https://resume.kokomi0728.eu.org/tags/NLP/"/>
    
    <category term="信息检索" scheme="https://resume.kokomi0728.eu.org/tags/%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2/"/>
    
  </entry>
  
  <entry>
    <title>线性预热</title>
    <link href="https://resume.kokomi0728.eu.org/posts/50e93fc8.html"/>
    <id>https://resume.kokomi0728.eu.org/posts/50e93fc8.html</id>
    <published>2023-11-19T16:00:00.000Z</published>
    <updated>2023-11-20T12:24:30.442Z</updated>
    
    <content type="html"><![CDATA[<p>在AI领域中，线性预热通常指的是在训练神经网络模型时逐渐增加学习率或其他训练参数，以在训练开始阶段更好地引导模型的学习过程。这是一种优化训练过程的策略，可以帮助模型更快地收敛到良好的解决方案。</p><span id="more"></span><p>在神经网络训练中，学习率是一个关键的超参数，它决定了在每次更新模型权重时调整的步长大小。线性预热的想法是，在训练开始时，使用一个相对较小的学习率，然后逐渐增加学习率，使得模型能够更容易地找到全局最优解或更好的局部最优解。</p><p>具体来说，线性预热的过程可以描述为以下步骤：</p><ol><li><p><strong>初始学习率设置：</strong> 初始阶段使用一个较小的学习率，通常是在零附近。</p></li><li><p><strong>逐渐增加学习率：</strong> 在训练的初始几个epoch中，逐渐增加学习率，可以是线性增加的方式，也可以是其他函数形式。这个过程通常在模型开始学习之前的几个epoch中完成。</p></li><li><p><strong>常规训练：</strong> 一旦线性预热阶段完成，模型将以较高的学习率进行正常训练，这有助于更快地调整权重并学习模型的有效表示。</p></li></ol><p>线性预热的优势在于它可以帮助模型在训练的早期阶段更好地适应数据，防止由于初始的不稳定性而导致的训练困难。这种策略在许多深度学习任务中都有助于加速模型的收敛并提高最终性能。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;在AI领域中，线性预热通常指的是在训练神经网络模型时逐渐增加学习率或其他训练参数，以在训练开始阶段更好地引导模型的学习过程。这是一种优化训练过程的策略，可以帮助模型更快地收敛到良好的解决方案。&lt;/p&gt;</summary>
    
    
    
    <category term="论文学习" scheme="https://resume.kokomi0728.eu.org/categories/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="机器学习" scheme="https://resume.kokomi0728.eu.org/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>使用Vscode调试Linux内核</title>
    <link href="https://resume.kokomi0728.eu.org/posts/c45db453.html"/>
    <id>https://resume.kokomi0728.eu.org/posts/c45db453.html</id>
    <published>2023-11-18T16:00:00.000Z</published>
    <updated>2023-11-23T14:06:53.919Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本文转载来源: <a href="https://wfloveiu.github.io/2023/11/20/Vscode%E8%B0%83%E8%AF%95Linux%E5%86%85%E6%A0%B8/">使用Vscode调试Linux内核</a></p></blockquote><span id="more"></span><p>上一篇博客我们在虚拟机中编译了Linux内核，并且使用Qemu和gdb进行调试，但是gdb的指令我还不熟练，还是想用vscode来调试，这样也更加方便</p><h2 id="Vscode插件安装"><a href="#Vscode插件安装" class="headerlink" title="Vscode插件安装"></a>Vscode插件安装</h2><h3 id="remote-ssh"><a href="#remote-ssh" class="headerlink" title="remote-ssh"></a>remote-ssh</h3><p><img src="https://raw.githubusercontent.com/wfloveiu/blogImage/main/img/202311201245007.png" alt="image-20231120124509960"></p><p>安装完成后右边工具栏会多出一个功能</p><p><img src="https://raw.githubusercontent.com/wfloveiu/blogImage/main/img/202311201246229.png" alt="image-20231120124653194"></p><p>按F1呼出对话框，输入<code>remote-ssh</code>，选择open ssh configuration file。</p><p><img src="https://raw.githubusercontent.com/wfloveiu/blogImage/main/img/202311201247886.png" alt="image-20231120124748850"></p><p>选择第一个配置文件</p><p><img src="https://raw.githubusercontent.com/wfloveiu/blogImage/main/img/202311201248231.png" alt="image-20231120124824202"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Host 自定义连接名称</span><br><span class="line">    HostName 服务器IP地址</span><br><span class="line">    User 用户名</span><br></pre></td></tr></table></figure><h3 id="C-C"><a href="#C-C" class="headerlink" title="C&#x2F;C++"></a>C&#x2F;C++</h3><p>安装C&#x2F;C++插件</p><p><img src="https://raw.githubusercontent.com/wfloveiu/blogImage/main/img/202311201251048.png" alt="image-20231120125110014"></p><p>依次点击【运行】-&gt;【打开配置】，将以下配置复制到launch.json中。</p><p><strong>该代码不需要更改，直接粘贴</strong></p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="comment">// 使用 IntelliSense 了解相关属性。 </span></span><br><span class="line">    <span class="comment">// 悬停以查看现有属性的描述。</span></span><br><span class="line">    <span class="comment">// 欲了解更多信息，请访问: https://go.microsoft.com/fwlink/?linkid=830387</span></span><br><span class="line">    <span class="attr">&quot;version&quot;</span><span class="punctuation">:</span> <span class="string">&quot;0.2.0&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;configurations&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">        <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;kernel-debug&quot;</span><span class="punctuation">,</span>   <span class="comment">//随便起名</span></span><br><span class="line">            <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;cppdbg&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;request&quot;</span><span class="punctuation">:</span> <span class="string">&quot;launch&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;miDebuggerServerAddress&quot;</span><span class="punctuation">:</span> <span class="string">&quot;127.0.0.1:1234&quot;</span><span class="punctuation">,</span>  <span class="comment">//远端调试地址，1234为qemu的监视端口</span></span><br><span class="line">            <span class="attr">&quot;program&quot;</span><span class="punctuation">:</span> <span class="string">&quot;$&#123;workspaceFolder&#125;/vmlinux&quot;</span><span class="punctuation">,</span>     <span class="comment">//当前目录下的vmlinux</span></span><br><span class="line">            <span class="attr">&quot;args&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;stopAtEntry&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">false</span></span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;cwd&quot;</span><span class="punctuation">:</span> <span class="string">&quot;$&#123;workspaceFolder&#125;&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;environment&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;externalConsole&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">false</span></span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;logging&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;engineLogging&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">false</span></span></span><br><span class="line">            <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;MIMode&quot;</span><span class="punctuation">:</span> <span class="string">&quot;gdb&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><h2 id="Vscode调试"><a href="#Vscode调试" class="headerlink" title="Vscode调试"></a>Vscode调试</h2><h3 id="在虚拟机中启动qemu"><a href="#在虚拟机中启动qemu" class="headerlink" title="在虚拟机中启动qemu"></a>在虚拟机中启动qemu</h3><p>在<strong>Linux内核文件夹下</strong>运行此命令</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">qemu-system-x86_64 -kernel ./arch/x86/boot/bzImage -initrd ../initramfs.cpio.gz -append <span class="string">&quot;nokaslr console=ttyS0&quot;</span> -s -S -nographic</span><br></pre></td></tr></table></figure><h3 id="打断点"><a href="#打断点" class="headerlink" title="打断点"></a>打断点</h3><p>打开init&#x2F;main.c，我打了如下的断点</p><p><img src="https://raw.githubusercontent.com/wfloveiu/blogImage/main/img/202311201300424.png" alt="image-20231120130037370"></p><h3 id="调试"><a href="#调试" class="headerlink" title="调试"></a>调试</h3><p><img src="https://raw.githubusercontent.com/wfloveiu/blogImage/main/img/202311201305050.png" alt="image-20231120130522009"></p><p>然后在vscode中就可以看到调试结果了</p><h2 id="代码中标红的问题"><a href="#代码中标红的问题" class="headerlink" title="代码中标红的问题"></a>代码中标红的问题</h2><p>代码标红是缺少compile_commands.json文件</p><p>我在B站上学习的时候，是跟着这位up主来的，我的解决方案如下：</p><p>在终端键入命令</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./scripts/clang-tools/gen_compile_commands.py</span><br></pre></td></tr></table></figure><p>在源码目录下就生成了<code>compile_commands.json</code>文件</p><p>在vscode中：<code>ctrl+shipt+p</code>选择C&#x2F;C++：Edit Coonfigurations,</p><p>在c_cpp_properties.json</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;configurations&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">        <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Linux&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;includePath&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                <span class="string">&quot;$&#123;workspaceFolder&#125;/**&quot;</span></span><br><span class="line">            <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;defines&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;compilerPath&quot;</span><span class="punctuation">:</span> <span class="string">&quot;/usr/bin/gcc&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;cStandard&quot;</span><span class="punctuation">:</span> <span class="string">&quot;c17&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;cppStandard&quot;</span><span class="punctuation">:</span> <span class="string">&quot;gnu++17&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;intelliSenseMode&quot;</span><span class="punctuation">:</span> <span class="string">&quot;linux-gcc-x64&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;compileCommands&quot;</span><span class="punctuation">:</span> <span class="string">&quot;$&#123;workspaceFolder&#125;/compile_commands.json&quot;</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;version&quot;</span><span class="punctuation">:</span> <span class="number">4</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><p>此后，main.c中的代码就不标红了</p>]]></content>
    
    
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;本文转载来源: &lt;a href=&quot;https://wfloveiu.github.io/2023/11/20/Vscode%E8%B0%83%E8%AF%95Linux%E5%86%85%E6%A0%B8/&quot;&gt;使用Vscode调试Linux内核&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="技术学习" scheme="https://resume.kokomi0728.eu.org/categories/%E6%8A%80%E6%9C%AF%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="Linux" scheme="https://resume.kokomi0728.eu.org/tags/Linux/"/>
    
    <category term="C/C++" scheme="https://resume.kokomi0728.eu.org/tags/C-C/"/>
    
  </entry>
  
  <entry>
    <title>使用虚拟机进行基于qemu和gdb的Linux内核调试</title>
    <link href="https://resume.kokomi0728.eu.org/posts/6c03d805.html"/>
    <id>https://resume.kokomi0728.eu.org/posts/6c03d805.html</id>
    <published>2023-11-17T16:00:00.000Z</published>
    <updated>2023-11-23T14:06:43.991Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本文转载来源: <a href="https://wfloveiu.github.io/2023/11/20/%E9%85%8D%E7%BD%AELinux%E5%86%85%E6%A0%B8%E7%BC%96%E8%AF%911/">使用虚拟机进行基于qemu和gdb的Linux内核调试</a></p></blockquote><span id="more"></span><h2 id="虚拟机配置"><a href="#虚拟机配置" class="headerlink" title="虚拟机配置"></a>虚拟机配置</h2><p>至少分8个核心（不然编译速度很慢，亲测）</p><p>磁盘大小分50G（编译后的内核大小有20多个G！）</p><h2 id="打开SSH"><a href="#打开SSH" class="headerlink" title="打开SSH"></a>打开SSH</h2><p>虚拟机中安装的是ubuntu22.04版本，默认没有安装和启用SSH服务</p><p><strong>更新软件源</strong></p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt update &amp;&amp; sudo apt upgrade -y</span><br></pre></td></tr></table></figure><p><strong>安装SSH(OpenSSH)</strong></p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install openssh-server -y</span><br></pre></td></tr></table></figure><p><strong>启动SSH服务</strong></p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo systemctl <span class="built_in">enable</span> --now ssh</span><br></pre></td></tr></table></figure><p><strong>检查是否启动成功</strong></p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo systemctl status ssh</span><br></pre></td></tr></table></figure><h2 id="内核编译"><a href="#内核编译" class="headerlink" title="内核编译"></a>内核编译</h2><h3 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安装相关依赖</span></span><br><span class="line">sudo apt-get install libncurses5-dev libssl-dev bison flex libelf-dev gcc g++ make openssl libc6-dev</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装gdb，这里使用apt安装（多次尝试的结果）</span></span><br><span class="line">sudo apt-get install gdb</span><br><span class="line">gdb --version <span class="comment"># gdb版本为12.1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#这里选择清华源，国内速度会快很多</span></span><br><span class="line">wget https://mirrors.tuna.tsinghua.edu.cn/kernel/v5.x/linux-5.14.tar.gz</span><br><span class="line"><span class="comment"># 解压</span></span><br><span class="line">tar -xvf linux-5.14.tar.gz</span><br><span class="line"></span><br><span class="line"><span class="comment">#配置编译选项</span></span><br><span class="line">make menuconfig</span><br></pre></td></tr></table></figure><p>然后会在此文件夹下生成 <strong>.&#x2F;config</strong>文件</p><p><strong>进入该文件，并做以下2处修改</strong></p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim ./.config</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/wfloveiu/blogImage/main/img/202311201128412.png" alt="image-20231117212013850"></p><p><strong>安装dwarves软件包（编译报错得出结论）</strong></p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install dwarves</span><br></pre></td></tr></table></figure><p><strong>BTF报错解决</strong></p><p>如果仅仅只进行了上边的配置，会报如下错误（我个人是这样）</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">  KSYMS   .tmp_vmlinux.kallsyms1.S</span><br><span class="line">  AS      .tmp_vmlinux.kallsyms1.S</span><br><span class="line">  LD      .tmp_vmlinux.kallsyms2</span><br><span class="line">  KSYMS   .tmp_vmlinux.kallsyms2.S</span><br><span class="line">  AS      .tmp_vmlinux.kallsyms2.S</span><br><span class="line">  LD      vmlinux</span><br><span class="line">  BTFIDS  vmlinux</span><br><span class="line">FAILED: load BTF from vmlinux: Invalid argument</span><br><span class="line">make: *** [Makefile:1187: vmlinux] Error 255</span><br><span class="line">make: *** Deleting file <span class="string">&#x27;vmlinux&#x27;</span></span><br></pre></td></tr></table></figure><p>查阅资料后，有三种解决方案：<a href="https://devkernel.io/posts/pahole-error/">https://devkernel.io/posts/pahole-error/</a></p><p>我使用的是<strong>第二种方法</strong>，对pahole软件包进行降级 :</p><p>查看pahole的版本，是1.25</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pahole --version  </span><br></pre></td></tr></table></figure><p>查看pahole的所有可用安装版本</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-cache policy pahole</span><br></pre></td></tr></table></figure><p>截图如下</p><p><img src="https://raw.githubusercontent.com/wfloveiu/blogImage/main/img/202311201127209.png" alt="1"></p><p>我们发现只有两个版本，因此只能降级为 1.22-8</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install pahole=1.22-8</span><br></pre></td></tr></table></figure><h3 id="编译"><a href="#编译" class="headerlink" title="编译"></a>编译</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">make -j8      <span class="comment">#8个线程并行编译，</span></span><br></pre></td></tr></table></figure><p>然后可能会弹出一个选择（1，2，3），直接选择1即可。等待一段时间，30分钟左右</p><h3 id="是否成功"><a href="#是否成功" class="headerlink" title="是否成功"></a>是否成功</h3><p>编译完成后，目录下会生成以下,那么就编译成功了</p><blockquote><p>.&#x2F;vmLinux</p><p>.&#x2F;arch&#x2F;x86&#x2F;boot&#x2F;bzImage</p><p>其中vmLinux为GDB所需的调试Map文件，bzImage为大内核文件</p></blockquote><h2 id="安装Qemu"><a href="#安装Qemu" class="headerlink" title="安装Qemu"></a>安装Qemu</h2><p>qemu是一款完全软件模拟(Binary translation)的虚拟化软件，在虚拟化的实现中性能相对较差。但利用它在测试环境中gdb调试Linux内核代码，是熟悉Linux内核代码的一个好方法。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#安装qemu</span></span><br><span class="line">sudo apt-get install qemu</span><br></pre></td></tr></table></figure><h2 id="安装编译busybox"><a href="#安装编译busybox" class="headerlink" title="安装编译busybox"></a>安装编译busybox</h2><p>安装busybox的目的是：借助BusyBox构建极简initramfs，提供基本的用户态可执行程序。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">wget https://busybox.net/downloads/busybox-1.36.1.tar.bz2  <span class="comment"># 去官网找最新版</span></span><br><span class="line">tar -xvf busybox-1.36.1.tar.bz2</span><br><span class="line"><span class="built_in">cd</span> busybox-1.36.1/</span><br><span class="line">make menuconfig</span><br></pre></td></tr></table></figure><p>在编译busybox之前，我们需要对其进行设置，执行<code>make menuconfig</code>，如下</p><p><img src="https://raw.githubusercontent.com/wfloveiu/blogImage/main/img/202311201135953.png" alt="image-20231120113536905"></p><p><img src="https://raw.githubusercontent.com/wfloveiu/blogImage/main/img/202311201136740.png" alt="image-20231120113608696"></p><p>这里一定要选择<strong>静态编译</strong>，编译好的可执行文件<code>busybox</code>不依赖动态链接库，可以独立运行，方便构建initramfs。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">make -j 8</span><br><span class="line">make &amp;&amp; make install <span class="comment"># 安装完成后生成的相关文件会在 _install 目录下</span></span><br></pre></td></tr></table></figure><h2 id="构建initramfs根文件系统"><a href="#构建initramfs根文件系统" class="headerlink" title="构建initramfs根文件系统"></a>构建initramfs根文件系统</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在busybox压缩包的下载目录下,创建的该文件夹，该文件夹下有</span></span><br><span class="line"><span class="comment"># wufang@wufang:~/linux_kernel/kernel_compile$ ls</span></span><br><span class="line"><span class="comment"># busybox-1.36.1   busybox-1.36.1.tar.bz2   linux-5.14  linux-5.14.tar.gz</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">mkdir</span> initramfs</span><br><span class="line"><span class="built_in">cd</span> initramfs</span><br><span class="line"><span class="built_in">cp</span> ../busybox-1.29.0/_install/* -rf ./ <span class="comment">#将_install文件夹下的所有文件复制到initramfs文件夹下</span></span><br><span class="line"><span class="built_in">mkdir</span> dev proc sys</span><br><span class="line">sudo <span class="built_in">cp</span> -a /dev/&#123;null,console,<span class="built_in">tty</span>,tty1,tty2,tty3,tty4&#125; dev/</span><br><span class="line"><span class="built_in">rm</span> -f linuxrc</span><br><span class="line">vim init</span><br></pre></td></tr></table></figure><p>添加如下代码</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/busybox sh</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;&#123;==DBG==&#125; INIT SCRIPT&quot;</span></span><br><span class="line">mount -t proc none /proc</span><br><span class="line">mount -t sysfs none /sys</span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> -e <span class="string">&quot;&#123;==DBG==&#125; Boot took <span class="subst">$(cut -d&#x27; &#x27; -f1 /proc/uptime)</span> seconds&quot;</span></span><br><span class="line"><span class="built_in">exec</span> /sbin/init</span><br></pre></td></tr></table></figure><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">chmod</span> a+x init 修改文件权限</span><br><span class="line"><span class="comment"># 完成后，initrams下有如下文件</span></span><br><span class="line"><span class="comment"># bin   dev   init  proc  sbin  sys   usr</span></span><br></pre></td></tr></table></figure><p><strong>打包initramfs</strong></p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">find . -print0 | cpio --null -ov --format=newc | gzip -9 &gt; ../initramfs.cpio.gz</span><br><span class="line"><span class="comment"># 此时在busybox压缩包的下载目录下，有如下文件</span></span><br><span class="line"><span class="comment"># busybox-1.36.1          initramfs               linux-5.14</span></span><br><span class="line"><span class="comment"># busybox-1.36.1.tar.bz2  initramfs.cpio.gz       linux-5.14.tar.gz</span></span><br></pre></td></tr></table></figure><h2 id="启动Qemu调试内核"><a href="#启动Qemu调试内核" class="headerlink" title="启动Qemu调试内核"></a>启动Qemu调试内核</h2><p>上述完成之后，就可以启动Qemu来调试内核了,启动代码如下（是一个指令）</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">qemu-system-x86_64 -kernel ./arch/x86/boot/bzImage -initrd ../initramfs.cpio.gz -append <span class="string">&quot;nokaslr console=ttyS0&quot;</span> -s -S -nographic</span><br></pre></td></tr></table></figure><ul><li><code>qemu-system-x86_64</code>：指定是x86，64位;</li><li><code>-kernel ./arch/x86/boot/bzImage</code>：指定启用的内核镜像；</li><li><code>-initrd ../initramfs.cpio.gz</code>：指定启动的内存文件系统</li><li><code>-append &quot;nokaslr console=ttyS0&quot;</code> ：附加参数，其中 <code>nokaslr</code> 参数<strong>必须添加进来</strong>，防止内核起始地址随机化，这样会导致 gdb 断点不能命中；</li><li><code>-s</code> ：监听在 gdb 1234 端口；</li><li><code>-S</code> ：表示启动后就挂起，等待 gdb 连接；</li><li><code>-nographic</code>：不启动图形界面</li></ul><p>开启另一个命令行窗口，输入<strong>gdb</strong>，即可开启调试</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(gdb) target remote localhost:1234  <span class="comment">#连接qemu监听的端口</span></span><br><span class="line">(gdb) <span class="built_in">break</span> start_kernel      <span class="comment">#在start_kernel打断点</span></span><br><span class="line">(gdb) <span class="built_in">break</span>  rest_init        <span class="comment">#在res_init打断点</span></span><br><span class="line">(gdb) c                       <span class="comment">#运行到断点处</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;本文转载来源: &lt;a href=&quot;https://wfloveiu.github.io/2023/11/20/%E9%85%8D%E7%BD%AELinux%E5%86%85%E6%A0%B8%E7%BC%96%E8%AF%911/&quot;&gt;使用虚拟机进行基于qemu和gdb的Linux内核调试&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="技术学习" scheme="https://resume.kokomi0728.eu.org/categories/%E6%8A%80%E6%9C%AF%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="Linux" scheme="https://resume.kokomi0728.eu.org/tags/Linux/"/>
    
    <category term="C/C++" scheme="https://resume.kokomi0728.eu.org/tags/C-C/"/>
    
  </entry>
  
  <entry>
    <title>TF-IDF</title>
    <link href="https://resume.kokomi0728.eu.org/posts/d7d7e9a3.html"/>
    <id>https://resume.kokomi0728.eu.org/posts/d7d7e9a3.html</id>
    <published>2023-11-16T16:00:00.000Z</published>
    <updated>2023-11-17T14:23:41.958Z</updated>
    
    <content type="html"><![CDATA[<p>$TF$是指归一化后的词频，$IDF$是指逆文档频率。给定一个文档集合$D$，有$d_1,d_2,d_3, \cdots ,d_n \in D$。</p><span id="more"></span><p>文档集合总共包含$m$个词（<strong>注：一般在计算TF−IDF时会去除如“的”这一类的停用词</strong>），有$w_1,w_2,w_3,\cdots,w_m \in W$。我们现在以计算词$w_i$在文档$d_j$中的$TF-IDF$指为例。$TF$的计算公式为：</p><p>$$TF&#x3D;\frac{freq(i,j)}{max_{len}(j)}$$</p><p>在这里$freq(i,j)$为$w_i$在$d_j$中出现的频率，$max_{len}(j)$为$d_j$长度。</p><p>$TF$只能是描述词在文档中的频率，但假设现在有个词为”我们“，这个词可能在文档集$D$中每篇文档中都会出现，并且有较高的频率。那么这一类词就不具有很好的区分文档的能力，为了降低这种通用词的作用，引入了$IDF$。</p><p>$IDF$的表达式如下：</p><p>$$IDF&#x3D;log(\frac{len(D)}{n(i)})$$<br>在这里$len(D)$表示文档集合$D$中文档的总数，$n(i)$表示含有$w_i$这个词的文档的数量。</p><p>得到$TF$和$IDF$之后，我们将这两个值相乘得到$TF−IDF$的值：</p><p>$$TF-IDF&#x3D;TF \times IDF$$</p><p>$TF$可以计算在一篇文档中词出现的频率，而$IDF$可以降低一些通用词的作用。因此对于一篇文档我们可以用文档中每个词的$TF-IDF$组成的向量来表示该文档，再根据余弦相似度这类的方法来计算文档之间的相关性。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;$TF$是指归一化后的词频，$IDF$是指逆文档频率。给定一个文档集合$D$，有$d_1,d_2,d_3, &#92;cdots ,d_n &#92;in D$。&lt;/p&gt;</summary>
    
    
    
    <category term="论文学习" scheme="https://resume.kokomi0728.eu.org/categories/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="NLP" scheme="https://resume.kokomi0728.eu.org/tags/NLP/"/>
    
    <category term="信息检索" scheme="https://resume.kokomi0728.eu.org/tags/%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2/"/>
    
  </entry>
  
  <entry>
    <title>torch.nn (Convolution Layers)</title>
    <link href="https://resume.kokomi0728.eu.org/posts/7a1e28a0.html"/>
    <id>https://resume.kokomi0728.eu.org/posts/7a1e28a0.html</id>
    <published>2023-11-14T16:00:00.000Z</published>
    <updated>2023-11-23T14:06:30.372Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本文转载自<a href="https://blog.csdn.net/xhyu61/article/details/133643860">【Pytorch笔记】7.torch.nn (Convolution Layers)</a></p></blockquote><span id="more"></span><p>我们常用torch.nn来封装网络，torch.nn为我们封装好了很多神经网络中不同的层，如卷积层、池化层、归一化层等。我们会把这些层像是串成一个牛肉串一样串起来，形成网络。</p><p>先从最简单的，都有哪些层开始学起。</p><h2 id="Convolution-Layers-卷积层"><a href="#Convolution-Layers-卷积层" class="headerlink" title="Convolution Layers - 卷积层"></a>Convolution Layers - 卷积层</h2><h3 id="torch-nn-Conv1d"><a href="#torch-nn-Conv1d" class="headerlink" title="torch.nn.Conv1d()"></a>torch.nn.Conv1d()</h3><p>1维卷积层。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.Conv1d(in_channels, </span><br><span class="line">out_channels, </span><br><span class="line">kernel_size, </span><br><span class="line">stride=<span class="number">1</span>, </span><br><span class="line">padding=<span class="number">0</span>, </span><br><span class="line">dilation=<span class="number">1</span>, </span><br><span class="line">groups=<span class="number">1</span>, </span><br><span class="line">bias=<span class="literal">True</span>, </span><br><span class="line">padding_mode=<span class="string">&#x27;zeros&#x27;</span>, </span><br><span class="line">device=<span class="literal">None</span>, </span><br><span class="line">dtype=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure><p><code>in_channels</code>：输入tensor的通道数；<br><code>out_channels</code>：输出tensor的通道数；<br><code>kernel_size</code>：卷积核的大小；<br><code>stride</code>：步长；<br><code>padding</code>：输入tensor的边界填充尺寸；<br><code>dilation</code>：卷积核之间的间距（下面这个图为<code>dilation=2</code>），默认为1；<br><img src="https://img-blog.csdnimg.cn/e26e43f7139c445d9d42324f3f4c82af.png" alt="在这里插入图片描述"></p><p><code>groups</code>：从输入通道到输出通道的阻塞连接数。<code>in_channel</code>和<code>out_channel</code>需要能被<code>groups</code>整除。更具体地：<br><code>groups=1</code>时所有输入均与所有输出进行卷积，<code>groups=2</code>时该操作相当于并排设置两个卷积层，每卷积层看到一半的输入通道，产生一半的输出通道，然后将两个卷积层连接起来。<code>groups=in_channel</code>时输入的每个通道都和相应的卷积核进行卷积；<br><code>bias</code>：是否添加可学习的偏差值，True为添加，False为不添加。<br><code>padding_mode</code>：填充模式，有以下取值：<code>zeros</code>（这个是默认值）、<code>reflect</code>、<code>replicate</code>、<code>circular</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">m = nn.Conv1d(in_channels=<span class="number">16</span>,</span><br><span class="line">              out_channels=<span class="number">33</span>,</span><br><span class="line">              kernel_size=<span class="number">3</span>,</span><br><span class="line">              stride=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># input: 批大小为20，每个数据通道为16，size=50</span></span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">20</span>, <span class="number">16</span>, <span class="number">50</span>)</span><br><span class="line">output = m(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(output.size())</span><br></pre></td></tr></table></figure><p>输出</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># output: 批大小为20，每个数据通道为33，size=24</span></span><br><span class="line">torch.Size([<span class="number">20</span>, <span class="number">33</span>, <span class="number">24</span>])</span><br></pre></td></tr></table></figure><h3 id="torch-nn-Conv2d"><a href="#torch-nn-Conv2d" class="headerlink" title="torch.nn.Conv2d()"></a>torch.nn.Conv2d()</h3><p>2维卷积层。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.Conv2d(in_channels, </span><br><span class="line">    out_channels, </span><br><span class="line">    kernel_size, </span><br><span class="line">    stride=<span class="number">1</span>, </span><br><span class="line">    padding=<span class="number">0</span>, </span><br><span class="line">    dilation=<span class="number">1</span>, </span><br><span class="line">    groups=<span class="number">1</span>, </span><br><span class="line">    bias=<span class="literal">True</span>, </span><br><span class="line">    padding_mode=<span class="string">&#x27;zeros&#x27;</span>, </span><br><span class="line">    device=<span class="literal">None</span>, </span><br><span class="line">    dtype=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure><p>参数与Conv1d()基本一样，不再赘述。 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">m = nn.Conv2d(in_channels=<span class="number">2</span>,</span><br><span class="line">              out_channels=<span class="number">3</span>,</span><br><span class="line">              kernel_size=<span class="number">3</span>,</span><br><span class="line">              stride=<span class="number">2</span>)</span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">20</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">6</span>)</span><br><span class="line">output = m(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(output.size())</span><br></pre></td></tr></table></figure><p>输出</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([<span class="number">20</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">2</span>])</span><br></pre></td></tr></table></figure><h3 id="torch-nn-Conv3d"><a href="#torch-nn-Conv3d" class="headerlink" title="torch.nn.Conv3d()"></a>torch.nn.Conv3d()</h3><p>3维卷积层。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.Conv3d(in_channels, </span><br><span class="line">    out_channels, </span><br><span class="line">    kernel_size, </span><br><span class="line">    stride=<span class="number">1</span>, </span><br><span class="line">    padding=<span class="number">0</span>, </span><br><span class="line">    dilation=<span class="number">1</span>, </span><br><span class="line">    groups=<span class="number">1</span>, </span><br><span class="line">    bias=<span class="literal">True</span>, </span><br><span class="line">    padding_mode=<span class="string">&#x27;zeros&#x27;</span>, </span><br><span class="line">    device=<span class="literal">None</span>, </span><br><span class="line">    dtype=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure><p>参数与Conv1d()基本一样，不再赘述。 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">m = nn.Conv3d(in_channels=<span class="number">2</span>,</span><br><span class="line">              out_channels=<span class="number">3</span>,</span><br><span class="line">              kernel_size=<span class="number">3</span>,</span><br><span class="line">              stride=<span class="number">2</span>)</span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">20</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>)</span><br><span class="line">output = m(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(output.size())</span><br></pre></td></tr></table></figure><p>输出</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([<span class="number">20</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>])</span><br></pre></td></tr></table></figure><h3 id="torch-nn-ConvTranspose1d"><a href="#torch-nn-ConvTranspose1d" class="headerlink" title="torch.nn.ConvTranspose1d()"></a>torch.nn.ConvTranspose1d()</h3><p>1维转置卷积层。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.ConvTranspose1d(in_channels, </span><br><span class="line"> out_channels, </span><br><span class="line"> kernel_size, </span><br><span class="line"> stride=<span class="number">1</span>, </span><br><span class="line"> padding=<span class="number">0</span>, </span><br><span class="line"> output_padding=<span class="number">0</span>, </span><br><span class="line"> groups=<span class="number">1</span>, </span><br><span class="line"> bias=<span class="literal">True</span>, </span><br><span class="line"> dilation=<span class="number">1</span>, </span><br><span class="line"> padding_mode=<span class="string">&#x27;zeros&#x27;</span>, </span><br><span class="line"> device=<span class="literal">None</span>, </span><br><span class="line"> dtype=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure><p>参数与Conv1d()基本一样，不再赘述。<br>唯一不同的是<code>output_padding</code>，与<code>padding</code>不同的是，<code>output_padding</code>是输出tensor的每一个边，外面填充的层数。<br>（<code>padding</code>是输入tensor的每个边填充的层数）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">m = nn.ConvTranspose1d(in_channels=<span class="number">2</span>,</span><br><span class="line">                       out_channels=<span class="number">3</span>,</span><br><span class="line">                       kernel_size=<span class="number">3</span>,</span><br><span class="line">                       stride=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">20</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">output = m(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(output.size())</span><br></pre></td></tr></table></figure><p>输出</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([<span class="number">20</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br></pre></td></tr></table></figure><h3 id="torch-nn-ConvTranspose2d"><a href="#torch-nn-ConvTranspose2d" class="headerlink" title="torch.nn.ConvTranspose2d()"></a>torch.nn.ConvTranspose2d()</h3><p>2维转置卷积层。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.ConvTranspose2d(in_channels, </span><br><span class="line"> out_channels, </span><br><span class="line"> kernel_size, </span><br><span class="line"> stride=<span class="number">1</span>, </span><br><span class="line"> padding=<span class="number">0</span>, </span><br><span class="line"> output_padding=<span class="number">0</span>, </span><br><span class="line"> groups=<span class="number">1</span>, </span><br><span class="line"> bias=<span class="literal">True</span>, </span><br><span class="line"> dilation=<span class="number">1</span>, </span><br><span class="line"> padding_mode=<span class="string">&#x27;zeros&#x27;</span>, </span><br><span class="line"> device=<span class="literal">None</span>, </span><br><span class="line"> dtype=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure><p>参数与Conv1d()基本一样，不再赘述。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">m = nn.ConvTranspose2d(in_channels=<span class="number">2</span>,</span><br><span class="line">                       out_channels=<span class="number">3</span>,</span><br><span class="line">                       kernel_size=<span class="number">3</span>,</span><br><span class="line">                       stride=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">20</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">output = m(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(output.size())</span><br></pre></td></tr></table></figure><p>输出</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([<span class="number">20</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">4</span>])</span><br></pre></td></tr></table></figure><h3 id="torch-nn-ConvTranspose3d"><a href="#torch-nn-ConvTranspose3d" class="headerlink" title="torch.nn.ConvTranspose3d()"></a>torch.nn.ConvTranspose3d()</h3><p>3维转置卷积层。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.ConvTranspose3d(in_channels, </span><br><span class="line"> out_channels, </span><br><span class="line"> kernel_size, </span><br><span class="line"> stride=<span class="number">1</span>, </span><br><span class="line"> padding=<span class="number">0</span>, </span><br><span class="line"> output_padding=<span class="number">0</span>, </span><br><span class="line"> groups=<span class="number">1</span>, </span><br><span class="line"> bias=<span class="literal">True</span>, </span><br><span class="line"> dilation=<span class="number">1</span>, </span><br><span class="line"> padding_mode=<span class="string">&#x27;zeros&#x27;</span>, </span><br><span class="line"> device=<span class="literal">None</span>, </span><br><span class="line"> dtype=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure><p>参数与Conv1d()基本一样，不再赘述。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">m = nn.ConvTranspose3d(in_channels=<span class="number">2</span>,</span><br><span class="line">                       out_channels=<span class="number">3</span>,</span><br><span class="line">                       kernel_size=<span class="number">3</span>,</span><br><span class="line">                       stride=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">20</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">output = m(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(output.size())</span><br></pre></td></tr></table></figure><p>输出</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([<span class="number">20</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>])</span><br></pre></td></tr></table></figure><h3 id="torch-nn-LazyConv1d"><a href="#torch-nn-LazyConv1d" class="headerlink" title="torch.nn.LazyConv1d()"></a>torch.nn.LazyConv1d()</h3><p>1维延迟初始化卷积层，当in_channel不确定时可使用这个层。<br>关于延迟初始化，大家可以参考这篇文章，我认为讲的很好：<br><a href="https://blog.csdn.net/weixin_43180762/article/details/124299823">俱往矣… - 延迟初始化——【torch学习笔记】</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.LazyConv1d(out_channels, </span><br><span class="line">kernel_size, </span><br><span class="line">stride=<span class="number">1</span>, </span><br><span class="line">padding=<span class="number">0</span>, </span><br><span class="line">dilation=<span class="number">1</span>, </span><br><span class="line">groups=<span class="number">1</span>, </span><br><span class="line">bias=<span class="literal">True</span>, </span><br><span class="line">padding_mode=<span class="string">&#x27;zeros&#x27;</span>, </span><br><span class="line">device=<span class="literal">None</span>, </span><br><span class="line">dtype=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure><p><strong>LazyConv1d没有in_channel参数</strong>。<br>这不代表这个层没有输入的通道，而是在调用时自动适配，并进行初始化。<br>引用文章中的一段代码，改成LazyConv1d，讲述使用方法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    nn.LazyConv1d(<span class="number">256</span>, <span class="number">2</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Linear(<span class="number">9</span>, <span class="number">10</span>)</span><br><span class="line">)</span><br><span class="line"><span class="built_in">print</span>(net)</span><br><span class="line">[net[i].state_dict() <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(net))]</span><br><span class="line"></span><br><span class="line">low = torch.finfo(torch.float32).<span class="built_in">min</span> / <span class="number">10</span></span><br><span class="line">high = torch.finfo(torch.float32).<span class="built_in">max</span> / <span class="number">10</span></span><br><span class="line">X = torch.zeros([<span class="number">2</span>, <span class="number">20</span>, <span class="number">10</span>], dtype=torch.float32).uniform_(low, high)</span><br><span class="line">net(X)</span><br><span class="line"><span class="built_in">print</span>(net)</span><br></pre></td></tr></table></figure><p>输出</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Sequential(</span><br><span class="line">  (<span class="number">0</span>): LazyConv1d(<span class="number">0</span>, <span class="number">256</span>, kernel_size=(<span class="number">2</span>,), stride=(<span class="number">1</span>,))</span><br><span class="line">  (<span class="number">1</span>): ReLU()</span><br><span class="line">  (<span class="number">2</span>): Linear(in_features=<span class="number">9</span>, out_features=<span class="number">10</span>, bias=<span class="literal">True</span>)</span><br><span class="line">)</span><br><span class="line">Sequential(</span><br><span class="line">  (<span class="number">0</span>): Conv1d(<span class="number">20</span>, <span class="number">256</span>, kernel_size=(<span class="number">2</span>,), stride=(<span class="number">1</span>,))</span><br><span class="line">  (<span class="number">1</span>): ReLU()</span><br><span class="line">  (<span class="number">2</span>): Linear(in_features=<span class="number">9</span>, out_features=<span class="number">10</span>, bias=<span class="literal">True</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>可以看出，未进行初始化时，in_features&#x3D;0。只有传入参数使用网络后才会根据输入进行初始化。</p><h3 id="torch-nn-LazyConv2d"><a href="#torch-nn-LazyConv2d" class="headerlink" title="torch.nn.LazyConv2d()"></a>torch.nn.LazyConv2d()</h3><p>2维延迟初始化卷积层。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.LazyConv2d(out_channels, </span><br><span class="line">kernel_size, </span><br><span class="line">stride=<span class="number">1</span>, </span><br><span class="line">padding=<span class="number">0</span>, </span><br><span class="line">dilation=<span class="number">1</span>, </span><br><span class="line">groups=<span class="number">1</span>, </span><br><span class="line">bias=<span class="literal">True</span>, </span><br><span class="line">padding_mode=<span class="string">&#x27;zeros&#x27;</span>, </span><br><span class="line">device=<span class="literal">None</span>, </span><br><span class="line">dtype=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure><h3 id="torch-nn-LazyConv3d"><a href="#torch-nn-LazyConv3d" class="headerlink" title="torch.nn.LazyConv3d()"></a>torch.nn.LazyConv3d()</h3><p>3维延迟初始化卷积层。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.LazyConv3d(out_channels, </span><br><span class="line">kernel_size, </span><br><span class="line">stride=<span class="number">1</span>, </span><br><span class="line">padding=<span class="number">0</span>, </span><br><span class="line">dilation=<span class="number">1</span>, </span><br><span class="line">groups=<span class="number">1</span>, </span><br><span class="line">bias=<span class="literal">True</span>, </span><br><span class="line">padding_mode=<span class="string">&#x27;zeros&#x27;</span>, </span><br><span class="line">device=<span class="literal">None</span>, </span><br><span class="line">dtype=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure><h3 id="torch-nn-LazyConvTranspose1d"><a href="#torch-nn-LazyConvTranspose1d" class="headerlink" title="torch.nn.LazyConvTranspose1d()"></a>torch.nn.LazyConvTranspose1d()</h3><p>1维延迟初始化转置卷积层。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.LazyConvTranspose1d(out_channels, </span><br><span class="line"> kernel_size, </span><br><span class="line"> stride=<span class="number">1</span>, </span><br><span class="line"> padding=<span class="number">0</span>, </span><br><span class="line"> output_padding=<span class="number">0</span>, </span><br><span class="line"> groups=<span class="number">1</span>, </span><br><span class="line"> bias=<span class="literal">True</span>, </span><br><span class="line"> dilation=<span class="number">1</span>, </span><br><span class="line"> padding_mode=<span class="string">&#x27;zeros&#x27;</span>, </span><br><span class="line"> device=<span class="literal">None</span>, </span><br><span class="line"> dtype=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure><h3 id="torch-nn-LazyConvTranspose2d"><a href="#torch-nn-LazyConvTranspose2d" class="headerlink" title="torch.nn.LazyConvTranspose2d()"></a>torch.nn.LazyConvTranspose2d()</h3><p>2维延迟初始化转置卷积层。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.LazyConvTranspose2d(out_channels, </span><br><span class="line"> kernel_size, </span><br><span class="line"> stride=<span class="number">1</span>, </span><br><span class="line"> padding=<span class="number">0</span>, </span><br><span class="line"> output_padding=<span class="number">0</span>, </span><br><span class="line"> groups=<span class="number">1</span>, </span><br><span class="line"> bias=<span class="literal">True</span>, </span><br><span class="line"> dilation=<span class="number">1</span>, </span><br><span class="line"> padding_mode=<span class="string">&#x27;zeros&#x27;</span>, </span><br><span class="line"> device=<span class="literal">None</span>, </span><br><span class="line"> dtype=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure><h3 id="torch-nn-LazyConvTranspose3d"><a href="#torch-nn-LazyConvTranspose3d" class="headerlink" title="torch.nn.LazyConvTranspose3d()"></a>torch.nn.LazyConvTranspose3d()</h3><p>3维延迟初始化转置卷积层。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.LazyConvTranspose3d(out_channels, </span><br><span class="line"> kernel_size, </span><br><span class="line"> stride=<span class="number">1</span>, </span><br><span class="line"> padding=<span class="number">0</span>, </span><br><span class="line"> output_padding=<span class="number">0</span>, </span><br><span class="line"> groups=<span class="number">1</span>, </span><br><span class="line"> bias=<span class="literal">True</span>, </span><br><span class="line"> dilation=<span class="number">1</span>, </span><br><span class="line"> padding_mode=<span class="string">&#x27;zeros&#x27;</span>, </span><br><span class="line"> device=<span class="literal">None</span>, </span><br><span class="line"> dtype=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure><h3 id="torch-nn-Unfold"><a href="#torch-nn-Unfold" class="headerlink" title="torch.nn.Unfold()"></a>torch.nn.Unfold()</h3><p>从一个批次的输入张量中提取出滑动的局部区域块。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.Unfold(kernel_size, </span><br><span class="line">dilation=<span class="number">1</span>, </span><br><span class="line">padding=<span class="number">0</span>, </span><br><span class="line">stride=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p><code>kernel_size</code>：滑动块的大小；<br><code>dilation</code>：卷积核之间的间距(torch.nn.Conv1d中有图示)；<br><code>padding</code>：输入tensor的边界填充尺寸；<br><code>stride</code>：滑块滑动的步长。</p><p>这里的输入必须是4维的tensor，否则会报这样的错误：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">NotImplementedError: Input Error: Only 4D <span class="built_in">input</span> Tensors are supported (got 2D)</span><br></pre></td></tr></table></figure><p>示例</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">t = torch.tensor([[[[<span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">3.</span>,  <span class="number">4.</span>],</span><br><span class="line">                    [<span class="number">5.</span>,  <span class="number">6.</span>,  <span class="number">7.</span>,  <span class="number">8.</span>],</span><br><span class="line">                    [<span class="number">9.</span>,  <span class="number">10.</span>, <span class="number">11.</span>, <span class="number">12.</span>],</span><br><span class="line">                    [<span class="number">13.</span>, <span class="number">14.</span>, <span class="number">15.</span>, <span class="number">16.</span>],]]])</span><br><span class="line"></span><br><span class="line">unfold = nn.Unfold(kernel_size=(<span class="number">2</span>, <span class="number">2</span>), dilation=<span class="number">1</span>, padding=<span class="number">0</span>, stride=<span class="number">1</span>)</span><br><span class="line">output = unfold(t)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br></pre></td></tr></table></figure><p>输出</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[ <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">3.</span>,  <span class="number">5.</span>,  <span class="number">6.</span>,  <span class="number">7.</span>,  <span class="number">9.</span>, <span class="number">10.</span>, <span class="number">11.</span>],</span><br><span class="line">         [ <span class="number">2.</span>,  <span class="number">3.</span>,  <span class="number">4.</span>,  <span class="number">6.</span>,  <span class="number">7.</span>,  <span class="number">8.</span>, <span class="number">10.</span>, <span class="number">11.</span>, <span class="number">12.</span>],</span><br><span class="line">         [ <span class="number">5.</span>,  <span class="number">6.</span>,  <span class="number">7.</span>,  <span class="number">9.</span>, <span class="number">10.</span>, <span class="number">11.</span>, <span class="number">13.</span>, <span class="number">14.</span>, <span class="number">15.</span>],</span><br><span class="line">         [ <span class="number">6.</span>,  <span class="number">7.</span>,  <span class="number">8.</span>, <span class="number">10.</span>, <span class="number">11.</span>, <span class="number">12.</span>, <span class="number">14.</span>, <span class="number">15.</span>, <span class="number">16.</span>]]])</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/85ec11fae3dc464bb1877d941cbf777a.png" alt="在这里插入图片描述"></p><h3 id="torch-nn-Fold"><a href="#torch-nn-Fold" class="headerlink" title="torch.nn.Fold()"></a>torch.nn.Fold()</h3><p>Unfold()的逆操作。当Unfold()时出现滑块有重复覆盖时会导致结果和原来不一样。因为Fold()的过程中对于同一个位置的元素进行加法处理。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.Fold(output_size, </span><br><span class="line">  kernel_size, </span><br><span class="line">  dilation=<span class="number">1</span>, </span><br><span class="line">  padding=<span class="number">0</span>, </span><br><span class="line">  stride=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>下面是Unfold()和Fold()结合的代码，Unfold()部分和上面代码相同。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">t = torch.tensor([[[[<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>, <span class="number">4.</span>],</span><br><span class="line">                  [<span class="number">5.</span>, <span class="number">6.</span>, <span class="number">7.</span>, <span class="number">8.</span>],</span><br><span class="line">                  [<span class="number">9.</span>, <span class="number">10.</span>, <span class="number">11.</span>, <span class="number">12.</span>],</span><br><span class="line">                  [<span class="number">13.</span>, <span class="number">14.</span>, <span class="number">15.</span>, <span class="number">16.</span>]]]])</span><br><span class="line"></span><br><span class="line">unfold = nn.Unfold(kernel_size=(<span class="number">2</span>, <span class="number">2</span>), dilation=<span class="number">1</span>, padding=<span class="number">0</span>, stride=<span class="number">1</span>)</span><br><span class="line">output = unfold(t)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br><span class="line">fold = nn.Fold(output_size=(<span class="number">4</span>, <span class="number">4</span>), kernel_size=(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">out = fold(output)</span><br><span class="line"><span class="built_in">print</span>(out)</span><br></pre></td></tr></table></figure><p>输出</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[ <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">3.</span>,  <span class="number">5.</span>,  <span class="number">6.</span>,  <span class="number">7.</span>,  <span class="number">9.</span>, <span class="number">10.</span>, <span class="number">11.</span>],</span><br><span class="line">         [ <span class="number">2.</span>,  <span class="number">3.</span>,  <span class="number">4.</span>,  <span class="number">6.</span>,  <span class="number">7.</span>,  <span class="number">8.</span>, <span class="number">10.</span>, <span class="number">11.</span>, <span class="number">12.</span>],</span><br><span class="line">         [ <span class="number">5.</span>,  <span class="number">6.</span>,  <span class="number">7.</span>,  <span class="number">9.</span>, <span class="number">10.</span>, <span class="number">11.</span>, <span class="number">13.</span>, <span class="number">14.</span>, <span class="number">15.</span>],</span><br><span class="line">         [ <span class="number">6.</span>,  <span class="number">7.</span>,  <span class="number">8.</span>, <span class="number">10.</span>, <span class="number">11.</span>, <span class="number">12.</span>, <span class="number">14.</span>, <span class="number">15.</span>, <span class="number">16.</span>]]])</span><br><span class="line">tensor([[[[ <span class="number">1.</span>,  <span class="number">4.</span>,  <span class="number">6.</span>,  <span class="number">4.</span>],</span><br><span class="line">          [<span class="number">10.</span>, <span class="number">24.</span>, <span class="number">28.</span>, <span class="number">16.</span>],</span><br><span class="line">          [<span class="number">18.</span>, <span class="number">40.</span>, <span class="number">44.</span>, <span class="number">24.</span>],</span><br><span class="line">          [<span class="number">13.</span>, <span class="number">28.</span>, <span class="number">30.</span>, <span class="number">16.</span>]]]])</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;本文转载自&lt;a href=&quot;https://blog.csdn.net/xhyu61/article/details/133643860&quot;&gt;【Pytorch笔记】7.torch.nn (Convolution Layers)&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="论文学习" scheme="https://resume.kokomi0728.eu.org/categories/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="机器学习" scheme="https://resume.kokomi0728.eu.org/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="PyTorch" scheme="https://resume.kokomi0728.eu.org/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>嵌入坍缩</title>
    <link href="https://resume.kokomi0728.eu.org/posts/3c1401a1.html"/>
    <id>https://resume.kokomi0728.eu.org/posts/3c1401a1.html</id>
    <published>2023-11-12T16:00:00.000Z</published>
    <updated>2023-11-17T14:22:07.655Z</updated>
    
    <content type="html"><![CDATA[<p>“Embedding collapse”（嵌入坍缩）是指在训练神经网络时，由于模型的复杂性或者数据分布的问题，导致嵌入空间中的不同输入被映射到相似的嵌入向量。这种情况会使得模型在处理不同输入时难以区分它们，因为它们被映射到了相似的表示。</p><span id="more"></span><p>嵌入坍缩通常是不希望出现的，因为模型需要在嵌入空间中保留输入数据的差异，以便正确地学习和泛化。嵌入坍缩可能导致模型在训练和测试时表现不佳，因为模型难以捕捉输入之间的微小差异。</p><p>以下是一些可能导致嵌入坍缩的原因：</p><ol><li><p><strong>数据不平衡：</strong> 如果训练数据中某些类别的样本数量远远超过其他类别，模型可能更容易将它们映射到相似的嵌入向量，而忽略了较少出现的类别。</p></li><li><p><strong>过度拟合：</strong> 当模型过度拟合训练数据时，它可能会学到一些特定的模式，而不是泛化到更广泛的情况。这可能导致嵌入坍缩，使得相似的输入被映射到相似的表示。</p></li><li><p><strong>模型复杂性：</strong> 过于复杂的模型可能会倾向于将输入映射到一个较小的嵌入空间，导致嵌入坍缩。这可能发生在具有大量参数的深度神经网络中。</p></li></ol><p>为了缓解嵌入坍缩的问题，可以尝试以下方法：</p><ul><li><p><strong>正则化：</strong> 添加正则化项，如L1或L2正则化，以防止模型学习过于复杂的表示。</p></li><li><p><strong>数据增强：</strong> 通过对训练数据进行变换或增强，引入更多的差异性，帮助模型学到更鲁棒的表示。</p></li><li><p><strong>平衡数据：</strong> 确保训练数据中的不同类别样本数量相对平衡，以防止某些类别的过度表示。</p></li><li><p><strong>监控训练过程：</strong> 使用监控工具和可视化方法来检查模型在嵌入空间中的学习情况，及时发现和解决嵌入坍缩问题。</p></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;“Embedding collapse”（嵌入坍缩）是指在训练神经网络时，由于模型的复杂性或者数据分布的问题，导致嵌入空间中的不同输入被映射到相似的嵌入向量。这种情况会使得模型在处理不同输入时难以区分它们，因为它们被映射到了相似的表示。&lt;/p&gt;</summary>
    
    
    
    <category term="论文学习" scheme="https://resume.kokomi0728.eu.org/categories/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="NLP" scheme="https://resume.kokomi0728.eu.org/tags/NLP/"/>
    
    <category term="信息检索" scheme="https://resume.kokomi0728.eu.org/tags/%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2/"/>
    
  </entry>
  
  <entry>
    <title>GLUE</title>
    <link href="https://resume.kokomi0728.eu.org/posts/9f7dd732.html"/>
    <id>https://resume.kokomi0728.eu.org/posts/9f7dd732.html</id>
    <published>2023-11-12T16:00:00.000Z</published>
    <updated>2023-11-17T14:22:21.573Z</updated>
    
    <content type="html"><![CDATA[<p>GLUE和SUPER-GLUE是用于衡量自然语言处理模型表现的两个基准测试体系:</p><span id="more"></span><p>GLUE(General Language Understanding Evaluation):</p><ul><li>由谷歌 AI 实验室于2018年提出。</li><li>包括9个不同类型的NLP任务,如句子推理、naming等。 </li><li>旨在评估模型在各种常见NLP任务上的泛化能力。</li></ul><p>SUPER-GLUE:</p><ul><li>是GLUE基准测试的升级版,由同一团队于2019年发布。</li><li>增加和改进了一些GLUE任务,加入了更难更复杂的新任务。</li><li>任务包括:想象性问答、多项选择、修辞关系等难点任务。</li><li>对模型的理解和推理能力要求更高。</li></ul><p>两个基准测试的主要区别:</p><ul><li>SUPER-GLUE的任务设置更难,对模型的要求更高。</li><li>可以更全面和细致地检测模型在不同程度的NLP任务中的表现。</li><li>成为评估新一代更强大NLP模型的主流标准,如BERT、GPT-3等。</li></ul><p>因此,在最新的研究中,模型表现都以其在GLUE和SUPER-GLUE上的效果进行报告,成为NLP进步的关键指标之一。</p><p>GLUE（General Language Understanding Evaluation）是一个用于评估自然语言处理（NLP）模型性能的基准测试集合。GLUE基准测试涵盖了多个任务，包括文本分类、语义相似度、自然语言推断等。下面是GLUE基准测试中包括的九个任务的详细解释：</p><ol><li><p><strong>CoLA（Corpus of Linguistic Acceptability）:</strong></p><ul><li><strong>任务：</strong> 判断给定的句子是否在语法和语义上是可接受的。</li><li><strong>示例：</strong> “He is playing the piano.”（可接受） vs. “Him play piano.”（不可接受）</li></ul></li><li><p><strong>SST-2（Stanford Sentiment Treebank）:</strong></p><ul><li><strong>任务：</strong> 对给定的句子进行情感分类，判断情感是正面、负面还是中性的。</li><li><strong>示例：</strong> “I love this product!”（正面） vs. “This is the worst movie ever.”（负面）</li></ul></li><li><p><strong>MRPC（Microsoft Research Paraphrase Corpus）:</strong></p><ul><li><strong>任务：</strong> 判断给定的两个句子是否是语义上相似的。</li><li><strong>示例：</strong> “The cat is on the mat.” vs. “The cat is lying on the mat.”（相似）</li></ul></li><li><p><strong>STS-B（Semantic Textual Similarity Benchmark）:</strong></p><ul><li><strong>任务：</strong> 对给定的两个句子进行语义相似度评分。</li><li><strong>示例：</strong> “A man is playing a large flute.” vs. “A man is playing a flute.”（相似度得分）</li></ul></li><li><p><strong>QQP（Quora Question Pairs）:</strong></p><ul><li><strong>任务：</strong> 判断给定的两个问题是否是语义上相似的。</li><li><strong>示例：</strong> “How can I be a good geologist?” vs. “What should I do to be a great geologist?”（相似）</li></ul></li><li><p><strong>MNLI（MultiNLI）:</strong></p><ul><li><strong>任务：</strong> 自然语言推断任务，给定一个前提句子和一个假设句子，判断假设在给定前提下是蕴含、矛盾还是中立关系。</li><li><strong>示例：</strong> Premise: “The cat is sitting on the windowsill.”， Hypothesis: “The cat is outside.”（矛盾）</li></ul></li><li><p><strong>QNLI（Question Natural Language Inference）:</strong></p><ul><li><strong>任务：</strong> 是MNLI任务的变体，专注于问题和短文本之间的推断关系。</li></ul></li><li><p><strong>RTE（Recognizing Textual Entailment）:</strong></p><ul><li><strong>任务：</strong> 自然语言推断任务，判断给定的两个文本之间是否存在蕴涵关系。</li><li><strong>示例：</strong> Text: “A soccer game with multiple males playing.”， Hypothesis: “Some men are playing a sport.”（蕴涵）</li></ul></li></ol><p>以上是GLUE基准测试中包括的九个任务，每个任务都有一组训练和测试数据，用于评估模型在不同自然语言处理任务上的性能。这些任务旨在涵盖多样的自然语言理解方面，从而全面评估模型的通用性。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;GLUE和SUPER-GLUE是用于衡量自然语言处理模型表现的两个基准测试体系:&lt;/p&gt;</summary>
    
    
    
    <category term="论文学习" scheme="https://resume.kokomi0728.eu.org/categories/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="NLP" scheme="https://resume.kokomi0728.eu.org/tags/NLP/"/>
    
    <category term="信息检索" scheme="https://resume.kokomi0728.eu.org/tags/%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2/"/>
    
  </entry>
  
  <entry>
    <title>L1正则化</title>
    <link href="https://resume.kokomi0728.eu.org/posts/18e73c3c.html"/>
    <id>https://resume.kokomi0728.eu.org/posts/18e73c3c.html</id>
    <published>2023-11-12T16:00:00.000Z</published>
    <updated>2023-11-17T14:22:31.685Z</updated>
    
    <content type="html"><![CDATA[<p>L1正则化是一种在机器学习中用于降低模型复杂度的技术。它通过向模型的损失函数添加L1范数（向量中各元素绝对值的和）的惩罚来实现。L1正则化的目的是促使模型学习到稀疏权重，即让一些特征对应的权重变为零，从而减少模型的复杂性。</p><span id="more"></span><p>对于一个线性回归模型，L1正则化的损失函数可以写成：</p><p>$$ J(\theta) &#x3D; \text{MSE}(\theta) + \lambda \sum_{i&#x3D;1}^{n} | \theta_i | $$</p><p>其中：</p><ul><li>$J(\theta)$ 是带有L1正则化的损失函数；</li><li>$\text{MSE}(\theta)$ 是均方误差（Mean Squared Error）损失函数，用于衡量模型的预测与实际值之间的差距；</li><li>$\lambda$ 是正则化强度，控制着正则化对总损失的影响；</li><li>$\sum_{i&#x3D;1}^{n} | \theta_i |$ 是模型权重向量 (\theta) 中所有权重的绝对值之和。</li></ul><p>L1正则化的效果是，一些特征对应的权重会变为零，从而达到特征选择（feature selection）的效果。这使得模型更加简单，减少了过拟合的风险，并提高了模型的泛化能力。</p><p>在深度学习中，L1正则化同样可以应用于神经网络的权重，通过控制权重的稀疏性来提高模型的泛化性能。在实践中，通常使用梯度下降等优化算法来最小化包含L1正则项的损失函数。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;L1正则化是一种在机器学习中用于降低模型复杂度的技术。它通过向模型的损失函数添加L1范数（向量中各元素绝对值的和）的惩罚来实现。L1正则化的目的是促使模型学习到稀疏权重，即让一些特征对应的权重变为零，从而减少模型的复杂性。&lt;/p&gt;</summary>
    
    
    
    <category term="论文学习" scheme="https://resume.kokomi0728.eu.org/categories/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="机器学习" scheme="https://resume.kokomi0728.eu.org/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="数学" scheme="https://resume.kokomi0728.eu.org/tags/%E6%95%B0%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>L2归一化</title>
    <link href="https://resume.kokomi0728.eu.org/posts/ff9e5701.html"/>
    <id>https://resume.kokomi0728.eu.org/posts/ff9e5701.html</id>
    <published>2023-11-12T16:00:00.000Z</published>
    <updated>2023-11-17T14:22:42.943Z</updated>
    
    <content type="html"><![CDATA[<p>L2归一化，也称为L2范数归一化，是一种向量归一化的方法，它将向量的每个元素除以其L2范数（Euclidean范数）。</p><span id="more"></span><p>L2范数是向量元素的平方和的平方根。对于一个向量 ($x &#x3D; [x_1, x_2, …, x_n]$)，其L2范数表示为：<br>$$ |x|_2 &#x3D; \sqrt{x_1^2 + x_2^2 + … + x_n^2} $$</p><p>L2归一化的过程是将向量的每个元素除以其L2范数，得到一个具有单位长度的向量。具体而言，对于向量 $x$，它的L2归一化 $\hat{x}$ 计算如下：</p><p>$$ \hat{x} &#x3D; \frac{x}{|x|_2} $$</p><p>L2归一化的目标是将向量的模（长度）缩放到1，从而使得向量在空间中的表示更加规范化，便于比较和处理。在深度学习中，L2归一化有时被用作正则化的手段，可以帮助控制模型的复杂度，防止过拟合。</p><p>在机器学习和深度学习中，除了L2归一化，还有L1归一化和Lp范数归一化等不同的向量归一化方法，用于处理不同的问题和优化需求。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;L2归一化，也称为L2范数归一化，是一种向量归一化的方法，它将向量的每个元素除以其L2范数（Euclidean范数）。&lt;/p&gt;</summary>
    
    
    
    <category term="论文学习" scheme="https://resume.kokomi0728.eu.org/categories/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="机器学习" scheme="https://resume.kokomi0728.eu.org/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="数学" scheme="https://resume.kokomi0728.eu.org/tags/%E6%95%B0%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>NLI</title>
    <link href="https://resume.kokomi0728.eu.org/posts/d68980e5.html"/>
    <id>https://resume.kokomi0728.eu.org/posts/d68980e5.html</id>
    <published>2023-11-12T16:00:00.000Z</published>
    <updated>2023-11-17T14:23:06.861Z</updated>
    
    <content type="html"><![CDATA[<p>NLI 是 “Natural Language Inference”（自然语言推断）的缩写。这是一种自然语言处理（NLP）任务，旨在评估一个系统对于给定的两个句子之间的关系的理解能力。通常情况下，这个任务被定义为三个类别的分类问题：</p><span id="more"></span><ol><li><p><strong>蕴涵（Entailment）：</strong> 如果一个句子可以从另一个句子中推断出来，那么这两个句子之间存在蕴涵关系。例如，如果第一个句子是 “狗在跑步”，第二个句子是 “动物在运动”，那么我们可以说第一个句子蕴含（entails）第二个句子。</p></li><li><p><strong>矛盾（Contradiction）：</strong> 如果两个句子之间存在逻辑上的矛盾，那么它们之间存在矛盾关系。例如，如果第一个句子是 “太阳从东方升起”，第二个句子是 “太阳从西方升起”，那么这两个句子之间存在矛盾关系。</p></li><li><p><strong>中性（Neutral）：</strong> 如果两个句子之间既不是蕴涵关系，也不是矛盾关系，那么它们之间是中性关系。例如，第一个句子是 “猫喜欢晒太阳”，第二个句子是 “天空是蓝色的”，这两个句子之间可能没有明显的蕴涵或矛盾关系。</p></li></ol><p>NLI 是自然语言处理中一个重要的任务，它具有广泛的应用，包括问答系统、机器翻译、对话系统等。训练一个在 NLI 任务上表现良好的模型可以提升系统对语义关系的理解能力，从而在各种应用中提供更准确的自然语言理解。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;NLI 是 “Natural Language Inference”（自然语言推断）的缩写。这是一种自然语言处理（NLP）任务，旨在评估一个系统对于给定的两个句子之间的关系的理解能力。通常情况下，这个任务被定义为三个类别的分类问题：&lt;/p&gt;</summary>
    
    
    
    <category term="论文学习" scheme="https://resume.kokomi0728.eu.org/categories/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="NLP" scheme="https://resume.kokomi0728.eu.org/tags/NLP/"/>
    
    <category term="信息检索" scheme="https://resume.kokomi0728.eu.org/tags/%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2/"/>
    
  </entry>
  
  <entry>
    <title>Zero-shot</title>
    <link href="https://resume.kokomi0728.eu.org/posts/a839b106.html"/>
    <id>https://resume.kokomi0728.eu.org/posts/a839b106.html</id>
    <published>2023-11-11T16:00:00.000Z</published>
    <updated>2023-11-17T14:24:06.223Z</updated>
    
    <content type="html"><![CDATA[<p>“Zero-shot” 是一种指在模型训练阶段未见过特定任务或类别的能力。在这种情况下，模型被要求执行没有直接训练样本的任务。这对于泛化到新领域或新任务非常有用。</p><span id="more"></span><p>举几个例子来说明 zero-shot 的概念：</p><ol><li><p><strong>图像分类：</strong> 假设有一个图像分类模型，经过训练可以识别猫、狗、汽车等类别。如果在测试阶段，模型被要求对一些在训练数据中从未见过的类别，比如大象、飞机进行分类，而它仍能够正确分类，那么这就是 zero-shot 图像分类。</p></li><li><p><strong>文本情感分析：</strong> 假设有一个情感分析模型，它在训练阶段只见过积极和消极的情感类别。如果在测试阶段，模型被要求对一种新的情感类别，比如中立或焦虑进行分类，而它能够适应并进行准确的分类，那么这就是 zero-shot 情感分析。</p></li><li><p><strong>自然语言处理（NLP）中的零样本学习：</strong> 在NLP领域，zero-shot 学习可能涉及到对一些未在训练中见过的主题或任务的理解。例如，一个文本生成模型在训练时只接触到了电影评论，但在测试时可以生成关于体育比赛的文本，而且质量仍然很高，这就是 zero-shot 学习。</p></li></ol><p>Zero-shot 能力通常需要模型具有很强的泛化能力，能够推广到新的情境和任务。在深度学习中，一些预训练的模型（如 GPT、BERT）通过大规模训练来获得广泛的知识，使它们在 zero-shot 或 few-shot 任务上表现出色。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;“Zero-shot” 是一种指在模型训练阶段未见过特定任务或类别的能力。在这种情况下，模型被要求执行没有直接训练样本的任务。这对于泛化到新领域或新任务非常有用。&lt;/p&gt;</summary>
    
    
    
    <category term="论文学习" scheme="https://resume.kokomi0728.eu.org/categories/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="机器学习" scheme="https://resume.kokomi0728.eu.org/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>词袋模型</title>
    <link href="https://resume.kokomi0728.eu.org/posts/e8b9bf93.html"/>
    <id>https://resume.kokomi0728.eu.org/posts/e8b9bf93.html</id>
    <published>2023-11-11T16:00:00.000Z</published>
    <updated>2023-11-17T14:19:21.983Z</updated>
    
    <content type="html"><![CDATA[<p>词袋模型（Bag of Words，简称BoW）是信息检索领域中常用的一种文本表示方法。它将文本表示为一个无序的词汇项集合，忽略了文本中词汇的顺序和语法结构，仅关注每个词汇在文本中出现的频率。</p><span id="more"></span><p>以下是词袋模型的基本步骤和解释：</p><ol><li><p><strong>构建词汇表（Vocabulary）：</strong> 首先，从文本数据中提取所有不同的词汇，形成一个词汇表。这个词汇表包含了文本中出现的所有单词，去除了停用词等不重要的词汇。</p></li><li><p><strong>向量化文本：</strong> 对于每个文本样本，将其表示为一个与词汇表等长的向量，向量的每个元素对应于词汇表中的一个词汇。向量的每个元素表示对应词汇在文本中出现的频率。</p><p>例如，如果有一个词汇表 <code>[&quot;apple&quot;, &quot;orange&quot;, &quot;banana&quot;]</code>，而文本 “I like apple and apple” 则可以表示为向量 <code>[2, 0, 0]</code>，其中第一个元素表示 “apple” 出现的次数，其他元素表示 “orange” 和 “banana” 的出现次数。</p></li><li><p><strong>忽略词序和语法：</strong> 词袋模型忽略了文本中词汇的顺序和语法结构，只关注词汇的出现频率。这意味着具有相同词汇分布的文本在词袋表示中是相似的，而不考虑它们的词汇顺序。</p></li><li><p><strong>稀疏表示：</strong> 由于大多数文本只包含词汇表中的一小部分词汇，词袋模型的表示通常是稀疏的，即大多数元素为零。这种表示形式有效地减少了存储和计算的复杂性。</p></li></ol><p>词袋模型是一种简单而有效的文本表示方法，常用于文本分类、信息检索和自然语言处理任务。然而，它也有一些局限性，例如无法捕捉词汇之间的语义关系和上下文信息。为了解决这些问题，后续的模型，如词嵌入（Word Embeddings）和深度学习模型，被引入以更好地捕获语义信息。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;词袋模型（Bag of Words，简称BoW）是信息检索领域中常用的一种文本表示方法。它将文本表示为一个无序的词汇项集合，忽略了文本中词汇的顺序和语法结构，仅关注每个词汇在文本中出现的频率。&lt;/p&gt;</summary>
    
    
    
    <category term="论文学习" scheme="https://resume.kokomi0728.eu.org/categories/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="NLP" scheme="https://resume.kokomi0728.eu.org/tags/NLP/"/>
    
    <category term="信息检索" scheme="https://resume.kokomi0728.eu.org/tags/%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2/"/>
    
  </entry>
  
  <entry>
    <title>nDCG</title>
    <link href="https://resume.kokomi0728.eu.org/posts/bec2b2c5.html"/>
    <id>https://resume.kokomi0728.eu.org/posts/bec2b2c5.html</id>
    <published>2023-11-09T16:00:00.000Z</published>
    <updated>2023-11-17T14:22:56.337Z</updated>
    
    <content type="html"><![CDATA[<p>nDCG（Normalized Discounted Cumulative Gain）是一种用于评估信息检索和推荐系统排序质量的指标。它是对DCG指标的标准化扩展。</p><span id="more"></span><p>在信息检索和推荐系统中，排序是将相关性较高的项目（例如搜索结果或推荐项）排在前面的过程。DCG是一种度量排序质量的指标，它考虑了项目的相关性以及它们在排序列表中的位置。DCG通过对相关性进行折扣，根据项目在排序列表中的位置赋予不同的权重，然后将这些权重进行累加。</p><p>nDCG对DCG进行标准化，以便进行不同排序列表之间的比较。它通过将DCG除以在理想排序下的最大可能DCG，得到一个取值范围在0到1之间的归一化指标。理想排序是指将相关性最高的项目排在最前面的排序。</p><p>nDCG的计算方式如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nDCG = DCG / IDCG</span><br></pre></td></tr></table></figure><p>其中，DCG是折扣累计增益（Discounted Cumulative Gain），IDCG是在理想排序下的最大可能折扣累计增益（Ideal DCG）。</p><p>nDCG是一种常用的排序质量指标，广泛应用于信息检索、推荐系统和广告排序等领域。它能够反映出排序结果的相关性和顺序，从而帮助评估和比较不同排序算法或系统的性能。较高的nDCG值表示排序结果更符合用户的偏好和期望。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;nDCG（Normalized Discounted Cumulative Gain）是一种用于评估信息检索和推荐系统排序质量的指标。它是对DCG指标的标准化扩展。&lt;/p&gt;</summary>
    
    
    
    <category term="论文学习" scheme="https://resume.kokomi0728.eu.org/categories/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="NLP" scheme="https://resume.kokomi0728.eu.org/tags/NLP/"/>
    
    <category term="信息检索" scheme="https://resume.kokomi0728.eu.org/tags/%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2/"/>
    
  </entry>
  
  <entry>
    <title>高斯建模</title>
    <link href="https://resume.kokomi0728.eu.org/posts/86d8f8d0.html"/>
    <id>https://resume.kokomi0728.eu.org/posts/86d8f8d0.html</id>
    <published>2023-11-08T16:00:00.000Z</published>
    <updated>2023-11-17T14:20:05.123Z</updated>
    
    <content type="html"><![CDATA[<p>高斯建模（Gaussian modeling）是一种统计建模方法，基于高斯分布（也称为正态分布）对数据进行建模和分析。高斯分布在概率统计中非常常见，因为它具有许多有用的性质和应用。</p><span id="more"></span><p>高斯建模假设数据的分布服从高斯分布。高斯分布可以由其均值（mean）和方差（variance）来完全描述。它的概率密度函数（probability density function，PDF）在一维情况下可以表示为：</p><p>$$<br>f(x) &#x3D; \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}<br>$$</p><p>其中，$x$ 是随机变量的取值，$\mu$ 是均值，$\sigma^2$ 是方差。</p><p>高斯建模的目标是通过对数据进行观察和分析，估计数据的均值和方差，并基于高斯分布对未知数据进行预测和推断。高斯建模在许多领域中都有广泛的应用，包括统计学、机器学习、图像处理、金融领域等。</p><p>在实际应用中，高斯建模可以用于以下任务：</p><ol><li><p>数据建模和分析：通过拟合高斯分布参数，对数据进行建模和分析，了解数据的分布特征和统计性质。</p></li><li><p>异常检测：通过比较观测数据与高斯分布的概率密度，识别和检测数据中的异常值或离群点。</p></li><li><p>数据生成和合成：基于已知的高斯分布参数，生成符合相同分布的合成数据，用于模拟和测试。</p></li><li><p>数据预测和推断：基于已有数据的高斯建模，对未知数据进行预测和推断，如预测某个未来时间点的观测值。</p></li></ol><p>高斯建模是概率统计和机器学习中一个重要的基础方法，广泛应用于各种数据分析和模型构建的任务中。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;高斯建模（Gaussian modeling）是一种统计建模方法，基于高斯分布（也称为正态分布）对数据进行建模和分析。高斯分布在概率统计中非常常见，因为它具有许多有用的性质和应用。&lt;/p&gt;</summary>
    
    
    
    <category term="论文学习" scheme="https://resume.kokomi0728.eu.org/categories/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="数学" scheme="https://resume.kokomi0728.eu.org/tags/%E6%95%B0%E5%AD%A6/"/>
    
    <category term="概率论" scheme="https://resume.kokomi0728.eu.org/tags/%E6%A6%82%E7%8E%87%E8%AE%BA/"/>
    
  </entry>
  
</feed>
