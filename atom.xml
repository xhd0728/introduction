<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Haidong Xin</title>
  
  <subtitle>HomePage of Haidong Xin</subtitle>
  <link href="https://resume.kokomi0728.eu.org/atom.xml" rel="self"/>
  
  <link href="https://resume.kokomi0728.eu.org/"/>
  <updated>2023-12-08T12:41:51.689Z</updated>
  <id>https://resume.kokomi0728.eu.org/</id>
  
  <author>
    <name>Haidong Xin</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>【一院一节】第二届“贡橙杯”CTF竞赛圆满落幕</title>
    <link href="https://resume.kokomi0728.eu.org/posts/b324fdd1.html"/>
    <id>https://resume.kokomi0728.eu.org/posts/b324fdd1.html</id>
    <published>2023-12-04T16:00:00.000Z</published>
    <updated>2023-12-08T12:41:51.689Z</updated>
    
    <content type="html"><![CDATA[<p>【一院一节】第二届“贡橙杯”CTF竞赛圆满落幕</p><span id="more"></span><p>CTF（Capture The Flag，夺旗赛），是网络安全领域技术人员之间进行技术竞技的一种常见比赛形式。第二届“贡橙杯”CTF竞赛由哈尔滨工程大学计算机科学与技术学院主办，网络安全技术社团承办，绿盟科技集团股份有限公司赞助了本次比赛。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;【一院一节】第二届“贡橙杯”CTF竞赛圆满落幕&lt;/p&gt;</summary>
    
    
    
    <category term="公众号推文" scheme="https://resume.kokomi0728.eu.org/categories/%E5%85%AC%E4%BC%97%E5%8F%B7%E6%8E%A8%E6%96%87/"/>
    
    
    <category term="CTF" scheme="https://resume.kokomi0728.eu.org/tags/CTF/"/>
    
  </entry>
  
  <entry>
    <title>Universal-Guided-Diffusion</title>
    <link href="https://resume.kokomi0728.eu.org/posts/23f23e19.html"/>
    <id>https://resume.kokomi0728.eu.org/posts/23f23e19.html</id>
    <published>2023-12-01T16:00:00.000Z</published>
    <updated>2023-12-02T09:08:17.689Z</updated>
    
    <content type="html"><![CDATA[<p>典型的扩散模型经过训练以接受特定形式的条件作用（最常见的是文本），并且如果不经过重新训练就不能接受其他形式的条件的作用。<br>这项工作中提出了一种通用制导算法(universal guidance algorithm)，使扩散模型能够通过任意制导方式进行控制。<br>实验表明该算法成功生成了具有引导功能的高质量他想，包括分割(segmentation)，人脸识别(face recognition)，对象检测(object detection)，分类器信号(classifier signals)。</p><span id="more"></span><p>原文代码：github.com/arpitbansal297/Universal-Guided-Diffusion</p><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>扩散模型是生成图像的强大工具，当今大多数扩散模型都是通过调节，从头开始构建扩散模型来控制的，以接受用户输入的特定形式。调节本身很强大，但是会导致模型受到单一模式的束缚，有新需求后需要重新从头开始训练一个新的模型，代价很大。</p><p>控制模型输出的更灵活的办法是使用guidance. 这种方法下，扩散模型充当通用的图像生成器，不再需要理解用户的需求。用户将该模型与guidance配对，以衡量是否满足某些标准。如：可以指导模型最小化生成的图像和用户选择的文本描述之间的CLIP分数。<br>图像创建过程中，每次迭代期间，迭代都会沿着guidance的梯度向下推动，让最终的图像满足用户的需求。</p><p>本文中，研究了guidance指导方法，使任何现成的模型或损失函数都可以用作guidance。因为guidance函数无需重新训练或修改，这种形式的guidance具有普遍性，因为它使扩散模型能够适应于任何目的。<br>从用户的角度看，guidance优于调节的方式，因为guidance让扩散模型变成一个能够进行多种用途的通用图像生成器。不幸的是人们普遍不认为这个可行。早期的扩散模型依赖于分类器的指导，后来很快转向了无分类器的方案，该方案需要使用无法更改的特定冻结本体从头开始对模型进行类标签训练。 </p><p>使用guidance的困难源自于采样过程中使用的噪声图像与训练guidance模型之间的域差距(domain gap)。当这个差距缩小时，guidance就会表现得很好。例如，成功使用CLIP模型引导，但前提是使用噪声输入重新开始训练CLIP。噪声的重新训练可以缩小domain gap，但是在财力和工程上代价很高。为避免额外的成本，本文通过改变抽样方案而不是改变模型的方式缩小domain gap。</p><p>总结本文的贡献：</p><ol><li>提出了一种能够为扩散模型提供通用的guidance的方法。本文提出的采样器(sampler)仅在去噪图像上评估模型，而不是在潜在噪声状态上评估。这样可以缩小困扰标准guidance方法的domain gap。这个策略为最终用户提供了许多使用guidance模式甚至同时使用多种模式的灵活性。底层扩散模型保持固定，无需进行任何类型的微调。</li><li>演示了不同限制下这种方法的效果，包括分类器标签(classifier labels)，人物身份(human identities)，分割图(segmentation maps)，来自对象检测器的注释(annotations from object detectors)，还有由逆线性问题(inverse linear problems)产生的约束。</li></ol><p><img src="https://github.com/arpitbansal297/Universal-Guided-Diffusion/raw/main/stable-diffusion-guided/data/images/all_cover_figure.png" alt=""></p><p>图1：由现成的网络作为guidance的扩散</p><h1 id="2-Background"><a href="#2-Background" class="headerlink" title="2 Background"></a>2 Background</h1><p>首先简要回顾扩散模型背后核心框架的最新文献，然后定义受控图像生成的问题的设置并讨论了先前的相关工作。</p><h2 id="2-1-Diffusion-Models-扩散模型"><a href="#2-1-Diffusion-Models-扩散模型" class="headerlink" title="2.1 Diffusion Models 扩散模型"></a>2.1 Diffusion Models 扩散模型</h2><p>扩散模型在很多领域都展现了强大的图像生成能力。</p><p>我们正式引入（无条件）扩散，这有助于描述不同类型模型的细微差别。<br>一个扩散模型可以被定义为$T$步正向过程和$T$步反向过程。从概念上讲，正向过程逐渐将不同幅度的高斯噪声添加到干净的图像$z<em>0$，反向过程尝试对噪声输入进行去噪，希望恢复成干净的数据点。<br>更具体地，给定表示噪声的尺度 $\alpha</em>{t_{t=1}^T}$ 的标量数组和一个干净的图像 $z_0$ ，将前向过程的 $t$ 个步骤应用于 $z_0$ 会产生一个噪声数据</p><script type="math/tex; mode=display">z_t=\sqrt{\alpha_t}z_0+(\sqrt{1-\alpha_t})\epsilon,\ \epsilon\sim\mathcal{N}(0, \mathbf{I})</script><p>一个扩散模型学习了一个去噪网络$\epsilon_\theta$，对于每一个$(z_0,t)$的对和任何一个样本$\epsilon$，都有</p><script type="math/tex; mode=display">\epsilon_\theta(z_t,t)\approx\epsilon=\frac{z_t-\sqrt{\alpha_t}z_0}{\sqrt{1-\alpha_t}}</script><p>相反的过程采用 $q(z<em>{t-1}|z_t,z_0)$ 的形式，有很多种具体的定义，其中 $q(\cdot|\cdot)$ 通常参数化为高斯分布。不同的工作研究了 $q(z</em>{t-1}|z_t,z_0)$ 的不同近似值，如DDIM采用如下方法预测干净样本：</p><script type="math/tex; mode=display">\hat{z}_0=\frac{z_t-(\sqrt{1-\alpha_t})\epsilon_\theta(z_t,t)}{\sqrt{\alpha_t}}</script><p>然后用预测出来的 $\hat{z}<em>0$，利用 $q(z</em>{t-1}|z<em>t,\hat{z}_0)$ 采样 $z</em>{t-1}$ 。</p><p>虽然各个采样方法的细节各不相同，每一种采样方法都是基于当前的样本 $z<em>t$ ，步数 $t$ ，预测的噪声 $\hat{\epsilon}$ 来预测 $z</em>{t-1}$ 。简便起见，定义这个过程为 $z_{t-1}=S(z_t,\hat{\epsilon},t)$ 。</p><h2 id="2-2-Controlled-Image-Generation-受控图像生成"><a href="#2-2-Controlled-Image-Generation-受控图像生成" class="headerlink" title="2.2 Controlled Image Generation 受控图像生成"></a>2.2 Controlled Image Generation 受控图像生成</h2><p>本文关注具有各种约束的受控图像生成。考虑一个可微引导函数(guidance function) $f$，如CLIP特征提取器或一个分割网络。当应用于图像时，得到一个向量$c=f(x)$。还考虑一个函数$\mathit{l}(\cdot, \cdot)$，用于衡量$c$和$c’$的接近程度。给定一个特定选择$c$，我们称之为prompt，相应的约束(基于$c,\mathit{l},f$)被形式化为$\mathit{l}(c,f(z))\approx 0$，目标是从图像分布中生成一个满足约束的样本$z$。<br>换言之，想要生成一个与prompt相匹配的图像。</p><blockquote><p>分割网络(segmentation network)：是一种深度学习模型，专门用于图像分割任务。图像分割是计算机视觉领域中的重要任务，其目标是将输入图像中的每个像素标记为属于预定义类别的一个区域，从而将图像分解成具有语义信息的区域。</p></blockquote><p>之前的研究将受控扩散生成图像分为两个分类。称第一类为条件图像生成(conditional image generation)，第二类是引导图像生成(guided image generation)。</p><h3 id="2-2-1-Conditional-Image-Generation-条件图像生成"><a href="#2-2-1-Conditional-Image-Generation-条件图像生成" class="headerlink" title="2.2.1 Conditional Image Generation 条件图像生成"></a>2.2.1 Conditional Image Generation 条件图像生成</h3><p>这一类的方法需要将prompt作为额外的输入训练新的扩散模型。比如：</p><ul><li><em>Classifier-free diffusion guidance</em> (arXiv:2207.12598)提出classifier-free guidance使用类别作为prompt，通过去噪网络的无条件和条件输出之间的线性插值来训练扩散模型。</li><li><em>Cold diffusion: Inverting arbitrary image transforms without noise</em> (arXiv:2208.09392)研究了引导函数是已知线性退化算子(linear degradation operator)，并训练了条件模型来解决线性逆问题。</li><li><em>Towards photorealistic image generation and editing with text-guided diffusion models</em> (arXiv:2112.10741)进一步扩展了classifier-free guidance，用描述性词汇作为prompt，以文本条件生成图像(text-conditional image generation)，然后训练一个扩散模型去提高生成的图像的CLIP表示和文本提示之间的相似性。<br>这些方法在不同类型的约束下都是成功的，但是重新训练扩散模型的要求使得它们的计算量很大。 </li></ul><blockquote><p>线性退化算子(linear degradation operator)：通常是指在信号处理或图像处理中用于模拟信号或图像因受到特定系统或过程影响而发生退化的操作。这个算子通常被用来描述信号或图像在传输、采集或处理过程中所经历的变化或损失。线性退化算子可以通过数学方式表示，并被用于生成模拟退化的信号或图像，以便进行处理算法的测试、评估和改进。</p><p>CLIP表示(CLIP Representation)：CLIP模型为文本和图像所学习到的特征表示。这些表示被设计为能够捕捉文本描述和图像内容之间的语义相关性，即相似的文本和图像在表示空间中距离较近，而不相似的文本和图像在表示空间中距离较远。</p></blockquote><h3 id="2-2-2-Guided-Image-Generation-引导图像生成"><a href="#2-2-2-Guided-Image-Generation-引导图像生成" class="headerlink" title="2.2.2 Guided Image Generation 引导图像生成"></a>2.2.2 Guided Image Generation 引导图像生成</h3><p>这一类生成采用冻结的预训练扩散模型作为基础模型，但是修改采样方法来通过引导函数的反馈指导图像生成。本文的方法属于这一类。先前的工作是通过各种限制和外部引导函数来实现的。举例来说：</p><ul><li><em>Diffusion models beat gans on image synthesis</em> (arXiv:2105.05233)提出了classifier guidance，在不同噪声尺度的图像上训练分类器作为引导函数$f$，包括了采样过程中分类器的梯度。但是，噪声图像的分类器是特定于domain的，通常不容易获得。本文的方法规避了这个问题。</li><li><em>Zero-shot image restoration using denoising diffusion null-space model</em> (arXiv:2212.00490)假设外部引导函数为线性算子，并利用基础模型生成位于线性算子零空间的图像分量。然而扩展该方法来处理非线性引导功能并非易事。</li><li><em>Diffusion posterior sampling for general noisy inverse problems</em> (arXiv:2209.14687)研究了一般的引导函数，在预期去噪图像上计算的引导函数的梯度，以此来修改采样过程。尽管如此，作者仅给出了更简单的非线性引导函数（如非线性模糊）的结果。</li></ul><blockquote><p>非线性模糊(Non-linear Blurring)：一种图像处理技术，与传统的线性模糊（如高斯模糊）不同，它使用非线性方法来模糊图像，改变图像的外观或增强图像的某些特征。在非线性模糊中，模糊效果可能不是均匀的，不同部分的图像可能会受到不同程度或不同方式的模糊处理。这种模糊方法可能更适合于某些图像或场景，因为它可以保留一些细节或特定的图像特征，同时模糊其他部分。</p></blockquote><p>本文的工作研究通用引导算法(universal guidance algorithm)，用于使用任何现成的引导函数通过扩散模型生成引导图像，如目标监测或分割网络。</p><h1 id="3-Universal-Guidance"><a href="#3-Universal-Guidance" class="headerlink" title="3 Universal Guidance"></a>3 Universal Guidance</h1><p>本文提出了一种引导算法，增强了扩散模型的图像采样方法，以包括来自现成辅助网络的引导。<br>本算法的灵感来自于经验观察，即通过Eq.3获得的重建感性图像$z_0$。虽然不完美，但仍然适合通用引导，以提供信息反馈来指导图像生成。</p><ul><li>3.1节，通过扩展分类器引导来激发本文的前向通用引导(forward universal guidance)，以利用这种观察并处理通用引导功能。</li><li>3.2节，提出了一种补充性后向通用引导(backward universal guidance)，以帮助强制生成的图像满足基于引导函数$f$的约束。</li><li>3.3节，讨论了一种简单但有用的自递归技巧，能够凭借经验提高生成图像的保真度。</li></ul><h2 id="3-1-Forward-Universal-Guidance-前向通用引导"><a href="#3-1-Forward-Universal-Guidance-前向通用引导" class="headerlink" title="3.1 Forward Universal Guidance 前向通用引导"></a>3.1 Forward Universal Guidance 前向通用引导</h2><p>为了通过外部引导函数$f$和损失函数$\mathit{l}$作为信息引导生成，一个直接想法是扩展分类器引导以接受一般引导功能。<br>更具体地，给定类别prompt $c$ ，分类器引导在每一步采样 $S(z<em>t,t)$ 中用下式(Eq.4)替换 $\epsilon</em>\theta(z_t,t)$ 来执行分类引导采样：</p><script type="math/tex; mode=display">\hat{\epsilon}_\theta(z_t,t)=\epsilon_\theta(z_t,t)-\sqrt{1-\alpha_t}\nabla_{z_t} \log p(c|z_t)</script><p>定义 $\mathit{l}<em>{ce}(\cdot,\cdot)$ 为交叉熵损失(cross-entropy loss)， $f</em>{cl}$ 为引导函数，输出类别的概率，Eq.4可以改写成：</p><script type="math/tex; mode=display">\hat{\epsilon}_\theta(z_t,t)=\epsilon_\theta(z_t,t)+\sqrt{1-\alpha_t}\nabla_{z_t} \mathit{l}_{ce}(c,f_{cl}(z_t))</script><p>注： $\log p(c|z<em>t)$和$-\mathit{l}</em>{ce}(c,f_{cl}(z_t))$ 在数值上并不相等，但是都是用于衡量和目标标签 $c$ 的差距，所以起到的作用是相同的。</p><p>直接用任何现成的引导函数和损失函数替换 $f<em>{cl}$ 和 $\mathit{l}</em>{ce}$ 在实践中行不通，因为 $f$ 很可能是在干净图像上训练的，输入有噪声的图像无法提供有意义的指导。这时我们可以利用 $\epsilon_\theta(z_t,t)$ 预测添加到数据点的噪声，然后可以用Eq.3来获得一个预测的干净噪声 $\hat{z}_0$ 。这里建议根据预测的干净数据的点计算指导：</p><script type="math/tex; mode=display">\hat{\epsilon}_\theta(z_t,t)=\epsilon_\theta(z_t,t)+s(t)\cdot\nabla_{z_t} \mathit{l}(c,f(\hat{z}_0))</script><p>$s(t)$ 控制每一步的引导强度，</p><script type="math/tex; mode=display">\nabla_{z_t} \mathit{l}(c,f(\hat{z}_0))=\nabla_{z_t}\mathit{l}(c,f(\frac{z_t-\sqrt{1-\alpha_t}\epsilon_\theta(z_t,t)}{\sqrt{\alpha_t}}))</script><p>（这一步就是 $\hat{z}<em>0$ 用Eq.3替换）<br>用Eq.6来表示前向引导。<br>在实践中，应用前向引导有效地使生成的图像更接近prompt，同时保持数据流中的生成轨迹。<br>_Diffusion posterior sampling for general noisy inverse problems.</em> (arXiv:2209.14687)也是相关的方法，引导步长是基于 $E[z_0|z_t]$ 计算的。这个方法从基于分数的生成框架中获取灵感，但导致了不同的更新办法。</p><h2 id="3-2-Backward-Universal-Guidance-反向通用引导"><a href="#3-2-Backward-Universal-Guidance-反向通用引导" class="headerlink" title="3.2 Backward Universal Guidance 反向通用引导"></a>3.2 Backward Universal Guidance 反向通用引导</h2><p>如4.2节会展示的，前向引导有时会过分优先考虑维持图像的真实性，导致没能满足给定的prompt。简单增加引导强度$s(t)$并不是最优的，因为这通常会导致不稳定，图像离开簇(manifold)的速度比降噪器校正它的速度快。<br>为了解决这个问题，提出后向通用引导(后向引导)，以补充前向引导并帮助强制生成的图像满足约束。<br>后向引导的关键思想是根据 $\hat{z}_0$ 优化干净图像，使其最适合prompt，然后在步骤 $t$ 将引导变化线性转换回噪声图像空间。</p><p>具体地，不是直接计算 $\nabla_{z_t}\mathit{l}(c,f(\hat{z}_0))$ ，而是计算干净数据空间中的引导变化 $\Delta z_0$ ：</p><script type="math/tex; mode=display">\Delta z_0=\arg \min_{\Delta} \mathit{l}(c,f(\hat{z}_0+\Delta))</script><p>求解Eq.7，采用 $m$ 步梯度下降，其中使用 $\Delta=0$ 作为起点。当 $\hat{z}_0+\Delta z_0$ 将 $\mathit{l}(c,f(z))$ 最小化时， $\Delta z_0$ 是干净数据空间内最符合限制的变动。然后将 $\Delta z_0$ 变换回到第 $t$ 步的噪声数据空间中，通过计算满足Eq.8的引导去噪预测 $\tilde{\epsilon}$ ：</p><script type="math/tex; mode=display">z_t=\sqrt{\alpha_t}(\hat{z}_0+\Delta z_0)+\sqrt{1-\alpha_t}\tilde{\epsilon}</script><p>使用Eq.3，可以重写 $\tilde{\epsilon}$ 作为原始去噪预测 $\epsilon_\theta(z_t,t)$ 的增强：</p><script type="math/tex; mode=display">\tilde{\epsilon}=\epsilon_\theta(z_t,t)-\sqrt{\alpha_t/(1-\alpha_t)}\Delta z_0</script><p>相比于前向引导，反向引导(如Eq.9)为生成图像提供了一个优化方向以匹配给定的prompt，因此优先考虑符合约束条件。此外，计算Eq.7的梯度的步长的成本比计算前向传播Eq.6要小，而且我们可以用多步梯度下降来求解Eq.7，进一步提高与给定prompt的匹配度。</p><h2 id="3-3-Per-step-Self-recurrence-逐步自递归"><a href="#3-3-Per-step-Self-recurrence-逐步自递归" class="headerlink" title="3.3 Per-step Self-recurrence 逐步自递归"></a>3.3 Per-step Self-recurrence 逐步自递归</h2><p>当我们将通用引导加到普通的生成pipeline中时，我们发现生成的图像和自然图像相比具有很奇怪的特征。我们尝试通过减少 $s(t)$ 来优先考虑真实性，但没有效果。尤其在复杂的引导函数情况下，既能保证真实性，又能满足引导限制的方法并不总是存在。我们猜想，由通用方法产生的引导方向并不总和真实性挂钩，尤其是在引导函数产生了过多信息损失的时候，这导致了图像逐步脱离了生成真实性图像的正轨。</p><p>为了解决问题，本文应用了逐步自递归(per-step self-recurrence)。更具体地，当 $z<em>{t-1}=S(z_t,\hat{\epsilon}_t,t)$ 被采样后，我们向 $z</em>{t-1}$ 重新注入随机高斯噪声 $\epsilon{‘}\sim \mathcal{N}(0,\mathbf{I})$ ，以此获得 $z_t^{‘}$ ：</p><script type="math/tex; mode=display">z_t^{'}=\sqrt{\alpha_t/\alpha_{t-1}}\cdot z_{t-1}+\sqrt{1-\alpha_t/\alpha_{t-1}}\cdot\epsilon^{'}</script><p>Eq.10保证了在第 $t$ 步中 $z_t^{‘}$ 具有合适的噪声尺度。在采样第 $t-1$ 步时，我们重复进行自递归 $k$ 次。</p><p>直观地说，自递归过程允许在相同的噪声尺度下，不断探索数据簇中的不同区域，允许使用更多的计算资源去找到一个既满足引导约束又保证图像质量的解决方案。<br>经验上来说，我们发现自递归过程可以在给定合适引导强度 $s(t)$ 以生成符合prompt的图像时保证足够的真实性。</p><p>图2展示了自递归对生成图像带来的作用。</p><p><img src="https://cdn.jsdelivr.net/gh/xhd0728/oss-github-picgo-repository/picgo/20231202162635.png" alt=""></p><p>图2：自递归过程在分割图像生成的应用。最左边的图像是给定的分割map，后面三个是自递归步数分别为1，4，10的结果。</p><p>我们在算法1中总结了通用引导算法（前向通用引导、反向通用引导、逐步自递归）。为了简单起见，算法1假设引导函数只有1个，但是可以轻易改成具有多对$(f,l)$。此外，前向和后向引导的目标不必相同，允许以不同的方式同时利用多种引导功能。</p><blockquote><p>算法1：通用引导<br>Parameter: 自递归步数$k$, 反向引导梯度步数$m$, 引导强度$s(t)$<br>Required: 从N(0,1)采样的$z<em>T$，扩散模型$\epsilon</em>\theta$，噪声尺度${\alpha<em>t}</em>{t=1}^T$，引导函数$f$，损失函数$l$，prompt $c$<br>for $t=T, T-1,\cdots,1$ do  (T步去噪)<br>    for $n=1,2,\cdots,k$ do  (k步自递归)<br>        利用Eq.3计算$\hat{z}<em>0$<br>        利用前向通用扰动Eq.6计算$\hat{\epsilon}</em>\theta$<br>        if $m&gt;0$ then<br>            使用$m$步梯度下降，求得最小化$Eq.7$的$\Delta z<em>0$<br>            使用Eq.9计算反向通用扰动 ($\hat{\epsilon}</em>\theta\leftarrow\hat{\epsilon}<em>\theta-\sqrt{\alpha_t/(1-\alpha_t)}\Delta z_0$)<br>        end if<br>        $z</em>{t-1}\leftarrow S(z<em>t,\hat{\epsilon}</em>\theta,t)$<br>        $\epsilon^{‘}\sim\mathcal{N}(0,I)$<br>        $z<em>t\leftarrow\sqrt{\alpha_t/\alpha</em>{t-1}}z<em>{t-1}+\sqrt{1-\alpha_t/\alpha</em>{t-1}}\epsilon^{‘}$<br>    end for<br>end for</p></blockquote><h1 id="4-Experiments"><a href="#4-Experiments" class="headerlink" title="4 Experiments"></a>4 Experiments</h1><p>本节展示了本文提出的通用引导算法在各种引导函数上的测试结果。具体地，在Stable Diffusion(一种基于文本条件生成的扩散模型)进行了实验，也在一个在ImageNet上训练的完全无条件的扩散模型上做了对比（使用了OpenAI提供的预训练模型）进行了实验。需要注意的是，Stable Diffusion也可以进行无条件的生成，只需要给定的文本是一个空字符串。</p><ul><li>4.1节是使用不同引导函数在Stable Diffusion上做的实验结果</li><li>4.2节是在ImageNet上训练的扩散模型的实验结果</li></ul><h2 id="4-1-Results-for-Stable-Diffusion"><a href="#4-1-Results-for-Stable-Diffusion" class="headerlink" title="4.1 Results for Stable Diffusion"></a>4.1 Results for Stable Diffusion</h2><p>本节将Stable Diffusion作为基础模型。实验中用到的引导函数由CLIP特征提取器，一个分割网络，脸部识别网络和目标检测网络。实验中发现应用前向引导足以产生符合给定prompt的高质量图像，因此设置$m=0$。为了对Stable Diffusion进行前向引导，我们将通过Eq.3计算出的潜变量前向给Stable Diffusion的图像解码器去获得预测的干净图像。下面的每个小节将讲述实验结果与对应的实现细节。</p><h3 id="4-1-1-CLIP-Guidance"><a href="#4-1-1-CLIP-Guidance" class="headerlink" title="4.1.1 CLIP Guidance"></a>4.1.1 CLIP Guidance</h3><p>CLIP是由OpenAI开发的目前最好的文本到图像的生成模型。为了将我们的算法应用到文本引导的图像生成，我们使用了CLIP的特征提取器作为引导函数。我们利用对于给定prompt得到的图像嵌入与CLIP文本嵌入之间的负余弦相似度构建损失函数。使用$s(t)=10\sqrt{1-\alpha_t}$、$k=8$，使用Stable Diffusion作为无条件图像生成器。</p><blockquote><p>文本到图像相似度模型(text-to-image similarity model)：是一种可以衡量文本描述和图像内容之间相似性的模型。这种模型的目标是理解自然语言描述（如句子、短语或段落）与对应图像之间的语义关联，并为它们分配一个相似度得分或概率。</p><p>余弦相似度(Cosine Similarity)：是衡量两个非零向量之间相似度的一种度量方式。它衡量的是两个向量的夹角的余弦值，范围在$[-1, 1]$之间，值越接近 1 表示两个向量越相似，值越接近 -1 表示两个向量越不相似。$Similarity(A,B)=\frac{A\cdot B}{||A||\ ||B||}$</p></blockquote><p>我们使用一定数量的文本prompt作为引导生成图像。为了更好地评估我们的通用引导算法并对比各种条件和引导，我们还用Stable Diffusion，用与输入相同的prompt生成了经典的(classical)、文字条件的(text-conditional)两种生成方式生成图像，然后将总结的结果放在图3。</p><p><img src="https://cdn.jsdelivr.net/gh/xhd0728/oss-github-picgo-repository/picgo/20231202162753.png" alt=""></p><p>图3：我们比较了通用引导算法和从头开始训练的文本条件模型之间匹配给定文本prompt的能力。结论表明，通用指导算法在生成满足文本约束的高质量图像的能力方面可以和专门的条件模型相媲美。</p><h3 id="4-1-2-Segmentation-Map-Guidance"><a href="#4-1-2-Segmentation-Map-Guidance" class="headerlink" title="4.1.2 Segmentation Map Guidance"></a>4.1.2 Segmentation Map Guidance</h3><p>为了使用分割图作为prompt来引导图像生成，我们使用带有分割头的MobileNetV3-Large，以及Pytorch中公开可用的预训练模型。分割网络输出每一个元素的分类概率，我们通过对给定prompt和生成的图像的预测分割之间的逐像素交叉熵损失，并对每个像素求和，来构建损失函数 $l$ 。我们设置 $s(t)=400\cdot\sqrt{1-\alpha_t}$ ， $k=10$ 。</p><p>实验中我们将不同形状对象的分割图与新的prompt结合起来。我们使用文本prompt作为Stable Diffusion的固定附加输入来执行文本条件采样，并引导文本条件生成的图像匹配给定的分割图。结果在图4中展示。从图4中可以看出，生成的图像在目标和北京之间有一个明显的边界，并且几乎完美地符合了给定的分割图。生成的目标和背景也依旧符合描述的文本（如狗的品种和环境的描述）。而且生成的图像都很逼真。</p><p><img src="https://cdn.jsdelivr.net/gh/xhd0728/oss-github-picgo-repository/picgo/20231202162938.png" alt=""></p><p>图4：除了匹配文本prompt以外，还由图像分割管道引导。每列包含为匹配prompt生成的图像示例和最左侧列中的分割图。最上面的行包含在没有引导的情况下生成的示例。</p><h3 id="4-1-3-Face-Recognition-Guidance"><a href="#4-1-3-Face-Recognition-Guidance" class="headerlink" title="4.1.3 Face Recognition Guidance"></a>4.1.3 Face Recognition Guidance</h3><p>为了引导图像生成以模仿特定人的脸部，我们编写了一个结合了人脸检测模块和人脸识别模块的引导功能。此设置从输入面部图像生成面部属性嵌入。我们使用多任务级联卷积网络(multi-task cascaded convolutional networks, MTCNN)作为人脸检测模块，使用facenet作为人脸识别模块。<br>引导函数$f$裁剪出检测到的面部并输出面部属性嵌入作为prompt，同时使用$l_1$-loss作为嵌入之间的损失函数。<br>需要注意的是，为了计算算法中的引导方向，我们仅通过facenet进行反向传播，并将MTCNN生成的面部裁剪掩模视为MTCNN的oracle输入。因为MTCNN使用不可微分的非极大值抑制。<br>这个实验中我们设置 $s(t)=20000\cdot\sqrt{1-\alpha_t}$ ， $k=2$ 。</p><blockquote><p>多任务卷积神经网络(Multi-task Cascaded Convolutional Neural Network)：一种用于人脸检测和对齐的神经网络模型。</p><p>Oracle输入(Oracle input)：通常用于描述一种理想化的情况或者模型的假设，即在某些特定情况下，系统或模型能够获得完美、准确的信息。具体来说，在机器学习或算法设计中，“Oracle 输入”指的是一个假设的输入源，它可以提供对于某个问题的最优解或者真实答案。这种理想化的输入源可以为算法提供完美的信息，使其能够在没有任何错误或者不确定性的情况下进行决策或者预测。<br>在实际情况下，”Oracle 输入”通常被用作理论上的思考工具或者是对比实验中的基准。</p></blockquote><p>我们探索了面部引导和文本prompt的不同组合方式。与分割图任务相似，我们使用文本prompt作为Stable Diffusion的固定附加调节，使用我们的算法引导此文本条件轨迹，以便生成的图像中的面部看起来和面部prompt相似。图5中，我们清晰地看出，在生成的图像上，给定面部prompt的面部特征被重现得非常完美。背景、材质或风格得描述性文本也能正确实现，并与生成的面孔完美融合。</p><p><img src="https://cdn.jsdelivr.net/gh/xhd0728/oss-github-picgo-repository/picgo/20231202163036.png" alt=""></p><p>图5：除了匹配文本prompt，这些图像还由面部识别系统引导。每列包含为匹配prompt而生成的图像示例以及最左列中图像的标识。最上面的行包含在没有指导的情况下生成的示例。</p><h3 id="4-1-4-Object-Location-Guidance"><a href="#4-1-4-Object-Location-Guidance" class="headerlink" title="4.1.4 Object Location Guidance"></a>4.1.4 Object Location Guidance</h3><p>对于Stable Diffusion，我们使用目标检测网络引导生成图像同样获得了不错的结果。实验中，我们使用带有Resnet-50-FPN的backbone的Faster-RCNN，以及Pytorch中公开可用的预训练模型作为目标检测器。我们使用带有类别标签的边界框作为目标位置prompt。我们使用三个独立的损失的和构建损失函数$l$。这三个损失分别为：</p><ul><li>锚分类损失(anchor classification loss)</li><li>边界框回归损失(bounding box regression loss)</li><li>区域标签分类损失(region label classification)<br>其中锚分类损失和边界框回归损失在区域建议头(region proposal head)计算，区域标签分类损失在区域分类头(region classification head)计算。<br>需要注意：和基础的R-CNN的训练对比，我们丢掉了额外的区域分类头中的边界框对齐损失(bounding box alignment loss)。<br>我们发现这种损失构建帮助我们在每一个prompt指定的位置上都生成了正确类别目标。<br>我们设置 $s(t)=100\cdot\sqrt{1-\alpha_t}$ ， $k=3$ 。</li></ul><blockquote><p>区域建议头(region proposal head)：深度学习目标检测模型中的一个组件，通常出现在一种流行的架构中，叫做Faster R-CNN。在Faster R-CNN中，整个模型被分为两个主要部分：一个用于生成候选区域(Region Proposal Network，RPN)，另一个用于分类和精细调整这些提议的区域(Region Classification and Regression Head)。<br>区域建议头是指对这些候选区域进行进一步处理的部分，它接受候选区域作为输入，并对其进行分类（识别区域内的对象类别）和回归（精细调整候选区域的位置）。这一部分的输出是最终目标检测框架的一部分，用于确定每个提议区域内是否包含目标以及它们的确切位置。</p><p>区域分类头(region classification head)：是目标检测模型中的一个组件，通常用于对提议的候选区域进行目标分类。它接受提议的区域作为输入，并对其内部的物体或目标进行识别分类。通常，这个组件会采用卷积神经网络（CNN）或其他类型的神经网络结构，将候选区域中的特征映射到各个可能的类别上。</p><p>边界框对齐损失(bounding box alignment loss)：是目标检测任务中用于训练模型的损失函数之一。这种损失函数旨在帮助模型准确地预测目标边界框的位置。它衡量模型预测的边界框和真实标签之间的差异，并尝试最小化这种差异，以便模型能够更精确地预测目标的位置。</p></blockquote><p>我们使用了不同的文本prompt和目标位置prompt的组合又进行了实验，用同样的方法使用文本prompt作为Stable Diffusion的矫正条件。使用通用引导算法，我们基于给定的文本prompt和位置prompt生成了多个图像，结果展示在图6。从图6中可以看出，由描述文本指定的目标都出现在指定的位置框当中，并且具有给定边界框指示的适当尺寸。每一个位置都被生成的高质量图像合适地填充，与各种图像内容prompt保持一致。</p><p><img src="https://cdn.jsdelivr.net/gh/xhd0728/oss-github-picgo-repository/picgo/20231202163125.png" alt=""></p><p>图6：除了匹配prompt之外，这些图像还由对象检测器引导。每列包含为匹配prompt生成的图像示例以及用于引导的边界框。顶行包含在没有引导的情况下生成的示例。</p><h3 id="4-1-5-Style-Guidance"><a href="#4-1-5-Style-Guidance" class="headerlink" title="4.1.5 Style Guidance"></a>4.1.5 Style Guidance</h3><p>最终，基于一张风格图像作为风格参考，通过引导图像生成让Stable Diffusion生成图像来结束整个实验。通过CLIP的图像特征提取器从风格图像中捕获参考风格，使用生成的图像嵌入作为prompt。损失函数计算生成图像的嵌入和风格图像的嵌入之间的负余弦相似度。与之前的实验类似，使用文本输入作为Stable Diffusion的固定条件来控制生成的内容。我们对不同的风格图像和不同的文本prompt进行组合进行了多项实验，结果见图7。从图7可以看出，生成的图像的内容和给定的文本prompt相似，展示的风格和给定的风格图像相近。实验中我们设置$s(t)=6\cdot\sqrt{1-\alpha_t}$，$k=6$。</p><p>更进一步，为了控制内容量，我们设置了尺度$\gamma$，Stable Diffusion中用来平衡文本条件生成和非文本条件生成的平衡尺度。每列分别设置为3.0, 3.0, 4.0。</p><p><img src="https://cdn.jsdelivr.net/gh/xhd0728/oss-github-picgo-repository/picgo/20231202163214.png" alt=""></p><p>图7：除了匹配文本prompt，这些图像被风格图像所引导。每一列包含匹配文本prompt和用于引导的风格图像的生成的图像。第一行的生成示例没有受到风格引导。</p><h2 id="4-2-Results-for-ImageNet-Diffusion"><a href="#4-2-Results-for-ImageNet-Diffusion" class="headerlink" title="4.2 Results for ImageNet Diffusion"></a>4.2 Results for ImageNet Diffusion</h2><p>本节展示了使用在ImageNet上训练的无条件扩散模型上的引导图像生成。我们在目标位置引导和混合引导图像引导生成任务上做了实验，称为分割引导修复(segmentation-guided inpainting)。我们还在附录中提供了使用CLIP guidance的其他实验。我们会分别讨论每个guidance的结果。</p><blockquote><p>分割引导修复(segmentation-guided inpainting)：一种图像修复技术，结合了图像分割和修复方法，以在图像中去除或填补缺失的部分。基本思想是利用图像分割算法来识别和定位图像中需要修复的区域，并利用图像修复或补全技术来填补这些区域。通常情况下，首先使用图像分割算法（如语义分割模型）对图像进行处理，识别出需要修复的部分，然后针对这些区域使用图像修复算法来进行修复或填补，使其外观自然并与周围的环境相匹配。</p></blockquote><h3 id="4-2-1-Object-Location-Guidance"><a href="#4-2-1-Object-Location-Guidance" class="headerlink" title="4.2.1 Object Location Guidance"></a>4.2.1 Object Location Guidance</h3><p>和Stable Diffusion的实验很像，我们使用同样的网络结构和同样的预训练模型作为目标检测网络，构建一个相同的损失函数$l$。但是，和Stable Diffusion不同的是，目标位置是可用于引导生成的唯一的prompt。<br>在这个实验中，我们设置 $s(t)=100\sqrt{1-\alpha_t}$ ， $k=3$ 。我们使用了不同的目标位置prompt：</p><ul><li>只有前向通用引导</li><li>既有前向通用引导，又有反向通用引导<br>从图8可以观察到既有前向通用引导，又有反向通用引导的生成方式更加逼真，也更加符合prompt。另一方面，如果只有前向通用引导，生成的图像保留了逼真，但是和prompt的类别和位置不太匹配。这个结果演示了我们通用引导算法的有效性，也证明了反向引导的必要性。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/xhd0728/oss-github-picgo-repository/picgo/20231202163247.png" alt=""></p><p>图8：使用无条件的 ImageNet 模型进行目标检测引导的生成。同时使用前向和后向引导生成的图像逼真，并且在指定的位置具有所需的物体。相比之下，仅使用前向引导生成的图像展示出错误类别的物体或位置/大小不准确。</p><h3 id="4-2-2-Sgementation-Guided-Inpainting"><a href="#4-2-2-Sgementation-Guided-Inpainting" class="headerlink" title="4.2.2 Sgementation-Guided Inpainting"></a>4.2.2 Sgementation-Guided Inpainting</h3><p>本实验中，我们的目标是探索我们的算法能否掌控多种引导函数。我们将修复遮罩(inpainting mask)，分类器(classifier)和一个分割网络(segmentation network)组合组合在一起进行引导生成。我们首先生成了带有遮罩区域的图像作为prompt。然后，我们选择一个物体类别 c 作为分类的提示，并生成一个分割掩码，其中被掩盖的区域被视为同一类别 $c$ 的前景对象。对非掩码区，使用 $l_2$ 损失作为修复的损失函数，并将 $s(t)=0$ ，或者等价地仅使用反向引导进行修复。我们使用与4.1描述的分割网络相同的结构，设置 $s(t)=200\sqrt{1-\alpha_t}$ 。对于分类引导，我们使用接受噪声输入的分类器，并执行原始分类器来引导Eq.4，来代替前向指引。结果在图9中展示。 可以看出，使用inpainting和分类器作为引导，我们的算法生成的图像既逼真，又能满足inpainting prompt，并被正确分类。添加分割引导后，我们的算法和分割图、修复prompt都近乎完美匹配，同时保持真实感，进一步改进了生成的图像。这表明我们的算法可以有效结合各个引导功能的反馈。</p><p><img src="https://cdn.jsdelivr.net/gh/xhd0728/oss-github-picgo-repository/picgo/20231202163325.png" alt=""></p><p>图9：我们的引导算法可以整合来自多个引导函数的反馈。第一列展示了修复的提示。第二列展示了分类器引导的修复，在此生成了与修复提示紧密匹配的狗的图像。第三列展示了同时使用分类器和分割引导生成的图像，在掩码区域精确生成了逼真的狗的图像。结果表明我们的算法有效地处理了多个引导函数。</p><h1 id="5-Conclusion"><a href="#5-Conclusion" class="headerlink" title="5 Conclusion"></a>5 Conclusion</h1><p>本文提出了通用引导算法，可以利用任何现成的基于固定基础扩散模型的引导函数执行引导图像生成。我们的算法只需要引导和损失函数是可微的，并避免了对引导函数或基础模型进行重新训练以适应特定类型提示的需求，我们展示了我们的算法在复杂引导（包括分割、人脸识别、物体检测系统）方面有预期的结果，甚至可以结合多个引导函数并同时使用。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;典型的扩散模型经过训练以接受特定形式的条件作用（最常见的是文本），并且如果不经过重新训练就不能接受其他形式的条件的作用。&lt;br&gt;这项工作中提出了一种通用制导算法(universal guidance algorithm)，使扩散模型能够通过任意制导方式进行控制。&lt;br&gt;实验表明该算法成功生成了具有引导功能的高质量他想，包括分割(segmentation)，人脸识别(face recognition)，对象检测(object detection)，分类器信号(classifier signals)。&lt;/p&gt;</summary>
    
    
    
    <category term="论文学习" scheme="https://resume.kokomi0728.eu.org/categories/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="计算机视觉" scheme="https://resume.kokomi0728.eu.org/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    <category term="扩散模型" scheme="https://resume.kokomi0728.eu.org/tags/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B/"/>
    
  </entry>
  
  <entry>
    <title>REPLUG: Retrieval-Augmented Black-Box Language Models</title>
    <link href="https://resume.kokomi0728.eu.org/posts/a8d6c668.html"/>
    <id>https://resume.kokomi0728.eu.org/posts/a8d6c668.html</id>
    <published>2023-11-30T16:00:00.000Z</published>
    <updated>2023-12-01T13:04:30.913Z</updated>
    
    <content type="html"><![CDATA[<p>23年的这篇论文提出了检索增强的新范式，即<strong>REPLUG</strong>。它将语言模型当作一个黑盒子，即冻结语言模型的参数不再优化，转而去优化检索组件让检索组件来适配语言模型，以此来消除语言模型的“幻觉”，减少事实性错误的生成。</p><p><strong>关键词：检索增强、向量检索、困惑度、幻觉、事实性错误</strong></p><span id="more"></span><h2 id="1-LLM的幻觉"><a href="#1-LLM的幻觉" class="headerlink" title="1. LLM的幻觉"></a>1. LLM的幻觉</h2><p><strong>GPT-3</strong>等<strong>LLM</strong>（large language models，大语言模型）能力很强，在各个任务上都有非常不错的表现。但是，在LLM强大的背后，有着很多不能被忽视的缺陷。</p><p>首先，<strong>LLM对资源的要求高</strong>。比如，假设我们微调BLOOM-176B，就需要72张显存为80GB的A100显卡。没钱没资源或者资源有限的我们除流下两行热泪外，也可以退而求其次，去使用一些小模型（附：GPT-1到GPT-3的参数量是越来越大，从5GB到45TB，你仔细品一品这个趋势😂）。</p><p>LLM的另一个问题是<strong>生成文本中的事实性错误</strong>。在闲聊场景下，大家对这种事实性错误的容忍度还比较高，顶多笑一笑，在微信上和自己的朋友嘀咕两句，或者转头跟同事调侃一下“也不过如此”而已；可是在非闲聊场景下，比如医学、金融、军事等，事实性错误就是不可容忍的。在这些场景下落地，要求通常都是非常苛刻的，否则后果难以承受。针对这个问题，我们则要想办法去消除大模型中的事实错误。</p><p><strong>LLM</strong>的事实性错误有一个更通用的术语叫“<strong>幻觉</strong>”（<strong>Hallucination</strong>）。“幻觉”一词最早用于图像合成等领域，后来在图像检测中用于描述检测到的虚假或错误目标等。在自然语言生成 (NLG) 任务中，“幻觉“是用来指代我们所说的”事实性错误”。</p><h2 id="2-检索增强范式的转变"><a href="#2-检索增强范式的转变" class="headerlink" title="2. 检索增强范式的转变"></a>2. 检索增强范式的转变</h2><h3 id="2-1-“白盒”式检索增强"><a href="#2-1-“白盒”式检索增强" class="headerlink" title="2.1 “白盒”式检索增强"></a>2.1 “白盒”式检索增强</h3><p>语言模型生成的文本自然流畅、语法正确，就像人说出来的一样。但实际上，可能毫无意义并且包含虚假信息。这样的文本以假乱真，就像人产生的幻觉一样。针对这个问题，学术界也有相应的解决办法，即<strong>检索增强</strong>。</p><p>其实，检索方法的重度使用区是开放域问答，近几年关于开放域问答的研究多采用<strong>信息检索+机器阅读理解二阶段范式</strong>：</p><ul><li><p>检索器从已有知识库（如维基百科等网页）中找到相关知识；</p></li><li><p>利用机器阅读理解算法从相关知识中找到问题的答案。</p></li></ul><p>同样地，在文本生成领域，<strong>检索增强语言模型</strong>的范式通常也包含两个步骤：</p><ul><li><p>检索器根据用户query（用户输入）从已有知识库获得相关知识；</p></li><li><p>生成器根据用户query和检索到的辅助知识进行最终预测。</p></li></ul><p>在以往的检索增强范式下，语言模型通常都被当作“<strong>白盒</strong>”，简单地说就是我们要么优化它们的参数（比如Atlas联合优化了检索器和语言模型），要么获得其表征。考虑到大语言模型对资源的要求，这条路线的代价是比较大的。</p><p>除此以外，还有一点让“白盒”的“白”不可行：很多大语言模型并没有开源。比如，我们目前只能<strong>通过网页或者OpenAI提供的API去访问</strong>ChatGPT*<em><strong>，这种未开放的大模型自然是无法微调的。在这种情形下，语言模型倒更像一个“</strong>黑盒</em>*”，我们只能喂给它们输入，然后从它们那里获得结果。</p><p>总的来说就是，<strong>越来越大的语言模型规模和语言模型的黑盒特性使得白盒这种检索增强范式不可行</strong>。</p><h3 id="2-2-“黑盒”式检索增强"><a href="#2-2-“黑盒”式检索增强" class="headerlink" title="2.2 “黑盒”式检索增强"></a>2.2 “黑盒”式检索增强</h3><p>今天我们要分享的这篇工作所提出的 <strong>REPLUG</strong> 模型可以说是<strong>“黑盒”式检索增强</strong>的代表。在这种新范式下，语言模型是一个黑盒子（它的参数被冻结了，不会被更新优化），<strong>检索组件才是可微调的部分</strong>。</p><p><img src="https://cdn.jsdelivr.net/gh/xhd0728/oss-github-picgo-repository/picgo/20231201170615.png"></p><p>如上图所示，简单地理解就是在“白盒”范式下，假设检索组件不优化，也就是我们确信检索组件检索到的相关文本是足够准确的，即喂给语言模型的输入文本所含的信息丰富且准确，并且对每一个用户query（用户输入）来说，检索出来的文本确定的，我们需要<strong>优化语言模型</strong>让它产生的输出更正确，也就是<strong>调节语言模型来适配检索组件</strong>；</p><p>当然“白盒”模式下，<strong>检索组件也可以一并优化</strong>，但是这个相应地会更复杂。因为检索组件一旦被训练，检索出的文本和用户query的表示就会改变，那么被选出来的文本也就变了。什么时候重新编码一遍所有文本又是一个待考量的问题。</p><p>“黑盒”范式下，语言模型的参数不会被优化，我们<strong>要优化的是检索组件</strong>从而确保检索出的知识更好，也就是给到语言模型的输入更好，这样语言模型产生的输出才更正确，也就是<strong>调节检索组件来适配语言模型</strong>。</p><h2 id="3-REPLUG-REPLUG-LSR"><a href="#3-REPLUG-REPLUG-LSR" class="headerlink" title="3. REPLUG &amp; REPLUG LSR"></a>3. REPLUG &amp; REPLUG LSR</h2><p><strong>RERLUG</strong>(Retrieve and Plug) 其实就是在语言模型上额外加了一个检索组件，利用检索组件获得一些相关信息，与原始输入一起作为语言模型的新输入，检索组件和语言模型都不需要训练。（个人认为某种程度上检索出来的加到原始输入即用户query上的这些文本有点像prompt）。</p><p><strong>RERLUG LST</strong>（RERLUG with LM-Supervised Retrieval）可以看作是 <strong>RERLUG <strong>的升级版，它</strong>利用语言模型产生监督信号，从而去优化检索组件</strong>，让检索组件倾向于挑选出能够<strong>降低语言模型所生成文本的困惑度</strong>。</p><h3 id="3-1-REPLUG"><a href="#3-1-REPLUG" class="headerlink" title="3.1 REPLUG"></a>3.1 REPLUG</h3><p><img src="https://cdn.jsdelivr.net/gh/xhd0728/oss-github-picgo-repository/picgo/20231201170709.png"></p><p>如上图所示，<strong>RERLUG</strong>主要包含两个步骤：</p><ul><li><p>利用<strong>检索器</strong>从外部语料库中检索出与用户query相关的文档集</p></li><li><p>接着，分别将每个文档内容与用户query拼接然后送入语言模型（并行），最后再<strong>集成</strong>预测的概率。</p></li></ul><p>下面，我们来分别介绍这两个核心部分。</p><h4 id="3-1-1-检索器"><a href="#3-1-1-检索器" class="headerlink" title="3.1.1 检索器"></a>3.1.1 检索器</h4><p><strong>REPLUG</strong> 使用 <strong>双塔结构的向量检索器</strong>（原文：a dense retriever based on the dual encoder architecture）。</p><p>所谓<strong>双塔&#x2F;双编码器网络</strong>其实就是将用户query和检索文档 <strong>分别编码</strong>，然后再利用 cosine 等距离函数计算二者的<strong>相似度</strong>。具体又可分为<strong>SDE</strong>（Simese dual encoder）和 <strong>ADE</strong>（Asymmetric dual encoder）：</p><ul><li><p><strong>SDE</strong>：虽说是双塔，但实际上用户query和检索文档共用一套参数，；</p></li><li><p><strong>ADE</strong>：编码用户query和检索文档的编码器共享部分参数或者完全不共享参数，是两套独立的参数网络。</p></li></ul><p><strong>不同于BERT等典型的交互式网络，SDE</strong> 和 <strong>ADE</strong> 的共同点是都不会进行深层交互。<strong>双塔结构一个最典型的应用是召回or粗排，也就是适用于对计算速度要求严格的场景</strong>。</p><p>根据论文的描述，可以知道 <strong>REPLUG <strong>采用的是</strong>SDE</strong>，即文档 $d$ 和用户query $x$ 都使用同一编码器进行向量化。具体地，在最后一层隐藏表示上利用<strong>平均池化</strong>获得文档 $d$ 的编码 $E(d)$；同理，获得输入 $x$ 的编码 $E(x)$，然后利用 <strong>cosine</strong> 计算二者相似度：</p><p>$$s(d,x)&#x3D;cos(E(d),E(x))$$</p><p>最后根据 $s(d,x)$选出 k 个最相关的文档。</p><p>显然，我们需要把用户query $x$ 和所有文档 $d \in D$ 比较；$D$ 是一个相当大的规模的数据，这个速度比较慢，所以 <strong>RERLUG <strong>使用了</strong>FAISS</strong>（可快速寻找k个最相似向量的系统，需要预先将所有文档编码）来加速。</p><h4 id="3-1-2-重构输入，加权集成概率"><a href="#3-1-2-重构输入，加权集成概率" class="headerlink" title="3.1.2 重构输入，加权集成概率"></a>3.1.2 重构输入，加权集成概率</h4><p>前面我们说过 <strong>REPLUG <strong>是分别将每个文档内容与用户query拼接，然后送入语言模型（并行），最后再</strong>集成</strong>预测的概率。那为什么要这么做呢？</p><p><strong>语言模型本身对输入是有限制的，输入的长度由语言模型的上下文窗口大小决定</strong>，比如GPT-3系列模型中，text-davinci-003最多可接受的输入是4000个token， text-curie-001则是2048token。所以我们不可能一股脑把检索出来的最相关的 $k$ 个文档都一次性喂给语言模型。</p><p>针对这个问题，<strong>REPLUG <strong>设计了一个</strong>集成策略</strong>。具体地，将每个文档 $d \in D’$ 都添加到 $x$ 的前面，再将拼接后的文本分别给到语言模型，最后将所有 $k$ 个的输出概率集成起来(这里利用了相关度加权)。这里 $D’$ 表示的就是最相关的 $k$ 个文档，集成策略的公式如下：</p><p>$$p(y|x,D)&#x3D;\sum\limits_{d \in D’}p(y|d \circ x) \cdot \lambda(d,x)$$</p><p>其中，$\circ$ 表示向量拼接，$\lambda(d,x)$ 是经过归一化的文档—用户query对的相似度：</p><p>$$\lambda(d,x)&#x3D;\frac{e^{s(d,x)}}{\sum_{d \in D’}e^{s(d,x)}}$$</p><h3 id="3-2-REPLUG-LSR"><a href="#3-2-REPLUG-LSR" class="headerlink" title="3.2 REPLUG LSR"></a>3.2 REPLUG LSR</h3><p>前文我们说，可以将 <strong>RERLUG LST</strong> 看作是 <strong>RERLUG</strong> 的升级版，它主要<strong>利用语言模型产生监督信号，从而去优化检索组件，让检索组件适配语言模型</strong>。这里面，可能大家最感兴趣的就是这个<strong>监督信号</strong>，它到底是什么，怎么来的，怎么计算的？</p><p>在 <strong>REPLUG LSR</strong>中，这个监督信号其实是通过“匹配检索文档的概率与语言模型输出序列的困惑度(perplexity)”而来。更直白地说，<strong>REPLUG LSR 希望检索器能够找到使得语言模型生成文本困惑度较低的文档</strong>。（这里原文使用的是likelihood，即似然。结合具体公式来看，个人认为叫概率更合适，所以下文我们都统一称为概率。）</p><h4 id="3-2-1-语言模型的困惑度"><a href="#3-2-1-语言模型的困惑度" class="headerlink" title="3.2.1 语言模型的困惑度"></a>3.2.1 语言模型的困惑度</h4><p>这里我们简单解释下困惑度。</p><p>首先<strong>语言模型</strong>不仅可以用来生成文本，本质上它提供了一种很自然的方式来<strong>估计句子的概率</strong>。<strong>越好的语言模型对于我们人类给出的一句通顺流畅的话，会给出越高的概率，这样的语言模型困惑度也就越小</strong>。简单地概括就是<strong>句子概率越大，语言模型越好，困惑度越小</strong>。</p><p>那么”检索器要找到使得语言模型生成文本困惑度得分较低的文档”就可以这么理解：<strong>检索器检索出来的文档 $d$ 可以提高语言模型生成答案 $y$ 的概率，并且文档的检索概率越高，语言模型生成的 $y$ 的概率也越高</strong>。</p><p>也就是说，<strong>REPLUG LSR</strong>的监督信号中涉及两个概率计算。</p><h4 id="3-2-2-概率计算"><a href="#3-2-2-概率计算" class="headerlink" title="3.2.2 概率计算"></a>3.2.2 概率计算</h4><p>首先是<strong>检索文档的概率</strong>，这个部分和前面的 $\lambda(d,x)$ 比较像，只不过多了一个<strong>缩放因子</strong>$\tau$:</p><p>$$P_R(d|x)&#x3D;\frac{e^{\frac{s(d,x)}{\tau}}}{\sum_{d \in D’}e^{\frac{s(d,x)}{\tau}}}$$</p><p>注意到，这里归一化也是在 $D’$ 上做的，所以这个归一化是<strong>近似归一化</strong>。</p><p>然后是<strong>语言模型输出序列的困惑度概率</strong>。假设 $P_{LM}(y|d,x)$ 计算的是在给定用户query $x$ 和检索文档 $d$ 的条件下，语言模型生成标准答案 $y$ 的概率；那么在文档集 $D$ 中，文档 $d$ 对应的语言模型输出序列的困惑度概率如下：</p><p>$$Q_R(d|x,y)&#x3D;\frac{e^{\frac{P_{LM}s(d,x)}{\beta}}}{\sum_{d \in D’}e^{\frac{P_{LM}s(d,x)}{\beta}}}$$</p><p>这里也是一个近似归一化，缩放因子为 $\beta$。</p><p>但是这里小喵有<strong>两个疑问</strong>：</p><p>(1) 如果语言模型是一个黑盒，那我们又该如何计算 $P_{LM}(y|d,x)$ 呢？</p><p>(2) 还有这里的并不是一个token，而是一个token序列，针对token序列前面的集成概率具体又是怎么计算的呢？</p><h4 id="3-2-3-损失函数"><a href="#3-2-3-损失函数" class="headerlink" title="3.2.3 损失函数"></a>3.2.3 损失函数</h4><p><img src="https://cdn.jsdelivr.net/gh/xhd0728/oss-github-picgo-repository/picgo/20231201170802.png"></p><p>我们前面说过<strong>文档的检索概率越高，语言模型生成的 $y$ 的概率也越高</strong>，也就是说我们希望这两个概率一致。而度量两个<strong>概率分布的距离</strong>可以通常选用的是<strong>KL散度</strong>，所以在 <strong>REPLUG LSR</strong>中，损失函数如下：</p><p>$$L&#x3D;\frac{1}{X}\sum\limits_{x \in X}KL(P_R(d|x)||Q_{LM}(d|x,y))$$</p><p>这里 $X$ 代表用户输入的集合。这个损失函数非常容易理解，并且某种程度上只有这样才能将语言模型的输出与检索器关联起来，从而去优化检索器。</p><p><strong>REPLUG LSR</strong> 通过<strong>优化检索器的参数</strong>降低损失函数的值从而让检索器适配到语言模型。这里，需要注意的是前面我们提过检索组件一旦被训练，检索出的文本和用户query的表示就会改变，那么被选出来的文本也就变了。所以在训练过程中， <strong>REPLUG LSR</strong> <strong>每隔 $T$ 个 training step 就会重新编码一遍所有文本</strong>。</p><h2 id="4-总结"><a href="#4-总结" class="headerlink" title="4. 总结"></a>4. 总结</h2><p>今天我们分享了一篇23年的新工作，它提出了检索增强语言模型的新范式。在这个范式下，利用语言模型产生监督信号，从而去优化检索组件，让检索组件适配语言模从而消除或减少幻觉即事实性错误的生成。</p><p>在今天这篇文章中，小喵主要集中在论文方法和思想的分享，感兴趣的读者朋友可以下载原文详细阅读实验部分。对于小喵的疑问，有知道的朋友可以后台私信小喵，咱们一起交流一下。</p><h4 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h4><p>[1]</p><p>《REPLUG: Retrieval-Augmented Black-Box Language Models》: <em><a href="https://arxiv.org/pdf/2301.12652.pdf">https://arxiv.org/pdf/2301.12652.pdf</a></em></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;23年的这篇论文提出了检索增强的新范式，即&lt;strong&gt;REPLUG&lt;/strong&gt;。它将语言模型当作一个黑盒子，即冻结语言模型的参数不再优化，转而去优化检索组件让检索组件来适配语言模型，以此来消除语言模型的“幻觉”，减少事实性错误的生成。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;关键词：检索增强、向量检索、困惑度、幻觉、事实性错误&lt;/strong&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="论文学习" scheme="https://resume.kokomi0728.eu.org/categories/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="信息检索" scheme="https://resume.kokomi0728.eu.org/tags/%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2/"/>
    
  </entry>
  
  <entry>
    <title>COCO-LM: Correcting and Contrasting Text Sequences for Language Model Pretraining</title>
    <link href="https://resume.kokomi0728.eu.org/posts/a5e8b87b.html"/>
    <id>https://resume.kokomi0728.eu.org/posts/a5e8b87b.html</id>
    <published>2023-11-24T16:00:00.000Z</published>
    <updated>2023-12-01T13:04:47.613Z</updated>
    
    <content type="html"><![CDATA[<p>最近在arxiv上看见一篇关于预训练模型的文章，觉得很不错，这里附上链接: <a href="https://arxiv.org/abs/2102.08473">arXiv</a></p><span id="more"></span><p>该篇文章2021年2月16日上传，提出了一种新的预训练模型的框架，个人认为COCO-LM结合了许多当下比较新进的思想，在后bert时代，一定程度上突破了对BERT模型传统的预训练方法。</p><p><img src="https://cdn.jsdelivr.net/gh/xhd0728/oss-github-picgo-repository/picgo/20231125154307.png"></p><p>那么，这篇文章主要结合了哪些思想呢？</p><p><img src="https://cdn.jsdelivr.net/gh/xhd0728/oss-github-picgo-repository/picgo/20231125154409.png"></p><p>1.第一点，我认为COCO-LM框架延续了ELECTRA预训练模型的思想。ELECTRA预训练模型主要应用了GAN对抗神经网络的思想，不了解的小伙伴们可以参考一些其他资料，这里我简单说一下我的理解。</p><p>GAN对抗神经网络在CV领域上应用比较成熟，在CV的应用上GAN主要包括两个神经网络模型：一个是生成式模型G，一个是判别式模型D。生成式模型的作用是通过随机噪声生成和原始样本相似的数据(注意这里是通过随机噪声)，判别式模型的作用是判断给定的实例是真实实例还是人为伪造的(也就是生成式模型所生成的)。那么这里就包含了对抗的思想，即生成式模型的目的是能够生成欺骗判别式模型的实例，判别式模型的目的是判别给定的实例是否是人为伪造的。</p><p><a href="https://blog.csdn.net/DFCED/article/details/105175097">图解 生成对抗网络GAN 原理 超详解</a></p><p>ELECTRA当中引用了这样的“对抗”思想，将判别式模型引入到了模型的预训练之中。像BERT、ROBERTA、XLNET等等预训练模型都属于生成式模型，在输入上用 [MASK] 遮蔽掉部分 tokens，再训练一个模型以重建出原始的 tokens。而ELECTRA预训练模型使用了判别式模型，其效果也出乎意料的好。</p><p>ELECTRA模型的主要思想也是包括了两个神经网络模型：一个生成式模型G，一个生成式模型D。生成式模型G是MLM(Masked Language Model)模型，给定一个真实样例(GAN的生成式模型给定的是随机噪声)，用 [MASK] 遮蔽掉部分 tokens，生成替换的tokens；判别式模型D判断输入中每个 token 是否是由生成器生成。其过程如图所示：</p><p><img src="https://cdn.jsdelivr.net/gh/xhd0728/oss-github-picgo-repository/picgo/20231125154846.png"></p><p>通过实验表明这种新的预训练任务比 MLM 更高效，该任务定义于全部的输入 tokens，而非仅仅被遮蔽掉的那一部分小小的输入子集。</p><p>在COCO-LM模型中Corrective Language Modeling (CLM)也延续了这样的思想。</p><p>2.COCO-LM模型引入了对比学习的思想，我认为是非常非常棒的创新点。最近刚好再看对比学习的相关paper，更多的是在CV领域中使用了对比学习，而COCO-LM刚好将对比学习带入到了NLP领域中。</p><p>什么是对比学习呢？</p><p>对比学习是一种自监督的学习方法。其主要思想我的理解是，把正样本距离拉近，正样本与负样本距离拉远。对比学习的例子如下：</p><ul><li>给每个例子绘制两个独立的增强函数</li><li>使用两种增强机制，为每个示例生成两个互相关联的视图</li><li>让相关视图互相吸引，同时排斥其他示例</li></ul><p><img src="https://cdn.jsdelivr.net/gh/xhd0728/oss-github-picgo-repository/picgo/20231125154953.png"></p><p>如上图，$(Z_1,Z_2),(Z_3,Z_4) \dots (Z_{2n-1},Z_{2n})$这些可以看作正例对，而$Z1$可以与除$Z1$、$Z2$的任何实例组成负例对，如$(Z1,Z3),(Z1,Z4)$等等。那么这样一个实例$X$，在一个大小为$N$的$batch$里便可以产生一个正例，以及$N-1$个负例，那么这个 $loss$ 就可以看做是一个 $N$ 分类问题，实际上就是一个交叉熵，由此可以进行网络模型的训练。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;最近在arxiv上看见一篇关于预训练模型的文章，觉得很不错，这里附上链接: &lt;a href=&quot;https://arxiv.org/abs/2102.08473&quot;&gt;arXiv&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="论文学习" scheme="https://resume.kokomi0728.eu.org/categories/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="信息检索" scheme="https://resume.kokomi0728.eu.org/tags/%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2/"/>
    
    <category term="预训练模型" scheme="https://resume.kokomi0728.eu.org/tags/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"/>
    
  </entry>
  
  <entry>
    <title>字节对编码(BPE)</title>
    <link href="https://resume.kokomi0728.eu.org/posts/e65f4108.html"/>
    <id>https://resume.kokomi0728.eu.org/posts/e65f4108.html</id>
    <published>2023-11-22T16:00:00.000Z</published>
    <updated>2023-11-23T14:15:58.282Z</updated>
    
    <content type="html"><![CDATA[<p>字节对编码（Byte Pair Encoding，BPE）是一种用于数据压缩和文本编码的技术，同时也在自然语言处理（NLP）中广泛应用于分词和子词分割。这种编码方法由Philip Gage在1994年提出，后来由Sennrich等人引入到机器翻译领域。</p><span id="more"></span><h3 id="BPE的基本思想："><a href="#BPE的基本思想：" class="headerlink" title="BPE的基本思想："></a>BPE的基本思想：</h3><p>BPE的主要思想是从输入文本的最小单位开始，通过不断合并出现频率最高的相邻单位，逐渐构建出更大的单位，直到达到预定的词汇大小或达到停止条件。在NLP中，这些单位可以是字符、子词，或者更大的单词。</p><h3 id="BPE算法步骤："><a href="#BPE算法步骤：" class="headerlink" title="BPE算法步骤："></a>BPE算法步骤：</h3><ol><li><p><strong>初始化：</strong> 将输入文本中的每个字符或子词视为一个初始的符号。</p></li><li><p><strong>计算频率：</strong> 统计相邻符号对的频率。</p></li><li><p><strong>合并：</strong> 合并频率最高的相邻符号对，形成一个新的符号，将其添加到符号表中。</p></li><li><p><strong>更新文本：</strong> 在输入文本中，用新的符号替换被合并的相邻符号。</p></li><li><p><strong>重复：</strong> 重复步骤2-4，直到符号表的大小达到预定的词汇大小或者达到停止条件。</p></li></ol><h3 id="例子："><a href="#例子：" class="headerlink" title="例子："></a>例子：</h3><p>假设有以下输入文本：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">aabacabadabacabae</span><br></pre></td></tr></table></figure><ol><li>初始化：</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a, a, b, a, c, a, b, a, d, a, b, a, c, a, b, a, e</span><br></pre></td></tr></table></figure><ol start="2"><li>计算频率：</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(a, a): 6, (a, b): 6, (b, a): 6, (a, c): 4, (c, a): 4, (b, d): 2, (d, a): 2, (d, a): 2, (b, a, c): 2, (a, c, a): 2, (c, a, b): 2, (a, b, a): 2</span><br></pre></td></tr></table></figure><ol start="3"><li>合并：</li></ol><p>合并频率最高的相邻符号对 (a, b)：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a, ab, a, c, a, ab, a, d, a, ab, a, c, a, ab, a, e</span><br></pre></td></tr></table></figure><ol start="4"><li>重复：</li></ol><p>继续迭代，重复计算频率、合并的步骤，直到达到预定的词汇大小。</p><p>BPE最终生成的合并的符号可以视为子词，用于表示文本中的重要词汇和模式。在NLP任务中，这种子词分割方法有助于解决未登录词（Out-of-Vocabulary，OOV）问题，同时提高对复杂词汇和短语的建模能力。 BPE被广泛用于机器翻译、文本生成等领域。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;字节对编码（Byte Pair Encoding，BPE）是一种用于数据压缩和文本编码的技术，同时也在自然语言处理（NLP）中广泛应用于分词和子词分割。这种编码方法由Philip Gage在1994年提出，后来由Sennrich等人引入到机器翻译领域。&lt;/p&gt;</summary>
    
    
    
    <category term="论文学习" scheme="https://resume.kokomi0728.eu.org/categories/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="NLP" scheme="https://resume.kokomi0728.eu.org/tags/NLP/"/>
    
    <category term="机器翻译" scheme="https://resume.kokomi0728.eu.org/tags/%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/"/>
    
  </entry>
  
  <entry>
    <title>Coreference Resolution</title>
    <link href="https://resume.kokomi0728.eu.org/posts/f0bcdccd.html"/>
    <id>https://resume.kokomi0728.eu.org/posts/f0bcdccd.html</id>
    <published>2023-11-20T16:00:00.000Z</published>
    <updated>2023-11-23T14:15:58.249Z</updated>
    
    <content type="html"><![CDATA[<p>Coreference Resolution（指代消解）是自然语言处理（NLP）领域的一个任务，其目标是识别一段文本中的代词（如”he”、”she”、”it”）或名词短语是否指代同一实体。这是一个重要的语言理解任务，因为它涉及到理解文本中实体之间的关系，使得计算机可以更好地理解上下文，并推断谁在说什么、指代什么。</p><span id="more"></span><p>考虑以下的例子：</p><p>“John went to the store. He bought a new laptop.”</p><p>在这个例子中，”He” 指代的是 “John”，这种关系就是一个指代关系。Coreference Resolution 的任务就是在文本中找到这种关系，将 “He” 和 “John” 连接起来。</p><p>Coreference Resolution 面临的挑战包括：</p><ol><li><strong>代词消解：</strong> 识别文本中的代词，并确定它们指代的实体。</li><li><strong>命名实体消解：</strong> 识别文本中的命名实体（如人名、地名）是否指代同一实体。</li><li><strong>解决跨句指代：</strong> 处理文本中跨句的指代关系，即一个实体在文本的不同句子中出现。</li></ol><p>Coreference Resolution 的应用广泛，包括文本理解、问答系统、机器翻译等。在这些任务中，正确处理代词消解和实体指代关系可以提高系统对文本的理解和生成的质量。</p><p>一些先进的NLP模型，尤其是基于深度学习的模型，已经在 Coreference Resolution 任务上取得了一些进展。这包括使用注意力机制、神经网络和大规模预训练模型等技术。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;Coreference Resolution（指代消解）是自然语言处理（NLP）领域的一个任务，其目标是识别一段文本中的代词（如”he”、”she”、”it”）或名词短语是否指代同一实体。这是一个重要的语言理解任务，因为它涉及到理解文本中实体之间的关系，使得计算机可以更好地理解上下文，并推断谁在说什么、指代什么。&lt;/p&gt;</summary>
    
    
    
    <category term="论文学习" scheme="https://resume.kokomi0728.eu.org/categories/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="NLP" scheme="https://resume.kokomi0728.eu.org/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Masked Language Modeling</title>
    <link href="https://resume.kokomi0728.eu.org/posts/c581aeed.html"/>
    <id>https://resume.kokomi0728.eu.org/posts/c581aeed.html</id>
    <published>2023-11-20T16:00:00.000Z</published>
    <updated>2023-11-23T14:15:58.269Z</updated>
    
    <content type="html"><![CDATA[<p>Masked Language Modeling（MLM，遮蔽语言建模）是一种自然语言处理（NLP）中的预训练任务，其主要目标是通过在输入文本中遮蔽（掩盖）一些单词并尝试预测这些遮蔽单词的身份来学习词汇和语言表示。这一任务通常在Transformer等深度学习模型上进行。</p><span id="more"></span><p>以下是Masked Language Modeling 的基本步骤：</p><ol><li><p><strong>遮蔽输入文本：</strong> 在输入文本中，随机选择一些单词，并将它们用特殊的标记（通常是”[MASK]”标记）替换。这使得模型在训练时需要预测这些被遮蔽的单词。</p></li><li><p><strong>模型预测：</strong> 将经过遮蔽的文本输入模型，并让模型预测被遮蔽的位置上的单词。在Transformer模型中，这通常涉及到在每个位置上进行softmax分类，使得模型预测每个词汇表中的单词的概率。</p></li><li><p><strong>计算损失：</strong> 通过比较模型的预测和真实的被遮蔽单词，计算一个损失值。损失函数通常是交叉熵损失。</p></li></ol><p>通过这个过程，模型被迫学习对上下文的理解，因为它需要根据上下文来预测被遮蔽的单词。这种预训练任务可以为后续特定任务（如文本分类、命名实体识别等）提供有用的语言表示。</p><p>BERT（Bidirectional Encoder Representations from Transformers）是一个采用MLM作为预训练任务的例子。BERT通过预测被遮蔽的单词，以及利用双向上下文信息，获得了在多种NLP任务上出色表现的通用语言表示。其他一些模型，如GPT-2（Generative Pre-trained Transformer 2）也使用了类似的预训练任务，尽管采用的是单向语言模型。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;Masked Language Modeling（MLM，遮蔽语言建模）是一种自然语言处理（NLP）中的预训练任务，其主要目标是通过在输入文本中遮蔽（掩盖）一些单词并尝试预测这些遮蔽单词的身份来学习词汇和语言表示。这一任务通常在Transformer等深度学习模型上进行。&lt;/p&gt;</summary>
    
    
    
    <category term="论文学习" scheme="https://resume.kokomi0728.eu.org/categories/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="NLP" scheme="https://resume.kokomi0728.eu.org/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>预热</title>
    <link href="https://resume.kokomi0728.eu.org/posts/fc31d48b.html"/>
    <id>https://resume.kokomi0728.eu.org/posts/fc31d48b.html</id>
    <published>2023-11-20T16:00:00.000Z</published>
    <updated>2023-11-23T14:15:58.291Z</updated>
    
    <content type="html"><![CDATA[<p>在深度学习中，”预热”（warm-up）通常指的是在训练的初期阶段逐渐增加学习率或其他训练参数的过程。这旨在帮助模型更好地适应数据，减少训练初期的不稳定性，并提高模型的性能。预热在训练神经网络时是一种常见的优化策略，具有以下好处：</p><span id="more"></span><ol><li><p><strong>稳定训练初期：</strong> 在训练的开始阶段，模型的权重可能处于一个相对不稳定的状态。通过逐渐增加学习率，模型可以更缓慢地更新权重，避免因为过大的梯度导致的不稳定性。</p></li><li><p><strong>避免陷入局部最优解：</strong> 如果学习率过大，模型可能会跳过或振荡在全局最优解附近，无法精确找到最优解。通过预热，可以减小学习率，使模型更容易收敛到合适的区域，然后再逐渐增加学习率。</p></li><li><p><strong>提高泛化性能：</strong> 预热可以帮助模型更好地捕捉数据的特征，从而提高在未见过的数据上的泛化性能。</p></li><li><p><strong>减小梯度爆炸和梯度消失问题：</strong> 在训练初期，由于初始权重和梯度的原因，可能出现梯度爆炸或梯度消失的问题。通过逐渐增加学习率，可以缓解这些问题。</p></li><li><p><strong>更好的权重初始化：</strong> 逐渐增加学习率可以帮助模型更好地选择适当的权重初始化，提高训练效果。</p></li><li><p><strong>更快的收敛：</strong> 预热可以使模型更快地收敛到一个相对较好的解，从而缩短整体训练时间。</p></li></ol><p>总体来说，预热是一种通过逐渐升高学习率来引导模型进入训练状态的策略，有助于训练过程更加平滑、稳定，提高模型性能。然而，具体的预热策略可能取决于任务、模型架构和数据集的特性。</p><p>在训练神经网络时，学习率的调度策略对于模型的性能和收敛速度至关重要。先预热后线性减小学习率和直接从最大学习率线性减小两种方式都有其优缺点，而更好的方式可能取决于具体的任务和数据。</p><h3 id="先预热，再线性减小学习率："><a href="#先预热，再线性减小学习率：" class="headerlink" title="先预热，再线性减小学习率："></a>先预热，再线性减小学习率：</h3><p><strong>优点：</strong></p><ol><li><strong>更稳定的训练初期：</strong> 预热阶段可以帮助模型更好地适应数据，减少训练初期的不稳定性。</li><li><strong>更好的泛化性能：</strong> 通过预热，模型可能更容易收敛到一个良好的初始点，有助于提高泛化性能。</li></ol><p><strong>缺点：</strong></p><ol><li><strong>训练时间增加：</strong> 预热阶段会增加整体训练时间。</li><li><strong>对预热参数敏感：</strong> 预热的效果可能对预热时选择的参数（如预热步数、预热学习率等）敏感。</li></ol><h3 id="直接从最大学习率线性减小："><a href="#直接从最大学习率线性减小：" class="headerlink" title="直接从最大学习率线性减小："></a>直接从最大学习率线性减小：</h3><p><strong>优点：</strong></p><ol><li><strong>更高的训练效率：</strong> 直接从最大学习率开始训练，节省了预热的时间，可能更高效。</li><li><strong>更少的超参数：</strong> 不需要设置预热相关的超参数。</li></ol><p><strong>缺点：</strong></p><ol><li><strong>训练初期不稳定：</strong> 直接从较大的学习率开始训练，可能导致训练初期的不稳定性。</li><li><strong>泛化性能可能较差：</strong> 初始时较大的学习率可能使得模型难以找到稳定的优化路径，影响泛化性能。</li></ol><h3 id="更好的训练方式："><a href="#更好的训练方式：" class="headerlink" title="更好的训练方式："></a>更好的训练方式：</h3><p>目前，没有一种单一的训练方式适用于所有情况，最佳的训练方式可能取决于具体的任务、数据和模型。一些其他可能的学习率调度策略包括余弦退火、学习率多项式衰减等。调整学习率调度策略时，可以通过验证集性能进行实验和调试，以找到最适合特定任务的策略。</p><p>在实践中，还有一些先进的学习率调度算法，如使用学习率预测器（learning rate schedulers with warmup and cooldown phases）或根据训练曲线动态调整学习率。这些方法可以更灵活地适应不同的训练阶段和数据特性。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;在深度学习中，”预热”（warm-up）通常指的是在训练的初期阶段逐渐增加学习率或其他训练参数的过程。这旨在帮助模型更好地适应数据，减少训练初期的不稳定性，并提高模型的性能。预热在训练神经网络时是一种常见的优化策略，具有以下好处：&lt;/p&gt;</summary>
    
    
    
    <category term="论文学习" scheme="https://resume.kokomi0728.eu.org/categories/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="机器学习" scheme="https://resume.kokomi0728.eu.org/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>GeLU</title>
    <link href="https://resume.kokomi0728.eu.org/posts/2e561321.html"/>
    <id>https://resume.kokomi0728.eu.org/posts/2e561321.html</id>
    <published>2023-11-19T16:00:00.000Z</published>
    <updated>2023-11-23T14:15:58.265Z</updated>
    
    <content type="html"><![CDATA[<p>GeLU（Gaussian Error Linear Unit）是一种激活函数，通常用于神经网络的隐藏层。它被设计用来克服一些传统激活函数（如ReLU）的一些限制，尤其是对于包含负值的输入数据。</p><span id="more"></span><p>GeLU的定义是：</p><p>$\text{GeLU}(x) &#x3D; 0.5x \left(1 + \tanh\left(\sqrt{\frac{2}{\pi}}\left(x + 0.044715x^3\right)\right)\right)$</p><p>其中，$\tanh$ 是双曲正切函数。</p><p>GeLU的性质包括：</p><ol><li><p><strong>平滑性：</strong> GeLU是平滑的函数，这意味着在整个实数域内都是可导的。这与一些其他激活函数，如ReLU，相比是一个优势，因为可导性对于训练过程中的梯度计算是重要的。</p></li><li><p><strong>近似线性：</strong> 当 $x$ 的值远离零点时，GeLU的形状接近于线性，这有助于减缓激活函数的饱和效应，使得网络更容易训练。</p></li><li><p><strong>零均值：</strong> 对于足够大的输入 $x$，GeLU的输出趋近于零均值。这对一些神经网络架构可能是有用的。</p></li></ol><p>GeLU的名字来源于它的形状类似于高斯误差函数（Gaussian Error Function）。GeLU被广泛应用于一些神经网络架构，尤其是在语言模型和自然语言处理任务中，但并非适用于所有情况。在实践中，研究人员会根据任务和模型的特性选择合适的激活函数。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;GeLU（Gaussian Error Linear Unit）是一种激活函数，通常用于神经网络的隐藏层。它被设计用来克服一些传统激活函数（如ReLU）的一些限制，尤其是对于包含负值的输入数据。&lt;/p&gt;</summary>
    
    
    
    <category term="论文学习" scheme="https://resume.kokomi0728.eu.org/categories/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="机器学习" scheme="https://resume.kokomi0728.eu.org/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="数学" scheme="https://resume.kokomi0728.eu.org/tags/%E6%95%B0%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>双序列采样</title>
    <link href="https://resume.kokomi0728.eu.org/posts/b3b84358.html"/>
    <id>https://resume.kokomi0728.eu.org/posts/b3b84358.html</id>
    <published>2023-11-19T16:00:00.000Z</published>
    <updated>2023-11-23T14:15:58.280Z</updated>
    
    <content type="html"><![CDATA[<p>BERT（Bidirectional Encoder Representations from Transformers）模型在预训练时采用了双序列（或双句子）采样的方法，这是为了帮助模型学习句子之间的关系和上下文信息。</p><span id="more"></span><p>在BERT的预训练过程中，模型的输入是一对句子，通常是一个文本中相邻的两个句子。这一对句子会被连接成一个输入序列，并使用特殊的分隔符标记（[SEP]）进行分隔。在原始BERT的预训练任务中，有一个叫做”Next Sentence Prediction”（NSP）的任务，目标是判断两个句子是否是相邻的。</p><p>双序列采样的步骤如下：</p><ol><li><p><strong>选择一篇文档：</strong> 从语料库中选择一篇文档。</p></li><li><p><strong>选择一个句子：</strong> 随机选择文档中的一个句子作为第一个句子（A）。</p></li><li><p><strong>选择下一个句子：</strong> 有50%的概率选择与第一个句子相邻的句子作为第二个句子（B），有50%的概率从语料库中随机选择一个不与第一个句子相邻的句子。</p></li><li><p><strong>构建输入序列：</strong> 将句子A和句子B连接成一个输入序列，并在它们之间插入一个特殊的分隔符标记 [SEP]。</p></li><li><p><strong>添加分类标签：</strong> 在输入序列的开头添加一个特殊的分类标签（[CLS]），用于预测下游任务的输出。</p></li></ol><p>这样，模型在预训练中不仅需要理解单个句子的上下文信息，还需要理解两个句子之间的关系。这有助于提高模型对文本理解的能力，使其能够更好地捕捉句子之间的语义关系。需要注意的是，一些后续的模型，如RoBERTa，已经移除了NSP任务，而专注于更好地优化单个句子的表示。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;BERT（Bidirectional Encoder Representations from Transformers）模型在预训练时采用了双序列（或双句子）采样的方法，这是为了帮助模型学习句子之间的关系和上下文信息。&lt;/p&gt;</summary>
    
    
    
    <category term="论文学习" scheme="https://resume.kokomi0728.eu.org/categories/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="NLP" scheme="https://resume.kokomi0728.eu.org/tags/NLP/"/>
    
    <category term="信息检索" scheme="https://resume.kokomi0728.eu.org/tags/%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2/"/>
    
  </entry>
  
  <entry>
    <title>线性预热</title>
    <link href="https://resume.kokomi0728.eu.org/posts/50e93fc8.html"/>
    <id>https://resume.kokomi0728.eu.org/posts/50e93fc8.html</id>
    <published>2023-11-19T16:00:00.000Z</published>
    <updated>2023-11-23T14:15:58.286Z</updated>
    
    <content type="html"><![CDATA[<p>在AI领域中，线性预热通常指的是在训练神经网络模型时逐渐增加学习率或其他训练参数，以在训练开始阶段更好地引导模型的学习过程。这是一种优化训练过程的策略，可以帮助模型更快地收敛到良好的解决方案。</p><span id="more"></span><p>在神经网络训练中，学习率是一个关键的超参数，它决定了在每次更新模型权重时调整的步长大小。线性预热的想法是，在训练开始时，使用一个相对较小的学习率，然后逐渐增加学习率，使得模型能够更容易地找到全局最优解或更好的局部最优解。</p><p>具体来说，线性预热的过程可以描述为以下步骤：</p><ol><li><p><strong>初始学习率设置：</strong> 初始阶段使用一个较小的学习率，通常是在零附近。</p></li><li><p><strong>逐渐增加学习率：</strong> 在训练的初始几个epoch中，逐渐增加学习率，可以是线性增加的方式，也可以是其他函数形式。这个过程通常在模型开始学习之前的几个epoch中完成。</p></li><li><p><strong>常规训练：</strong> 一旦线性预热阶段完成，模型将以较高的学习率进行正常训练，这有助于更快地调整权重并学习模型的有效表示。</p></li></ol><p>线性预热的优势在于它可以帮助模型在训练的早期阶段更好地适应数据，防止由于初始的不稳定性而导致的训练困难。这种策略在许多深度学习任务中都有助于加速模型的收敛并提高最终性能。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;在AI领域中，线性预热通常指的是在训练神经网络模型时逐渐增加学习率或其他训练参数，以在训练开始阶段更好地引导模型的学习过程。这是一种优化训练过程的策略，可以帮助模型更快地收敛到良好的解决方案。&lt;/p&gt;</summary>
    
    
    
    <category term="论文学习" scheme="https://resume.kokomi0728.eu.org/categories/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="机器学习" scheme="https://resume.kokomi0728.eu.org/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>使用Vscode调试Linux内核</title>
    <link href="https://resume.kokomi0728.eu.org/posts/c45db453.html"/>
    <id>https://resume.kokomi0728.eu.org/posts/c45db453.html</id>
    <published>2023-11-18T16:00:00.000Z</published>
    <updated>2023-11-24T06:57:15.498Z</updated>
    
    <content type="html"><![CDATA[<p>文章介绍了在Vscode中配置和使用远程SSH插件进行Linux内核的调试，以替代之前使用Qemu和gdb的方式。通过安装远程SSH插件和C&#x2F;C++插件，配置调试环境，并展示了在Vscode中设置断点、进行调试的步骤。文章还解决了在代码中出现红色标记的问题，提供了生成<code>compile_commands.json</code>文件的命令和在Vscode中配置的步骤，使读者能够顺利进行Vscode调试。</p><span id="more"></span><p>上一篇博客我们在虚拟机中编译了Linux内核，并且使用Qemu和gdb进行调试，但是gdb的指令我还不熟练，还是想用vscode来调试，这样也更加方便</p><h2 id="Vscode插件安装"><a href="#Vscode插件安装" class="headerlink" title="Vscode插件安装"></a>Vscode插件安装</h2><h3 id="remote-ssh"><a href="#remote-ssh" class="headerlink" title="remote-ssh"></a>remote-ssh</h3><p><img src="https://raw.githubusercontent.com/wfloveiu/blogImage/main/img/202311201245007.png" alt="image-20231120124509960"></p><p>安装完成后右边工具栏会多出一个功能</p><p><img src="https://raw.githubusercontent.com/wfloveiu/blogImage/main/img/202311201246229.png" alt="image-20231120124653194"></p><p>按F1呼出对话框，输入<code>remote-ssh</code>，选择open ssh configuration file。</p><p><img src="https://raw.githubusercontent.com/wfloveiu/blogImage/main/img/202311201247886.png" alt="image-20231120124748850"></p><p>选择第一个配置文件</p><p><img src="https://raw.githubusercontent.com/wfloveiu/blogImage/main/img/202311201248231.png" alt="image-20231120124824202"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Host 自定义连接名称</span><br><span class="line">    HostName 服务器IP地址</span><br><span class="line">    User 用户名</span><br></pre></td></tr></table></figure><h3 id="C-C"><a href="#C-C" class="headerlink" title="C&#x2F;C++"></a>C&#x2F;C++</h3><p>安装C&#x2F;C++插件</p><p><img src="https://raw.githubusercontent.com/wfloveiu/blogImage/main/img/202311201251048.png" alt="image-20231120125110014"></p><p>依次点击【运行】-&gt;【打开配置】，将以下配置复制到launch.json中。</p><p><strong>该代码不需要更改，直接粘贴</strong></p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="comment">// 使用 IntelliSense 了解相关属性。 </span></span><br><span class="line">    <span class="comment">// 悬停以查看现有属性的描述。</span></span><br><span class="line">    <span class="comment">// 欲了解更多信息，请访问: https://go.microsoft.com/fwlink/?linkid=830387</span></span><br><span class="line">    <span class="attr">&quot;version&quot;</span><span class="punctuation">:</span> <span class="string">&quot;0.2.0&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;configurations&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">        <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;kernel-debug&quot;</span><span class="punctuation">,</span>   <span class="comment">//随便起名</span></span><br><span class="line">            <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;cppdbg&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;request&quot;</span><span class="punctuation">:</span> <span class="string">&quot;launch&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;miDebuggerServerAddress&quot;</span><span class="punctuation">:</span> <span class="string">&quot;127.0.0.1:1234&quot;</span><span class="punctuation">,</span>  <span class="comment">//远端调试地址，1234为qemu的监视端口</span></span><br><span class="line">            <span class="attr">&quot;program&quot;</span><span class="punctuation">:</span> <span class="string">&quot;$&#123;workspaceFolder&#125;/vmlinux&quot;</span><span class="punctuation">,</span>     <span class="comment">//当前目录下的vmlinux</span></span><br><span class="line">            <span class="attr">&quot;args&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;stopAtEntry&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">false</span></span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;cwd&quot;</span><span class="punctuation">:</span> <span class="string">&quot;$&#123;workspaceFolder&#125;&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;environment&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;externalConsole&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">false</span></span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;logging&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;engineLogging&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">false</span></span></span><br><span class="line">            <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;MIMode&quot;</span><span class="punctuation">:</span> <span class="string">&quot;gdb&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><h2 id="Vscode调试"><a href="#Vscode调试" class="headerlink" title="Vscode调试"></a>Vscode调试</h2><h3 id="在虚拟机中启动qemu"><a href="#在虚拟机中启动qemu" class="headerlink" title="在虚拟机中启动qemu"></a>在虚拟机中启动qemu</h3><p>在<strong>Linux内核文件夹下</strong>运行此命令</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">qemu-system-x86_64 -kernel ./arch/x86/boot/bzImage -initrd ../initramfs.cpio.gz -append <span class="string">&quot;nokaslr console=ttyS0&quot;</span> -s -S -nographic</span><br></pre></td></tr></table></figure><h3 id="打断点"><a href="#打断点" class="headerlink" title="打断点"></a>打断点</h3><p>打开init&#x2F;main.c，我打了如下的断点</p><p><img src="https://raw.githubusercontent.com/wfloveiu/blogImage/main/img/202311201300424.png" alt="image-20231120130037370"></p><h3 id="调试"><a href="#调试" class="headerlink" title="调试"></a>调试</h3><p><img src="https://raw.githubusercontent.com/wfloveiu/blogImage/main/img/202311201305050.png" alt="image-20231120130522009"></p><p>然后在vscode中就可以看到调试结果了</p><h2 id="代码中标红的问题"><a href="#代码中标红的问题" class="headerlink" title="代码中标红的问题"></a>代码中标红的问题</h2><p>代码标红是缺少compile_commands.json文件</p><p>我在B站上学习的时候，是跟着这位up主来的，我的解决方案如下：</p><p>在终端键入命令</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./scripts/clang-tools/gen_compile_commands.py</span><br></pre></td></tr></table></figure><p>在源码目录下就生成了<code>compile_commands.json</code>文件</p><p>在vscode中：<code>ctrl+shipt+p</code>选择C&#x2F;C++：Edit Coonfigurations,</p><p>在c_cpp_properties.json</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;configurations&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">        <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Linux&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;includePath&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                <span class="string">&quot;$&#123;workspaceFolder&#125;/**&quot;</span></span><br><span class="line">            <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;defines&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;compilerPath&quot;</span><span class="punctuation">:</span> <span class="string">&quot;/usr/bin/gcc&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;cStandard&quot;</span><span class="punctuation">:</span> <span class="string">&quot;c17&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;cppStandard&quot;</span><span class="punctuation">:</span> <span class="string">&quot;gnu++17&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;intelliSenseMode&quot;</span><span class="punctuation">:</span> <span class="string">&quot;linux-gcc-x64&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;compileCommands&quot;</span><span class="punctuation">:</span> <span class="string">&quot;$&#123;workspaceFolder&#125;/compile_commands.json&quot;</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;version&quot;</span><span class="punctuation">:</span> <span class="number">4</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><p>此后，main.c中的代码就不标红了</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;文章介绍了在Vscode中配置和使用远程SSH插件进行Linux内核的调试，以替代之前使用Qemu和gdb的方式。通过安装远程SSH插件和C&amp;#x2F;C++插件，配置调试环境，并展示了在Vscode中设置断点、进行调试的步骤。文章还解决了在代码中出现红色标记的问题，提供了生成&lt;code&gt;compile_commands.json&lt;/code&gt;文件的命令和在Vscode中配置的步骤，使读者能够顺利进行Vscode调试。&lt;/p&gt;</summary>
    
    
    
    <category term="技术学习" scheme="https://resume.kokomi0728.eu.org/categories/%E6%8A%80%E6%9C%AF%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="Linux" scheme="https://resume.kokomi0728.eu.org/tags/Linux/"/>
    
    <category term="C/C++" scheme="https://resume.kokomi0728.eu.org/tags/C-C/"/>
    
  </entry>
  
  <entry>
    <title>使用虚拟机进行基于qemu和gdb的Linux内核调试</title>
    <link href="https://resume.kokomi0728.eu.org/posts/6c03d805.html"/>
    <id>https://resume.kokomi0728.eu.org/posts/6c03d805.html</id>
    <published>2023-11-17T16:00:00.000Z</published>
    <updated>2023-11-24T06:56:47.586Z</updated>
    
    <content type="html"><![CDATA[<p>文章介绍了在虚拟机中配置和编译Linux内核的步骤。涵盖了虚拟机配置要求、开启SSH服务、内核编译的准备工作、解决编译中的报错、安装Qemu以及编译busybox等步骤。最后，通过构建initramfs根文件系统和使用Qemu启动内核进行调试，详细说明了连接和调试的步骤。文章还提供了丰富的命令行示例和截图，为读者提供了一份详尽的指南。</p><span id="more"></span><h2 id="虚拟机配置"><a href="#虚拟机配置" class="headerlink" title="虚拟机配置"></a>虚拟机配置</h2><p>至少分8个核心（不然编译速度很慢，亲测）</p><p>磁盘大小分50G（编译后的内核大小有20多个G！）</p><h2 id="打开SSH"><a href="#打开SSH" class="headerlink" title="打开SSH"></a>打开SSH</h2><p>虚拟机中安装的是ubuntu22.04版本，默认没有安装和启用SSH服务</p><p><strong>更新软件源</strong></p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt update &amp;&amp; sudo apt upgrade -y</span><br></pre></td></tr></table></figure><p><strong>安装SSH(OpenSSH)</strong></p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install openssh-server -y</span><br></pre></td></tr></table></figure><p><strong>启动SSH服务</strong></p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo systemctl <span class="built_in">enable</span> --now ssh</span><br></pre></td></tr></table></figure><p><strong>检查是否启动成功</strong></p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo systemctl status ssh</span><br></pre></td></tr></table></figure><h2 id="内核编译"><a href="#内核编译" class="headerlink" title="内核编译"></a>内核编译</h2><h3 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安装相关依赖</span></span><br><span class="line">sudo apt-get install libncurses5-dev libssl-dev bison flex libelf-dev gcc g++ make openssl libc6-dev</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装gdb，这里使用apt安装（多次尝试的结果）</span></span><br><span class="line">sudo apt-get install gdb</span><br><span class="line">gdb --version <span class="comment"># gdb版本为12.1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#这里选择清华源，国内速度会快很多</span></span><br><span class="line">wget https://mirrors.tuna.tsinghua.edu.cn/kernel/v5.x/linux-5.14.tar.gz</span><br><span class="line"><span class="comment"># 解压</span></span><br><span class="line">tar -xvf linux-5.14.tar.gz</span><br><span class="line"></span><br><span class="line"><span class="comment">#配置编译选项</span></span><br><span class="line">make menuconfig</span><br></pre></td></tr></table></figure><p>然后会在此文件夹下生成 <strong>.&#x2F;config</strong>文件</p><p><strong>进入该文件，并做以下2处修改</strong></p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim ./.config</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/wfloveiu/blogImage/main/img/202311201128412.png" alt="image-20231117212013850"></p><p><strong>安装dwarves软件包（编译报错得出结论）</strong></p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install dwarves</span><br></pre></td></tr></table></figure><p><strong>BTF报错解决</strong></p><p>如果仅仅只进行了上边的配置，会报如下错误（我个人是这样）</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">  KSYMS   .tmp_vmlinux.kallsyms1.S</span><br><span class="line">  AS      .tmp_vmlinux.kallsyms1.S</span><br><span class="line">  LD      .tmp_vmlinux.kallsyms2</span><br><span class="line">  KSYMS   .tmp_vmlinux.kallsyms2.S</span><br><span class="line">  AS      .tmp_vmlinux.kallsyms2.S</span><br><span class="line">  LD      vmlinux</span><br><span class="line">  BTFIDS  vmlinux</span><br><span class="line">FAILED: load BTF from vmlinux: Invalid argument</span><br><span class="line">make: *** [Makefile:1187: vmlinux] Error 255</span><br><span class="line">make: *** Deleting file <span class="string">&#x27;vmlinux&#x27;</span></span><br></pre></td></tr></table></figure><p>查阅资料后，有三种解决方案：<a href="https://devkernel.io/posts/pahole-error/">https://devkernel.io/posts/pahole-error/</a></p><p>我使用的是<strong>第二种方法</strong>，对pahole软件包进行降级 :</p><p>查看pahole的版本，是1.25</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pahole --version  </span><br></pre></td></tr></table></figure><p>查看pahole的所有可用安装版本</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-cache policy pahole</span><br></pre></td></tr></table></figure><p>截图如下</p><p><img src="https://raw.githubusercontent.com/wfloveiu/blogImage/main/img/202311201127209.png" alt="1"></p><p>我们发现只有两个版本，因此只能降级为 1.22-8</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install pahole=1.22-8</span><br></pre></td></tr></table></figure><h3 id="编译"><a href="#编译" class="headerlink" title="编译"></a>编译</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">make -j8      <span class="comment">#8个线程并行编译，</span></span><br></pre></td></tr></table></figure><p>然后可能会弹出一个选择（1，2，3），直接选择1即可。等待一段时间，30分钟左右</p><h3 id="是否成功"><a href="#是否成功" class="headerlink" title="是否成功"></a>是否成功</h3><p>编译完成后，目录下会生成以下,那么就编译成功了</p><blockquote><p>.&#x2F;vmLinux</p><p>.&#x2F;arch&#x2F;x86&#x2F;boot&#x2F;bzImage</p><p>其中vmLinux为GDB所需的调试Map文件，bzImage为大内核文件</p></blockquote><h2 id="安装Qemu"><a href="#安装Qemu" class="headerlink" title="安装Qemu"></a>安装Qemu</h2><p>qemu是一款完全软件模拟(Binary translation)的虚拟化软件，在虚拟化的实现中性能相对较差。但利用它在测试环境中gdb调试Linux内核代码，是熟悉Linux内核代码的一个好方法。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#安装qemu</span></span><br><span class="line">sudo apt-get install qemu</span><br></pre></td></tr></table></figure><h2 id="安装编译busybox"><a href="#安装编译busybox" class="headerlink" title="安装编译busybox"></a>安装编译busybox</h2><p>安装busybox的目的是：借助BusyBox构建极简initramfs，提供基本的用户态可执行程序。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">wget https://busybox.net/downloads/busybox-1.36.1.tar.bz2  <span class="comment"># 去官网找最新版</span></span><br><span class="line">tar -xvf busybox-1.36.1.tar.bz2</span><br><span class="line"><span class="built_in">cd</span> busybox-1.36.1/</span><br><span class="line">make menuconfig</span><br></pre></td></tr></table></figure><p>在编译busybox之前，我们需要对其进行设置，执行<code>make menuconfig</code>，如下</p><p><img src="https://raw.githubusercontent.com/wfloveiu/blogImage/main/img/202311201135953.png" alt="image-20231120113536905"></p><p><img src="https://raw.githubusercontent.com/wfloveiu/blogImage/main/img/202311201136740.png" alt="image-20231120113608696"></p><p>这里一定要选择<strong>静态编译</strong>，编译好的可执行文件<code>busybox</code>不依赖动态链接库，可以独立运行，方便构建initramfs。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">make -j 8</span><br><span class="line">make &amp;&amp; make install <span class="comment"># 安装完成后生成的相关文件会在 _install 目录下</span></span><br></pre></td></tr></table></figure><h2 id="构建initramfs根文件系统"><a href="#构建initramfs根文件系统" class="headerlink" title="构建initramfs根文件系统"></a>构建initramfs根文件系统</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在busybox压缩包的下载目录下,创建的该文件夹，该文件夹下有</span></span><br><span class="line"><span class="comment"># wufang@wufang:~/linux_kernel/kernel_compile$ ls</span></span><br><span class="line"><span class="comment"># busybox-1.36.1   busybox-1.36.1.tar.bz2   linux-5.14  linux-5.14.tar.gz</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">mkdir</span> initramfs</span><br><span class="line"><span class="built_in">cd</span> initramfs</span><br><span class="line"><span class="built_in">cp</span> ../busybox-1.29.0/_install/* -rf ./ <span class="comment">#将_install文件夹下的所有文件复制到initramfs文件夹下</span></span><br><span class="line"><span class="built_in">mkdir</span> dev proc sys</span><br><span class="line">sudo <span class="built_in">cp</span> -a /dev/&#123;null,console,<span class="built_in">tty</span>,tty1,tty2,tty3,tty4&#125; dev/</span><br><span class="line"><span class="built_in">rm</span> -f linuxrc</span><br><span class="line">vim init</span><br></pre></td></tr></table></figure><p>添加如下代码</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/busybox sh</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;&#123;==DBG==&#125; INIT SCRIPT&quot;</span></span><br><span class="line">mount -t proc none /proc</span><br><span class="line">mount -t sysfs none /sys</span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> -e <span class="string">&quot;&#123;==DBG==&#125; Boot took <span class="subst">$(cut -d&#x27; &#x27; -f1 /proc/uptime)</span> seconds&quot;</span></span><br><span class="line"><span class="built_in">exec</span> /sbin/init</span><br></pre></td></tr></table></figure><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">chmod</span> a+x init 修改文件权限</span><br><span class="line"><span class="comment"># 完成后，initrams下有如下文件</span></span><br><span class="line"><span class="comment"># bin   dev   init  proc  sbin  sys   usr</span></span><br></pre></td></tr></table></figure><p><strong>打包initramfs</strong></p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">find . -print0 | cpio --null -ov --format=newc | gzip -9 &gt; ../initramfs.cpio.gz</span><br><span class="line"><span class="comment"># 此时在busybox压缩包的下载目录下，有如下文件</span></span><br><span class="line"><span class="comment"># busybox-1.36.1          initramfs               linux-5.14</span></span><br><span class="line"><span class="comment"># busybox-1.36.1.tar.bz2  initramfs.cpio.gz       linux-5.14.tar.gz</span></span><br></pre></td></tr></table></figure><h2 id="启动Qemu调试内核"><a href="#启动Qemu调试内核" class="headerlink" title="启动Qemu调试内核"></a>启动Qemu调试内核</h2><p>上述完成之后，就可以启动Qemu来调试内核了,启动代码如下（是一个指令）</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">qemu-system-x86_64 -kernel ./arch/x86/boot/bzImage -initrd ../initramfs.cpio.gz -append <span class="string">&quot;nokaslr console=ttyS0&quot;</span> -s -S -nographic</span><br></pre></td></tr></table></figure><ul><li><code>qemu-system-x86_64</code>：指定是x86，64位;</li><li><code>-kernel ./arch/x86/boot/bzImage</code>：指定启用的内核镜像；</li><li><code>-initrd ../initramfs.cpio.gz</code>：指定启动的内存文件系统</li><li><code>-append &quot;nokaslr console=ttyS0&quot;</code> ：附加参数，其中 <code>nokaslr</code> 参数<strong>必须添加进来</strong>，防止内核起始地址随机化，这样会导致 gdb 断点不能命中；</li><li><code>-s</code> ：监听在 gdb 1234 端口；</li><li><code>-S</code> ：表示启动后就挂起，等待 gdb 连接；</li><li><code>-nographic</code>：不启动图形界面</li></ul><p>开启另一个命令行窗口，输入<strong>gdb</strong>，即可开启调试</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(gdb) target remote localhost:1234  <span class="comment">#连接qemu监听的端口</span></span><br><span class="line">(gdb) <span class="built_in">break</span> start_kernel      <span class="comment">#在start_kernel打断点</span></span><br><span class="line">(gdb) <span class="built_in">break</span>  rest_init        <span class="comment">#在res_init打断点</span></span><br><span class="line">(gdb) c                       <span class="comment">#运行到断点处</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;文章介绍了在虚拟机中配置和编译Linux内核的步骤。涵盖了虚拟机配置要求、开启SSH服务、内核编译的准备工作、解决编译中的报错、安装Qemu以及编译busybox等步骤。最后，通过构建initramfs根文件系统和使用Qemu启动内核进行调试，详细说明了连接和调试的步骤。文章还提供了丰富的命令行示例和截图，为读者提供了一份详尽的指南。&lt;/p&gt;</summary>
    
    
    
    <category term="技术学习" scheme="https://resume.kokomi0728.eu.org/categories/%E6%8A%80%E6%9C%AF%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="Linux" scheme="https://resume.kokomi0728.eu.org/tags/Linux/"/>
    
    <category term="C/C++" scheme="https://resume.kokomi0728.eu.org/tags/C-C/"/>
    
  </entry>
  
  <entry>
    <title>TF-IDF</title>
    <link href="https://resume.kokomi0728.eu.org/posts/d7d7e9a3.html"/>
    <id>https://resume.kokomi0728.eu.org/posts/d7d7e9a3.html</id>
    <published>2023-11-16T16:00:00.000Z</published>
    <updated>2023-11-23T14:15:58.274Z</updated>
    
    <content type="html"><![CDATA[<p>$TF$是指归一化后的词频，$IDF$是指逆文档频率。给定一个文档集合$D$，有$d_1,d_2,d_3, \cdots ,d_n \in D$。</p><span id="more"></span><p>文档集合总共包含$m$个词（<strong>注：一般在计算TF−IDF时会去除如“的”这一类的停用词</strong>），有$w_1,w_2,w_3,\cdots,w_m \in W$。我们现在以计算词$w_i$在文档$d_j$中的$TF-IDF$指为例。$TF$的计算公式为：</p><p>$$TF&#x3D;\frac{freq(i,j)}{max_{len}(j)}$$</p><p>在这里$freq(i,j)$为$w_i$在$d_j$中出现的频率，$max_{len}(j)$为$d_j$长度。</p><p>$TF$只能是描述词在文档中的频率，但假设现在有个词为”我们“，这个词可能在文档集$D$中每篇文档中都会出现，并且有较高的频率。那么这一类词就不具有很好的区分文档的能力，为了降低这种通用词的作用，引入了$IDF$。</p><p>$IDF$的表达式如下：</p><p>$$IDF&#x3D;log(\frac{len(D)}{n(i)})$$<br>在这里$len(D)$表示文档集合$D$中文档的总数，$n(i)$表示含有$w_i$这个词的文档的数量。</p><p>得到$TF$和$IDF$之后，我们将这两个值相乘得到$TF−IDF$的值：</p><p>$$TF-IDF&#x3D;TF \times IDF$$</p><p>$TF$可以计算在一篇文档中词出现的频率，而$IDF$可以降低一些通用词的作用。因此对于一篇文档我们可以用文档中每个词的$TF-IDF$组成的向量来表示该文档，再根据余弦相似度这类的方法来计算文档之间的相关性。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;$TF$是指归一化后的词频，$IDF$是指逆文档频率。给定一个文档集合$D$，有$d_1,d_2,d_3, &#92;cdots ,d_n &#92;in D$。&lt;/p&gt;</summary>
    
    
    
    <category term="论文学习" scheme="https://resume.kokomi0728.eu.org/categories/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="NLP" scheme="https://resume.kokomi0728.eu.org/tags/NLP/"/>
    
    <category term="信息检索" scheme="https://resume.kokomi0728.eu.org/tags/%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2/"/>
    
  </entry>
  
  <entry>
    <title>torch.nn (Convolution Layers)</title>
    <link href="https://resume.kokomi0728.eu.org/posts/7a1e28a0.html"/>
    <id>https://resume.kokomi0728.eu.org/posts/7a1e28a0.html</id>
    <published>2023-11-14T16:00:00.000Z</published>
    <updated>2023-12-01T09:33:11.845Z</updated>
    
    <content type="html"><![CDATA[<p>该文章介绍了在深度学习中使用 <code>torch.nn</code> 封装网络时常用的一些卷积层，包括 <code>Conv1d</code>、<code>Conv2d</code>、<code>Conv3d</code>、<code>ConvTranspose1d</code>、<code>ConvTranspose2d</code>、<code>ConvTranspose3d</code> 等。文章详细解释了每个层的参数和用法，并介绍了一些延迟初始化的卷积层，如 <code>LazyConv1d</code>、<code>LazyConv2d</code> 等，以及 <code>Unfold</code> 和 <code>Fold</code> 操作的用法。这对于深入理解卷积神经网络的构建和操作提供了实用的指南。</p><span id="more"></span><p>我们常用torch.nn来封装网络，torch.nn为我们封装好了很多神经网络中不同的层，如卷积层、池化层、归一化层等。我们会把这些层像是串成一个牛肉串一样串起来，形成网络。</p><p>先从最简单的，都有哪些层开始学起。</p><h2 id="Convolution-Layers-卷积层"><a href="#Convolution-Layers-卷积层" class="headerlink" title="Convolution Layers - 卷积层"></a>Convolution Layers - 卷积层</h2><h3 id="torch-nn-Conv1d"><a href="#torch-nn-Conv1d" class="headerlink" title="torch.nn.Conv1d()"></a>torch.nn.Conv1d()</h3><p>1维卷积层。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.Conv1d(in_channels, </span><br><span class="line">out_channels, </span><br><span class="line">kernel_size, </span><br><span class="line">stride=<span class="number">1</span>, </span><br><span class="line">padding=<span class="number">0</span>, </span><br><span class="line">dilation=<span class="number">1</span>, </span><br><span class="line">groups=<span class="number">1</span>, </span><br><span class="line">bias=<span class="literal">True</span>, </span><br><span class="line">padding_mode=<span class="string">&#x27;zeros&#x27;</span>, </span><br><span class="line">device=<span class="literal">None</span>, </span><br><span class="line">dtype=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure><p><code>in_channels</code>：输入tensor的通道数；<br><code>out_channels</code>：输出tensor的通道数；<br><code>kernel_size</code>：卷积核的大小；<br><code>stride</code>：步长；<br><code>padding</code>：输入tensor的边界填充尺寸；<br><code>dilation</code>：卷积核之间的间距（下面这个图为<code>dilation=2</code>），默认为1；<br><img src="https://img-blog.csdnimg.cn/e26e43f7139c445d9d42324f3f4c82af.png" alt="在这里插入图片描述"></p><p><code>groups</code>：从输入通道到输出通道的阻塞连接数。<code>in_channel</code>和<code>out_channel</code>需要能被<code>groups</code>整除。更具体地：<br><code>groups=1</code>时所有输入均与所有输出进行卷积，<code>groups=2</code>时该操作相当于并排设置两个卷积层，每卷积层看到一半的输入通道，产生一半的输出通道，然后将两个卷积层连接起来。<code>groups=in_channel</code>时输入的每个通道都和相应的卷积核进行卷积；<br><code>bias</code>：是否添加可学习的偏差值，True为添加，False为不添加。<br><code>padding_mode</code>：填充模式，有以下取值：<code>zeros</code>（这个是默认值）、<code>reflect</code>、<code>replicate</code>、<code>circular</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">m = nn.Conv1d(in_channels=<span class="number">16</span>,</span><br><span class="line">              out_channels=<span class="number">33</span>,</span><br><span class="line">              kernel_size=<span class="number">3</span>,</span><br><span class="line">              stride=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># input: 批大小为20，每个数据通道为16，size=50</span></span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">20</span>, <span class="number">16</span>, <span class="number">50</span>)</span><br><span class="line">output = m(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(output.size())</span><br></pre></td></tr></table></figure><p>输出</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># output: 批大小为20，每个数据通道为33，size=24</span></span><br><span class="line">torch.Size([<span class="number">20</span>, <span class="number">33</span>, <span class="number">24</span>])</span><br></pre></td></tr></table></figure><h3 id="torch-nn-Conv2d"><a href="#torch-nn-Conv2d" class="headerlink" title="torch.nn.Conv2d()"></a>torch.nn.Conv2d()</h3><p>2维卷积层。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.Conv2d(in_channels, </span><br><span class="line">    out_channels, </span><br><span class="line">    kernel_size, </span><br><span class="line">    stride=<span class="number">1</span>, </span><br><span class="line">    padding=<span class="number">0</span>, </span><br><span class="line">    dilation=<span class="number">1</span>, </span><br><span class="line">    groups=<span class="number">1</span>, </span><br><span class="line">    bias=<span class="literal">True</span>, </span><br><span class="line">    padding_mode=<span class="string">&#x27;zeros&#x27;</span>, </span><br><span class="line">    device=<span class="literal">None</span>, </span><br><span class="line">    dtype=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure><p>参数与Conv1d()基本一样，不再赘述。 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">m = nn.Conv2d(in_channels=<span class="number">2</span>,</span><br><span class="line">              out_channels=<span class="number">3</span>,</span><br><span class="line">              kernel_size=<span class="number">3</span>,</span><br><span class="line">              stride=<span class="number">2</span>)</span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">20</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">6</span>)</span><br><span class="line">output = m(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(output.size())</span><br></pre></td></tr></table></figure><p>输出</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([<span class="number">20</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">2</span>])</span><br></pre></td></tr></table></figure><h3 id="torch-nn-Conv3d"><a href="#torch-nn-Conv3d" class="headerlink" title="torch.nn.Conv3d()"></a>torch.nn.Conv3d()</h3><p>3维卷积层。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.Conv3d(in_channels, </span><br><span class="line">    out_channels, </span><br><span class="line">    kernel_size, </span><br><span class="line">    stride=<span class="number">1</span>, </span><br><span class="line">    padding=<span class="number">0</span>, </span><br><span class="line">    dilation=<span class="number">1</span>, </span><br><span class="line">    groups=<span class="number">1</span>, </span><br><span class="line">    bias=<span class="literal">True</span>, </span><br><span class="line">    padding_mode=<span class="string">&#x27;zeros&#x27;</span>, </span><br><span class="line">    device=<span class="literal">None</span>, </span><br><span class="line">    dtype=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure><p>参数与Conv1d()基本一样，不再赘述。 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">m = nn.Conv3d(in_channels=<span class="number">2</span>,</span><br><span class="line">              out_channels=<span class="number">3</span>,</span><br><span class="line">              kernel_size=<span class="number">3</span>,</span><br><span class="line">              stride=<span class="number">2</span>)</span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">20</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>)</span><br><span class="line">output = m(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(output.size())</span><br></pre></td></tr></table></figure><p>输出</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([<span class="number">20</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>])</span><br></pre></td></tr></table></figure><h3 id="torch-nn-ConvTranspose1d"><a href="#torch-nn-ConvTranspose1d" class="headerlink" title="torch.nn.ConvTranspose1d()"></a>torch.nn.ConvTranspose1d()</h3><p>1维转置卷积层。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.ConvTranspose1d(in_channels, </span><br><span class="line"> out_channels, </span><br><span class="line"> kernel_size, </span><br><span class="line"> stride=<span class="number">1</span>, </span><br><span class="line"> padding=<span class="number">0</span>, </span><br><span class="line"> output_padding=<span class="number">0</span>, </span><br><span class="line"> groups=<span class="number">1</span>, </span><br><span class="line"> bias=<span class="literal">True</span>, </span><br><span class="line"> dilation=<span class="number">1</span>, </span><br><span class="line"> padding_mode=<span class="string">&#x27;zeros&#x27;</span>, </span><br><span class="line"> device=<span class="literal">None</span>, </span><br><span class="line"> dtype=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure><p>参数与Conv1d()基本一样，不再赘述。<br>唯一不同的是<code>output_padding</code>，与<code>padding</code>不同的是，<code>output_padding</code>是输出tensor的每一个边，外面填充的层数。<br>（<code>padding</code>是输入tensor的每个边填充的层数）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">m = nn.ConvTranspose1d(in_channels=<span class="number">2</span>,</span><br><span class="line">                       out_channels=<span class="number">3</span>,</span><br><span class="line">                       kernel_size=<span class="number">3</span>,</span><br><span class="line">                       stride=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">20</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">output = m(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(output.size())</span><br></pre></td></tr></table></figure><p>输出</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([<span class="number">20</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br></pre></td></tr></table></figure><h3 id="torch-nn-ConvTranspose2d"><a href="#torch-nn-ConvTranspose2d" class="headerlink" title="torch.nn.ConvTranspose2d()"></a>torch.nn.ConvTranspose2d()</h3><p>2维转置卷积层。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.ConvTranspose2d(in_channels, </span><br><span class="line"> out_channels, </span><br><span class="line"> kernel_size, </span><br><span class="line"> stride=<span class="number">1</span>, </span><br><span class="line"> padding=<span class="number">0</span>, </span><br><span class="line"> output_padding=<span class="number">0</span>, </span><br><span class="line"> groups=<span class="number">1</span>, </span><br><span class="line"> bias=<span class="literal">True</span>, </span><br><span class="line"> dilation=<span class="number">1</span>, </span><br><span class="line"> padding_mode=<span class="string">&#x27;zeros&#x27;</span>, </span><br><span class="line"> device=<span class="literal">None</span>, </span><br><span class="line"> dtype=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure><p>参数与Conv1d()基本一样，不再赘述。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">m = nn.ConvTranspose2d(in_channels=<span class="number">2</span>,</span><br><span class="line">                       out_channels=<span class="number">3</span>,</span><br><span class="line">                       kernel_size=<span class="number">3</span>,</span><br><span class="line">                       stride=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">20</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">output = m(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(output.size())</span><br></pre></td></tr></table></figure><p>输出</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([<span class="number">20</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">4</span>])</span><br></pre></td></tr></table></figure><h3 id="torch-nn-ConvTranspose3d"><a href="#torch-nn-ConvTranspose3d" class="headerlink" title="torch.nn.ConvTranspose3d()"></a>torch.nn.ConvTranspose3d()</h3><p>3维转置卷积层。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.ConvTranspose3d(in_channels, </span><br><span class="line"> out_channels, </span><br><span class="line"> kernel_size, </span><br><span class="line"> stride=<span class="number">1</span>, </span><br><span class="line"> padding=<span class="number">0</span>, </span><br><span class="line"> output_padding=<span class="number">0</span>, </span><br><span class="line"> groups=<span class="number">1</span>, </span><br><span class="line"> bias=<span class="literal">True</span>, </span><br><span class="line"> dilation=<span class="number">1</span>, </span><br><span class="line"> padding_mode=<span class="string">&#x27;zeros&#x27;</span>, </span><br><span class="line"> device=<span class="literal">None</span>, </span><br><span class="line"> dtype=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure><p>参数与Conv1d()基本一样，不再赘述。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">m = nn.ConvTranspose3d(in_channels=<span class="number">2</span>,</span><br><span class="line">                       out_channels=<span class="number">3</span>,</span><br><span class="line">                       kernel_size=<span class="number">3</span>,</span><br><span class="line">                       stride=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">20</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">output = m(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(output.size())</span><br></pre></td></tr></table></figure><p>输出</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([<span class="number">20</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>])</span><br></pre></td></tr></table></figure><h3 id="torch-nn-LazyConv1d"><a href="#torch-nn-LazyConv1d" class="headerlink" title="torch.nn.LazyConv1d()"></a>torch.nn.LazyConv1d()</h3><p>1维延迟初始化卷积层，当in_channel不确定时可使用这个层。<br>关于延迟初始化，大家可以参考这篇文章，我认为讲的很好：<br><a href="https://blog.csdn.net/weixin_43180762/article/details/124299823">俱往矣… - 延迟初始化——【torch学习笔记】</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.LazyConv1d(out_channels, </span><br><span class="line">kernel_size, </span><br><span class="line">stride=<span class="number">1</span>, </span><br><span class="line">padding=<span class="number">0</span>, </span><br><span class="line">dilation=<span class="number">1</span>, </span><br><span class="line">groups=<span class="number">1</span>, </span><br><span class="line">bias=<span class="literal">True</span>, </span><br><span class="line">padding_mode=<span class="string">&#x27;zeros&#x27;</span>, </span><br><span class="line">device=<span class="literal">None</span>, </span><br><span class="line">dtype=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure><p><strong>LazyConv1d没有in_channel参数</strong>。<br>这不代表这个层没有输入的通道，而是在调用时自动适配，并进行初始化。<br>引用文章中的一段代码，改成LazyConv1d，讲述使用方法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    nn.LazyConv1d(<span class="number">256</span>, <span class="number">2</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Linear(<span class="number">9</span>, <span class="number">10</span>)</span><br><span class="line">)</span><br><span class="line"><span class="built_in">print</span>(net)</span><br><span class="line">[net[i].state_dict() <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(net))]</span><br><span class="line"></span><br><span class="line">low = torch.finfo(torch.float32).<span class="built_in">min</span> / <span class="number">10</span></span><br><span class="line">high = torch.finfo(torch.float32).<span class="built_in">max</span> / <span class="number">10</span></span><br><span class="line">X = torch.zeros([<span class="number">2</span>, <span class="number">20</span>, <span class="number">10</span>], dtype=torch.float32).uniform_(low, high)</span><br><span class="line">net(X)</span><br><span class="line"><span class="built_in">print</span>(net)</span><br></pre></td></tr></table></figure><p>输出</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Sequential(</span><br><span class="line">  (<span class="number">0</span>): LazyConv1d(<span class="number">0</span>, <span class="number">256</span>, kernel_size=(<span class="number">2</span>,), stride=(<span class="number">1</span>,))</span><br><span class="line">  (<span class="number">1</span>): ReLU()</span><br><span class="line">  (<span class="number">2</span>): Linear(in_features=<span class="number">9</span>, out_features=<span class="number">10</span>, bias=<span class="literal">True</span>)</span><br><span class="line">)</span><br><span class="line">Sequential(</span><br><span class="line">  (<span class="number">0</span>): Conv1d(<span class="number">20</span>, <span class="number">256</span>, kernel_size=(<span class="number">2</span>,), stride=(<span class="number">1</span>,))</span><br><span class="line">  (<span class="number">1</span>): ReLU()</span><br><span class="line">  (<span class="number">2</span>): Linear(in_features=<span class="number">9</span>, out_features=<span class="number">10</span>, bias=<span class="literal">True</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>可以看出，未进行初始化时，in_features&#x3D;0。只有传入参数使用网络后才会根据输入进行初始化。</p><h3 id="torch-nn-LazyConv2d"><a href="#torch-nn-LazyConv2d" class="headerlink" title="torch.nn.LazyConv2d()"></a>torch.nn.LazyConv2d()</h3><p>2维延迟初始化卷积层。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.LazyConv2d(out_channels, </span><br><span class="line">kernel_size, </span><br><span class="line">stride=<span class="number">1</span>, </span><br><span class="line">padding=<span class="number">0</span>, </span><br><span class="line">dilation=<span class="number">1</span>, </span><br><span class="line">groups=<span class="number">1</span>, </span><br><span class="line">bias=<span class="literal">True</span>, </span><br><span class="line">padding_mode=<span class="string">&#x27;zeros&#x27;</span>, </span><br><span class="line">device=<span class="literal">None</span>, </span><br><span class="line">dtype=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure><h3 id="torch-nn-LazyConv3d"><a href="#torch-nn-LazyConv3d" class="headerlink" title="torch.nn.LazyConv3d()"></a>torch.nn.LazyConv3d()</h3><p>3维延迟初始化卷积层。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.LazyConv3d(out_channels, </span><br><span class="line">kernel_size, </span><br><span class="line">stride=<span class="number">1</span>, </span><br><span class="line">padding=<span class="number">0</span>, </span><br><span class="line">dilation=<span class="number">1</span>, </span><br><span class="line">groups=<span class="number">1</span>, </span><br><span class="line">bias=<span class="literal">True</span>, </span><br><span class="line">padding_mode=<span class="string">&#x27;zeros&#x27;</span>, </span><br><span class="line">device=<span class="literal">None</span>, </span><br><span class="line">dtype=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure><h3 id="torch-nn-LazyConvTranspose1d"><a href="#torch-nn-LazyConvTranspose1d" class="headerlink" title="torch.nn.LazyConvTranspose1d()"></a>torch.nn.LazyConvTranspose1d()</h3><p>1维延迟初始化转置卷积层。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.LazyConvTranspose1d(out_channels, </span><br><span class="line"> kernel_size, </span><br><span class="line"> stride=<span class="number">1</span>, </span><br><span class="line"> padding=<span class="number">0</span>, </span><br><span class="line"> output_padding=<span class="number">0</span>, </span><br><span class="line"> groups=<span class="number">1</span>, </span><br><span class="line"> bias=<span class="literal">True</span>, </span><br><span class="line"> dilation=<span class="number">1</span>, </span><br><span class="line"> padding_mode=<span class="string">&#x27;zeros&#x27;</span>, </span><br><span class="line"> device=<span class="literal">None</span>, </span><br><span class="line"> dtype=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure><h3 id="torch-nn-LazyConvTranspose2d"><a href="#torch-nn-LazyConvTranspose2d" class="headerlink" title="torch.nn.LazyConvTranspose2d()"></a>torch.nn.LazyConvTranspose2d()</h3><p>2维延迟初始化转置卷积层。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.LazyConvTranspose2d(out_channels, </span><br><span class="line"> kernel_size, </span><br><span class="line"> stride=<span class="number">1</span>, </span><br><span class="line"> padding=<span class="number">0</span>, </span><br><span class="line"> output_padding=<span class="number">0</span>, </span><br><span class="line"> groups=<span class="number">1</span>, </span><br><span class="line"> bias=<span class="literal">True</span>, </span><br><span class="line"> dilation=<span class="number">1</span>, </span><br><span class="line"> padding_mode=<span class="string">&#x27;zeros&#x27;</span>, </span><br><span class="line"> device=<span class="literal">None</span>, </span><br><span class="line"> dtype=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure><h3 id="torch-nn-LazyConvTranspose3d"><a href="#torch-nn-LazyConvTranspose3d" class="headerlink" title="torch.nn.LazyConvTranspose3d()"></a>torch.nn.LazyConvTranspose3d()</h3><p>3维延迟初始化转置卷积层。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.LazyConvTranspose3d(out_channels, </span><br><span class="line"> kernel_size, </span><br><span class="line"> stride=<span class="number">1</span>, </span><br><span class="line"> padding=<span class="number">0</span>, </span><br><span class="line"> output_padding=<span class="number">0</span>, </span><br><span class="line"> groups=<span class="number">1</span>, </span><br><span class="line"> bias=<span class="literal">True</span>, </span><br><span class="line"> dilation=<span class="number">1</span>, </span><br><span class="line"> padding_mode=<span class="string">&#x27;zeros&#x27;</span>, </span><br><span class="line"> device=<span class="literal">None</span>, </span><br><span class="line"> dtype=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure><h3 id="torch-nn-Unfold"><a href="#torch-nn-Unfold" class="headerlink" title="torch.nn.Unfold()"></a>torch.nn.Unfold()</h3><p>从一个批次的输入张量中提取出滑动的局部区域块。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.Unfold(kernel_size, </span><br><span class="line">dilation=<span class="number">1</span>, </span><br><span class="line">padding=<span class="number">0</span>, </span><br><span class="line">stride=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p><code>kernel_size</code>：滑动块的大小；<br><code>dilation</code>：卷积核之间的间距(torch.nn.Conv1d中有图示)；<br><code>padding</code>：输入tensor的边界填充尺寸；<br><code>stride</code>：滑块滑动的步长。</p><p>这里的输入必须是4维的tensor，否则会报这样的错误：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">NotImplementedError: Input Error: Only 4D <span class="built_in">input</span> Tensors are supported (got 2D)</span><br></pre></td></tr></table></figure><p>示例</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">t = torch.tensor([[[[<span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">3.</span>,  <span class="number">4.</span>],</span><br><span class="line">                    [<span class="number">5.</span>,  <span class="number">6.</span>,  <span class="number">7.</span>,  <span class="number">8.</span>],</span><br><span class="line">                    [<span class="number">9.</span>,  <span class="number">10.</span>, <span class="number">11.</span>, <span class="number">12.</span>],</span><br><span class="line">                    [<span class="number">13.</span>, <span class="number">14.</span>, <span class="number">15.</span>, <span class="number">16.</span>],]]])</span><br><span class="line"></span><br><span class="line">unfold = nn.Unfold(kernel_size=(<span class="number">2</span>, <span class="number">2</span>), dilation=<span class="number">1</span>, padding=<span class="number">0</span>, stride=<span class="number">1</span>)</span><br><span class="line">output = unfold(t)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br></pre></td></tr></table></figure><p>输出</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[ <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">3.</span>,  <span class="number">5.</span>,  <span class="number">6.</span>,  <span class="number">7.</span>,  <span class="number">9.</span>, <span class="number">10.</span>, <span class="number">11.</span>],</span><br><span class="line">         [ <span class="number">2.</span>,  <span class="number">3.</span>,  <span class="number">4.</span>,  <span class="number">6.</span>,  <span class="number">7.</span>,  <span class="number">8.</span>, <span class="number">10.</span>, <span class="number">11.</span>, <span class="number">12.</span>],</span><br><span class="line">         [ <span class="number">5.</span>,  <span class="number">6.</span>,  <span class="number">7.</span>,  <span class="number">9.</span>, <span class="number">10.</span>, <span class="number">11.</span>, <span class="number">13.</span>, <span class="number">14.</span>, <span class="number">15.</span>],</span><br><span class="line">         [ <span class="number">6.</span>,  <span class="number">7.</span>,  <span class="number">8.</span>, <span class="number">10.</span>, <span class="number">11.</span>, <span class="number">12.</span>, <span class="number">14.</span>, <span class="number">15.</span>, <span class="number">16.</span>]]])</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/85ec11fae3dc464bb1877d941cbf777a.png" alt="在这里插入图片描述"></p><h3 id="torch-nn-Fold"><a href="#torch-nn-Fold" class="headerlink" title="torch.nn.Fold()"></a>torch.nn.Fold()</h3><p>Unfold()的逆操作。当Unfold()时出现滑块有重复覆盖时会导致结果和原来不一样。因为Fold()的过程中对于同一个位置的元素进行加法处理。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.Fold(output_size, </span><br><span class="line">  kernel_size, </span><br><span class="line">  dilation=<span class="number">1</span>, </span><br><span class="line">  padding=<span class="number">0</span>, </span><br><span class="line">  stride=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>下面是Unfold()和Fold()结合的代码，Unfold()部分和上面代码相同。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">t = torch.tensor([[[[<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>, <span class="number">4.</span>],</span><br><span class="line">                  [<span class="number">5.</span>, <span class="number">6.</span>, <span class="number">7.</span>, <span class="number">8.</span>],</span><br><span class="line">                  [<span class="number">9.</span>, <span class="number">10.</span>, <span class="number">11.</span>, <span class="number">12.</span>],</span><br><span class="line">                  [<span class="number">13.</span>, <span class="number">14.</span>, <span class="number">15.</span>, <span class="number">16.</span>]]]])</span><br><span class="line"></span><br><span class="line">unfold = nn.Unfold(kernel_size=(<span class="number">2</span>, <span class="number">2</span>), dilation=<span class="number">1</span>, padding=<span class="number">0</span>, stride=<span class="number">1</span>)</span><br><span class="line">output = unfold(t)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br><span class="line">fold = nn.Fold(output_size=(<span class="number">4</span>, <span class="number">4</span>), kernel_size=(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">out = fold(output)</span><br><span class="line"><span class="built_in">print</span>(out)</span><br></pre></td></tr></table></figure><p>输出</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[ <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">3.</span>,  <span class="number">5.</span>,  <span class="number">6.</span>,  <span class="number">7.</span>,  <span class="number">9.</span>, <span class="number">10.</span>, <span class="number">11.</span>],</span><br><span class="line">         [ <span class="number">2.</span>,  <span class="number">3.</span>,  <span class="number">4.</span>,  <span class="number">6.</span>,  <span class="number">7.</span>,  <span class="number">8.</span>, <span class="number">10.</span>, <span class="number">11.</span>, <span class="number">12.</span>],</span><br><span class="line">         [ <span class="number">5.</span>,  <span class="number">6.</span>,  <span class="number">7.</span>,  <span class="number">9.</span>, <span class="number">10.</span>, <span class="number">11.</span>, <span class="number">13.</span>, <span class="number">14.</span>, <span class="number">15.</span>],</span><br><span class="line">         [ <span class="number">6.</span>,  <span class="number">7.</span>,  <span class="number">8.</span>, <span class="number">10.</span>, <span class="number">11.</span>, <span class="number">12.</span>, <span class="number">14.</span>, <span class="number">15.</span>, <span class="number">16.</span>]]])</span><br><span class="line">tensor([[[[ <span class="number">1.</span>,  <span class="number">4.</span>,  <span class="number">6.</span>,  <span class="number">4.</span>],</span><br><span class="line">          [<span class="number">10.</span>, <span class="number">24.</span>, <span class="number">28.</span>, <span class="number">16.</span>],</span><br><span class="line">          [<span class="number">18.</span>, <span class="number">40.</span>, <span class="number">44.</span>, <span class="number">24.</span>],</span><br><span class="line">          [<span class="number">13.</span>, <span class="number">28.</span>, <span class="number">30.</span>, <span class="number">16.</span>]]]])</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;该文章介绍了在深度学习中使用 &lt;code&gt;torch.nn&lt;/code&gt; 封装网络时常用的一些卷积层，包括 &lt;code&gt;Conv1d&lt;/code&gt;、&lt;code&gt;Conv2d&lt;/code&gt;、&lt;code&gt;Conv3d&lt;/code&gt;、&lt;code&gt;ConvTranspose1d&lt;/code&gt;、&lt;code&gt;ConvTranspose2d&lt;/code&gt;、&lt;code&gt;ConvTranspose3d&lt;/code&gt; 等。文章详细解释了每个层的参数和用法，并介绍了一些延迟初始化的卷积层，如 &lt;code&gt;LazyConv1d&lt;/code&gt;、&lt;code&gt;LazyConv2d&lt;/code&gt; 等，以及 &lt;code&gt;Unfold&lt;/code&gt; 和 &lt;code&gt;Fold&lt;/code&gt; 操作的用法。这对于深入理解卷积神经网络的构建和操作提供了实用的指南。&lt;/p&gt;</summary>
    
    
    
    <category term="技术学习" scheme="https://resume.kokomi0728.eu.org/categories/%E6%8A%80%E6%9C%AF%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="PyTorch" scheme="https://resume.kokomi0728.eu.org/tags/PyTorch/"/>
    
    <category term="机器学习" scheme="https://resume.kokomi0728.eu.org/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>嵌入坍缩</title>
    <link href="https://resume.kokomi0728.eu.org/posts/3c1401a1.html"/>
    <id>https://resume.kokomi0728.eu.org/posts/3c1401a1.html</id>
    <published>2023-11-12T16:00:00.000Z</published>
    <updated>2023-11-23T14:15:58.251Z</updated>
    
    <content type="html"><![CDATA[<p>“Embedding collapse”（嵌入坍缩）是指在训练神经网络时，由于模型的复杂性或者数据分布的问题，导致嵌入空间中的不同输入被映射到相似的嵌入向量。这种情况会使得模型在处理不同输入时难以区分它们，因为它们被映射到了相似的表示。</p><span id="more"></span><p>嵌入坍缩通常是不希望出现的，因为模型需要在嵌入空间中保留输入数据的差异，以便正确地学习和泛化。嵌入坍缩可能导致模型在训练和测试时表现不佳，因为模型难以捕捉输入之间的微小差异。</p><p>以下是一些可能导致嵌入坍缩的原因：</p><ol><li><p><strong>数据不平衡：</strong> 如果训练数据中某些类别的样本数量远远超过其他类别，模型可能更容易将它们映射到相似的嵌入向量，而忽略了较少出现的类别。</p></li><li><p><strong>过度拟合：</strong> 当模型过度拟合训练数据时，它可能会学到一些特定的模式，而不是泛化到更广泛的情况。这可能导致嵌入坍缩，使得相似的输入被映射到相似的表示。</p></li><li><p><strong>模型复杂性：</strong> 过于复杂的模型可能会倾向于将输入映射到一个较小的嵌入空间，导致嵌入坍缩。这可能发生在具有大量参数的深度神经网络中。</p></li></ol><p>为了缓解嵌入坍缩的问题，可以尝试以下方法：</p><ul><li><p><strong>正则化：</strong> 添加正则化项，如L1或L2正则化，以防止模型学习过于复杂的表示。</p></li><li><p><strong>数据增强：</strong> 通过对训练数据进行变换或增强，引入更多的差异性，帮助模型学到更鲁棒的表示。</p></li><li><p><strong>平衡数据：</strong> 确保训练数据中的不同类别样本数量相对平衡，以防止某些类别的过度表示。</p></li><li><p><strong>监控训练过程：</strong> 使用监控工具和可视化方法来检查模型在嵌入空间中的学习情况，及时发现和解决嵌入坍缩问题。</p></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;“Embedding collapse”（嵌入坍缩）是指在训练神经网络时，由于模型的复杂性或者数据分布的问题，导致嵌入空间中的不同输入被映射到相似的嵌入向量。这种情况会使得模型在处理不同输入时难以区分它们，因为它们被映射到了相似的表示。&lt;/p&gt;</summary>
    
    
    
    <category term="论文学习" scheme="https://resume.kokomi0728.eu.org/categories/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="NLP" scheme="https://resume.kokomi0728.eu.org/tags/NLP/"/>
    
    <category term="信息检索" scheme="https://resume.kokomi0728.eu.org/tags/%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2/"/>
    
  </entry>
  
  <entry>
    <title>GLUE</title>
    <link href="https://resume.kokomi0728.eu.org/posts/9f7dd732.html"/>
    <id>https://resume.kokomi0728.eu.org/posts/9f7dd732.html</id>
    <published>2023-11-12T16:00:00.000Z</published>
    <updated>2023-11-23T14:15:58.263Z</updated>
    
    <content type="html"><![CDATA[<p>GLUE和SUPER-GLUE是用于衡量自然语言处理模型表现的两个基准测试体系:</p><span id="more"></span><p>GLUE(General Language Understanding Evaluation):</p><ul><li>由谷歌 AI 实验室于2018年提出。</li><li>包括9个不同类型的NLP任务,如句子推理、naming等。 </li><li>旨在评估模型在各种常见NLP任务上的泛化能力。</li></ul><p>SUPER-GLUE:</p><ul><li>是GLUE基准测试的升级版,由同一团队于2019年发布。</li><li>增加和改进了一些GLUE任务,加入了更难更复杂的新任务。</li><li>任务包括:想象性问答、多项选择、修辞关系等难点任务。</li><li>对模型的理解和推理能力要求更高。</li></ul><p>两个基准测试的主要区别:</p><ul><li>SUPER-GLUE的任务设置更难,对模型的要求更高。</li><li>可以更全面和细致地检测模型在不同程度的NLP任务中的表现。</li><li>成为评估新一代更强大NLP模型的主流标准,如BERT、GPT-3等。</li></ul><p>因此,在最新的研究中,模型表现都以其在GLUE和SUPER-GLUE上的效果进行报告,成为NLP进步的关键指标之一。</p><p>GLUE（General Language Understanding Evaluation）是一个用于评估自然语言处理（NLP）模型性能的基准测试集合。GLUE基准测试涵盖了多个任务，包括文本分类、语义相似度、自然语言推断等。下面是GLUE基准测试中包括的九个任务的详细解释：</p><ol><li><p><strong>CoLA（Corpus of Linguistic Acceptability）:</strong></p><ul><li><strong>任务：</strong> 判断给定的句子是否在语法和语义上是可接受的。</li><li><strong>示例：</strong> “He is playing the piano.”（可接受） vs. “Him play piano.”（不可接受）</li></ul></li><li><p><strong>SST-2（Stanford Sentiment Treebank）:</strong></p><ul><li><strong>任务：</strong> 对给定的句子进行情感分类，判断情感是正面、负面还是中性的。</li><li><strong>示例：</strong> “I love this product!”（正面） vs. “This is the worst movie ever.”（负面）</li></ul></li><li><p><strong>MRPC（Microsoft Research Paraphrase Corpus）:</strong></p><ul><li><strong>任务：</strong> 判断给定的两个句子是否是语义上相似的。</li><li><strong>示例：</strong> “The cat is on the mat.” vs. “The cat is lying on the mat.”（相似）</li></ul></li><li><p><strong>STS-B（Semantic Textual Similarity Benchmark）:</strong></p><ul><li><strong>任务：</strong> 对给定的两个句子进行语义相似度评分。</li><li><strong>示例：</strong> “A man is playing a large flute.” vs. “A man is playing a flute.”（相似度得分）</li></ul></li><li><p><strong>QQP（Quora Question Pairs）:</strong></p><ul><li><strong>任务：</strong> 判断给定的两个问题是否是语义上相似的。</li><li><strong>示例：</strong> “How can I be a good geologist?” vs. “What should I do to be a great geologist?”（相似）</li></ul></li><li><p><strong>MNLI（MultiNLI）:</strong></p><ul><li><strong>任务：</strong> 自然语言推断任务，给定一个前提句子和一个假设句子，判断假设在给定前提下是蕴含、矛盾还是中立关系。</li><li><strong>示例：</strong> Premise: “The cat is sitting on the windowsill.”， Hypothesis: “The cat is outside.”（矛盾）</li></ul></li><li><p><strong>QNLI（Question Natural Language Inference）:</strong></p><ul><li><strong>任务：</strong> 是MNLI任务的变体，专注于问题和短文本之间的推断关系。</li></ul></li><li><p><strong>RTE（Recognizing Textual Entailment）:</strong></p><ul><li><strong>任务：</strong> 自然语言推断任务，判断给定的两个文本之间是否存在蕴涵关系。</li><li><strong>示例：</strong> Text: “A soccer game with multiple males playing.”， Hypothesis: “Some men are playing a sport.”（蕴涵）</li></ul></li></ol><p>以上是GLUE基准测试中包括的九个任务，每个任务都有一组训练和测试数据，用于评估模型在不同自然语言处理任务上的性能。这些任务旨在涵盖多样的自然语言理解方面，从而全面评估模型的通用性。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;GLUE和SUPER-GLUE是用于衡量自然语言处理模型表现的两个基准测试体系:&lt;/p&gt;</summary>
    
    
    
    <category term="论文学习" scheme="https://resume.kokomi0728.eu.org/categories/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="NLP" scheme="https://resume.kokomi0728.eu.org/tags/NLP/"/>
    
    <category term="信息检索" scheme="https://resume.kokomi0728.eu.org/tags/%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2/"/>
    
  </entry>
  
  <entry>
    <title>L1正则化</title>
    <link href="https://resume.kokomi0728.eu.org/posts/18e73c3c.html"/>
    <id>https://resume.kokomi0728.eu.org/posts/18e73c3c.html</id>
    <published>2023-11-12T16:00:00.000Z</published>
    <updated>2023-11-23T14:15:58.266Z</updated>
    
    <content type="html"><![CDATA[<p>L1正则化是一种在机器学习中用于降低模型复杂度的技术。它通过向模型的损失函数添加L1范数（向量中各元素绝对值的和）的惩罚来实现。L1正则化的目的是促使模型学习到稀疏权重，即让一些特征对应的权重变为零，从而减少模型的复杂性。</p><span id="more"></span><p>对于一个线性回归模型，L1正则化的损失函数可以写成：</p><p>$$ J(\theta) &#x3D; \text{MSE}(\theta) + \lambda \sum_{i&#x3D;1}^{n} | \theta_i | $$</p><p>其中：</p><ul><li>$J(\theta)$ 是带有L1正则化的损失函数；</li><li>$\text{MSE}(\theta)$ 是均方误差（Mean Squared Error）损失函数，用于衡量模型的预测与实际值之间的差距；</li><li>$\lambda$ 是正则化强度，控制着正则化对总损失的影响；</li><li>$\sum_{i&#x3D;1}^{n} | \theta_i |$ 是模型权重向量 (\theta) 中所有权重的绝对值之和。</li></ul><p>L1正则化的效果是，一些特征对应的权重会变为零，从而达到特征选择（feature selection）的效果。这使得模型更加简单，减少了过拟合的风险，并提高了模型的泛化能力。</p><p>在深度学习中，L1正则化同样可以应用于神经网络的权重，通过控制权重的稀疏性来提高模型的泛化性能。在实践中，通常使用梯度下降等优化算法来最小化包含L1正则项的损失函数。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;L1正则化是一种在机器学习中用于降低模型复杂度的技术。它通过向模型的损失函数添加L1范数（向量中各元素绝对值的和）的惩罚来实现。L1正则化的目的是促使模型学习到稀疏权重，即让一些特征对应的权重变为零，从而减少模型的复杂性。&lt;/p&gt;</summary>
    
    
    
    <category term="论文学习" scheme="https://resume.kokomi0728.eu.org/categories/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="机器学习" scheme="https://resume.kokomi0728.eu.org/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="数学" scheme="https://resume.kokomi0728.eu.org/tags/%E6%95%B0%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>L2归一化</title>
    <link href="https://resume.kokomi0728.eu.org/posts/ff9e5701.html"/>
    <id>https://resume.kokomi0728.eu.org/posts/ff9e5701.html</id>
    <published>2023-11-12T16:00:00.000Z</published>
    <updated>2023-11-23T14:15:58.268Z</updated>
    
    <content type="html"><![CDATA[<p>L2归一化，也称为L2范数归一化，是一种向量归一化的方法，它将向量的每个元素除以其L2范数（Euclidean范数）。</p><span id="more"></span><p>L2范数是向量元素的平方和的平方根。对于一个向量 ($x &#x3D; [x_1, x_2, …, x_n]$)，其L2范数表示为：<br>$$ |x|_2 &#x3D; \sqrt{x_1^2 + x_2^2 + … + x_n^2} $$</p><p>L2归一化的过程是将向量的每个元素除以其L2范数，得到一个具有单位长度的向量。具体而言，对于向量 $x$，它的L2归一化 $\hat{x}$ 计算如下：</p><p>$$ \hat{x} &#x3D; \frac{x}{|x|_2} $$</p><p>L2归一化的目标是将向量的模（长度）缩放到1，从而使得向量在空间中的表示更加规范化，便于比较和处理。在深度学习中，L2归一化有时被用作正则化的手段，可以帮助控制模型的复杂度，防止过拟合。</p><p>在机器学习和深度学习中，除了L2归一化，还有L1归一化和Lp范数归一化等不同的向量归一化方法，用于处理不同的问题和优化需求。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;L2归一化，也称为L2范数归一化，是一种向量归一化的方法，它将向量的每个元素除以其L2范数（Euclidean范数）。&lt;/p&gt;</summary>
    
    
    
    <category term="论文学习" scheme="https://resume.kokomi0728.eu.org/categories/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="机器学习" scheme="https://resume.kokomi0728.eu.org/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="数学" scheme="https://resume.kokomi0728.eu.org/tags/%E6%95%B0%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>NLI</title>
    <link href="https://resume.kokomi0728.eu.org/posts/d68980e5.html"/>
    <id>https://resume.kokomi0728.eu.org/posts/d68980e5.html</id>
    <published>2023-11-12T16:00:00.000Z</published>
    <updated>2023-11-23T14:15:58.270Z</updated>
    
    <content type="html"><![CDATA[<p>NLI 是 “Natural Language Inference”（自然语言推断）的缩写。这是一种自然语言处理（NLP）任务，旨在评估一个系统对于给定的两个句子之间的关系的理解能力。通常情况下，这个任务被定义为三个类别的分类问题：</p><span id="more"></span><ol><li><p><strong>蕴涵（Entailment）：</strong> 如果一个句子可以从另一个句子中推断出来，那么这两个句子之间存在蕴涵关系。例如，如果第一个句子是 “狗在跑步”，第二个句子是 “动物在运动”，那么我们可以说第一个句子蕴含（entails）第二个句子。</p></li><li><p><strong>矛盾（Contradiction）：</strong> 如果两个句子之间存在逻辑上的矛盾，那么它们之间存在矛盾关系。例如，如果第一个句子是 “太阳从东方升起”，第二个句子是 “太阳从西方升起”，那么这两个句子之间存在矛盾关系。</p></li><li><p><strong>中性（Neutral）：</strong> 如果两个句子之间既不是蕴涵关系，也不是矛盾关系，那么它们之间是中性关系。例如，第一个句子是 “猫喜欢晒太阳”，第二个句子是 “天空是蓝色的”，这两个句子之间可能没有明显的蕴涵或矛盾关系。</p></li></ol><p>NLI 是自然语言处理中一个重要的任务，它具有广泛的应用，包括问答系统、机器翻译、对话系统等。训练一个在 NLI 任务上表现良好的模型可以提升系统对语义关系的理解能力，从而在各种应用中提供更准确的自然语言理解。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;NLI 是 “Natural Language Inference”（自然语言推断）的缩写。这是一种自然语言处理（NLP）任务，旨在评估一个系统对于给定的两个句子之间的关系的理解能力。通常情况下，这个任务被定义为三个类别的分类问题：&lt;/p&gt;</summary>
    
    
    
    <category term="论文学习" scheme="https://resume.kokomi0728.eu.org/categories/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="NLP" scheme="https://resume.kokomi0728.eu.org/tags/NLP/"/>
    
    <category term="信息检索" scheme="https://resume.kokomi0728.eu.org/tags/%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2/"/>
    
  </entry>
  
</feed>
