{"posts":[{"title":"哈尔滨工程大学——语料库智能检索系统","text":"For：哈尔滨工程大学国际合作教育学院 校内访问地址：哈尔滨工程大学语料库智能检索系统","link":"/2023/10/08/HEU%20Corpus%20Web/"},{"title":"基于深度学习的多功能图像处理和社交平台","text":"Contribution：完成基于CNN网络将输入的内容图像与风格图像的目标特征提取，实现画风迁移功能；基于NoGAN生成对抗网络模型进行生成器的预训练与评价，实现黑白图像色彩填充；负责服务端处理任务调度和网页端开发 Award：人工智能创意赛东北赛区二等奖","link":"/2023/10/08/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%A4%9A%E5%8A%9F%E8%83%BD%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E5%92%8C%E7%A4%BE%E4%BA%A4%E5%B9%B3%E5%8F%B0/"},{"title":"MoHub平台支持工科教学","text":"From: 工业知识模型互联平台-MoHub 原文链接：哈工程李超老师-计算机逻辑设计综合实验课程","link":"/2023/10/07/MoHub%E5%B9%B3%E5%8F%B0%E6%94%AF%E6%8C%81%E5%B7%A5%E7%A7%91%E6%95%99%E5%AD%A6/"},{"title":"海洋鹰眼——舰船识别与海况观测系统","text":"Background：旨在准确有效的海上船舶识别技术以提高船舶航行安全，是船舶智能化发展的关键技术 Contribution：负责数据标注和强化数据样本工作，使用YOLOv5模型完成目标检测与船只定位，检测准确率达到67.26% Award：“建行杯”第九届中国国际“互联网+”大学生创新创业大赛省级铜奖","link":"/2023/10/08/%E6%B5%B7%E6%B4%8B%E9%B9%B0%E7%9C%BC%E2%80%94%E2%80%94%E8%88%B0%E8%88%B9%E8%AF%86%E5%88%AB%E4%B8%8E%E6%B5%B7%E5%86%B5%E8%A7%82%E6%B5%8B%E7%B3%BB%E7%BB%9F/"},{"title":"基于音频特诊分析的动态鸟类识别分析","text":"Background：旨在提供轻量化的鸟类音频识别与分类工具，帮助森林工作者保护生物多样性 Contribution：使用Kaggle Bird CLEF 2023比赛中的数据集，基于Mel频谱特征提取和EfficientNet-B4模型实现对鸟类音频的自动分类 Award：中国大学生计算机设计大赛黑龙江赛区省级三等奖","link":"/2023/10/08/%E5%9F%BA%E4%BA%8E%E9%9F%B3%E9%A2%91%E7%89%B9%E8%AF%8A%E5%88%86%E6%9E%90%E7%9A%84%E5%8A%A8%E6%80%81%E9%B8%9F%E7%B1%BB%E8%AF%86%E5%88%AB%E5%88%86%E6%9E%90/"},{"title":"ANN检索","text":"ANN（Approximate Nearest Neighbors）检索是一种用于寻找给定查询点附近近似最近邻的技术。在大规模数据集中，寻找最近邻（nearest neighbors）是一个计算密集型任务，尤其是当数据集的维度较高时。ANN检索的目标是以更高的效率找到近似的最近邻，而不一定要找到确切的最近邻。 ANN检索常用于各种机器学习、数据挖掘和信息检索任务中。特别是在大规模数据集中，ANN检索技术可以大幅减少搜索时间，从而提高算法的效率。 ANN检索的方法有很多种，其中包括但不限于： 树结构方法：例如KD树、Ball 树、VP 树、M 树等，这些方法通过构建树形结构来快速搜索近似最近邻。 哈希方法：例如局部敏感哈希（LSH），LSH使用哈希函数将数据点映射到桶（buckets），从而加速最近邻的搜索。 近似搜索算法：例如最近邻算法（FLANN），这些算法采用近似的方法来快速查找最近邻。 这些技术可以在不牺牲太多精度的情况下，在大型数据集中实现高效的最近邻搜索。ANN检索通常用于提供近似的解决方案，对于许多应用而言，高效的近似结果已经足够满足需求，并且能大大减少搜索时间和计算资源的消耗。","link":"/2023/11/03/%E7%9F%A5%E8%AF%86%E7%82%B9/%E3%80%90%E7%9F%A5%E8%AF%86%E7%82%B9%E3%80%91ANN%E6%A3%80%E7%B4%A2/"},{"title":"基于计算机视觉的盲人视觉辅助智能眼镜","text":"Background：旨在提升盲人和视障人士的生活质量，超越传统导盲辅助工具 Contribution：使用COCO 2017 数据集，基于YOLOv3模型完成目标检测和移动端导盲App设计与开发 Award：“建行杯”第九届中国国际“互联网+”大学生创新创业大赛省级银奖","link":"/2023/10/08/%E5%9F%BA%E4%BA%8E%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%9A%84%E7%9B%B2%E4%BA%BA%E8%A7%86%E8%A7%89%E8%BE%85%E5%8A%A9%E6%99%BA%E8%83%BD%E7%9C%BC%E9%95%9C/"},{"title":"嵌入坍缩","text":"“Embedding collapse”（嵌入坍缩）是指在训练神经网络时，由于模型的复杂性或者数据分布的问题，导致嵌入空间中的不同输入被映射到相似的嵌入向量。这种情况会使得模型在处理不同输入时难以区分它们，因为它们被映射到了相似的表示。 嵌入坍缩通常是不希望出现的，因为模型需要在嵌入空间中保留输入数据的差异，以便正确地学习和泛化。嵌入坍缩可能导致模型在训练和测试时表现不佳，因为模型难以捕捉输入之间的微小差异。 以下是一些可能导致嵌入坍缩的原因： 数据不平衡： 如果训练数据中某些类别的样本数量远远超过其他类别，模型可能更容易将它们映射到相似的嵌入向量，而忽略了较少出现的类别。 过度拟合： 当模型过度拟合训练数据时，它可能会学到一些特定的模式，而不是泛化到更广泛的情况。这可能导致嵌入坍缩，使得相似的输入被映射到相似的表示。 模型复杂性： 过于复杂的模型可能会倾向于将输入映射到一个较小的嵌入空间，导致嵌入坍缩。这可能发生在具有大量参数的深度神经网络中。 为了缓解嵌入坍缩的问题，可以尝试以下方法： 正则化： 添加正则化项，如L1或L2正则化，以防止模型学习过于复杂的表示。 数据增强： 通过对训练数据进行变换或增强，引入更多的差异性，帮助模型学到更鲁棒的表示。 平衡数据： 确保训练数据中的不同类别样本数量相对平衡，以防止某些类别的过度表示。 监控训练过程： 使用监控工具和可视化方法来检查模型在嵌入空间中的学习情况，及时发现和解决嵌入坍缩问题。","link":"/2023/11/13/%E7%9F%A5%E8%AF%86%E7%82%B9/%E3%80%90%E7%9F%A5%E8%AF%86%E7%82%B9%E3%80%91Embedding%20collapse/"},{"title":"GLUE","text":"GLUE和SUPER-GLUE是用于衡量自然语言处理模型表现的两个基准测试体系: GLUE(General Language Understanding Evaluation): 由谷歌 AI 实验室于2018年提出。 包括9个不同类型的NLP任务,如句子推理、naming等。 旨在评估模型在各种常见NLP任务上的泛化能力。 SUPER-GLUE: 是GLUE基准测试的升级版,由同一团队于2019年发布。 增加和改进了一些GLUE任务,加入了更难更复杂的新任务。 任务包括:想象性问答、多项选择、修辞关系等难点任务。 对模型的理解和推理能力要求更高。 两个基准测试的主要区别: SUPER-GLUE的任务设置更难,对模型的要求更高。 可以更全面和细致地检测模型在不同程度的NLP任务中的表现。 成为评估新一代更强大NLP模型的主流标准,如BERT、GPT-3等。 因此,在最新的研究中,模型表现都以其在GLUE和SUPER-GLUE上的效果进行报告,成为NLP进步的关键指标之一。 GLUE（General Language Understanding Evaluation）是一个用于评估自然语言处理（NLP）模型性能的基准测试集合。GLUE基准测试涵盖了多个任务，包括文本分类、语义相似度、自然语言推断等。下面是GLUE基准测试中包括的九个任务的详细解释： CoLA（Corpus of Linguistic Acceptability）: 任务： 判断给定的句子是否在语法和语义上是可接受的。 示例： “He is playing the piano.”（可接受） vs. “Him play piano.”（不可接受） SST-2（Stanford Sentiment Treebank）: 任务： 对给定的句子进行情感分类，判断情感是正面、负面还是中性的。 示例： “I love this product!”（正面） vs. “This is the worst movie ever.”（负面） MRPC（Microsoft Research Paraphrase Corpus）: 任务： 判断给定的两个句子是否是语义上相似的。 示例： “The cat is on the mat.” vs. “The cat is lying on the mat.”（相似） STS-B（Semantic Textual Similarity Benchmark）: 任务： 对给定的两个句子进行语义相似度评分。 示例： “A man is playing a large flute.” vs. “A man is playing a flute.”（相似度得分） QQP（Quora Question Pairs）: 任务： 判断给定的两个问题是否是语义上相似的。 示例： “How can I be a good geologist?” vs. “What should I do to be a great geologist?”（相似） MNLI（MultiNLI）: 任务： 自然语言推断任务，给定一个前提句子和一个假设句子，判断假设在给定前提下是蕴含、矛盾还是中立关系。 示例： Premise: “The cat is sitting on the windowsill.”， Hypothesis: “The cat is outside.”（矛盾） QNLI（Question Natural Language Inference）: 任务： 是MNLI任务的变体，专注于问题和短文本之间的推断关系。 RTE（Recognizing Textual Entailment）: 任务： 自然语言推断任务，判断给定的两个文本之间是否存在蕴涵关系。 示例： Text: “A soccer game with multiple males playing.”， Hypothesis: “Some men are playing a sport.”（蕴涵） 以上是GLUE基准测试中包括的九个任务，每个任务都有一组训练和测试数据，用于评估模型在不同自然语言处理任务上的性能。这些任务旨在涵盖多样的自然语言理解方面，从而全面评估模型的通用性。","link":"/2023/11/13/%E7%9F%A5%E8%AF%86%E7%82%B9/%E3%80%90%E7%9F%A5%E8%AF%86%E7%82%B9%E3%80%91GLUE/"},{"title":"BM25算法","text":"$BM25$算法通常用来做搜索相关性评分的，也是$ES$中的搜索算法，通常用来计算$query$和文本集合$D$中每篇文本之间的相关性。 我们用$Q$表示$query$，在这里$Q$一般是一个句子。在这里我们要对$Q$进行语素解析（一般是分词），在这里以分词为例，我们对$Q$进行分词，得到$q_1,q_2,\\cdots,q_t$这样一个词序列。给定文本$d \\in D$，现在以计算$Q$和$d$之间的分数（相关性），其表达式如下：$$Score(Q,d)=\\sum\\limits^t_{i=1}w_i\\times R(q_i,d)$$上面式子中$w_i$表示$q_i$的权重，$R(q_i,d)$为$q_i$和$d$的相关性，$Score(Q,d)$就是每个语素$q_i$和$d$的相关性的加权和。 $w_i$的计算方法有很多，一般是用$IDF$来表示的，但这里的$IDF$计算和上面的有所不同，具体的表达式如下：$$w_i=IDF(q_i)=log\\frac{N-n(q_i)+0.5}{n(qi)+0.5}$$上面式子中$N$表示文本集合中文本的总数量，$n(q_i)$表示包含$q_i$这个词的文本的数量，$0.5$主要是做平滑处理。 $R(q_i,d)$的计算公式如下： $$R(q_i,d)=\\frac{f_i \\times (k_1+1)}{f_i+K}\\times \\frac{qf_i\\times(k_2+1)}{qf_i+k_2}$$其中 $$K=k_1\\times(1−b+b\\times \\frac{dl}{avgdl})$$ 上面式子中$f_i$为$q_i$在文本$d$中出现的频率，$qf_i$为$q_i$在$Q$中出现的频率，$k_1,k_2,b$都是可调节的参数，$dl,avgdl$分别为文本$d$的长度和文本集$D$中所有文本的平均长度。 一般$qf_i=1$，取$k_2=0$，则可以去除后一项，将上面式子改写成： $$R(q_i,d)=\\frac{f_i\\times(k_1+1)}{f_i+K}$$ 通常设置$k_1=2,b=0.75$。参数$b$的作用主要是调节文本长度对相关性的影响。","link":"/2023/11/01/%E7%9F%A5%E8%AF%86%E7%82%B9/%E3%80%90%E7%9F%A5%E8%AF%86%E7%82%B9%E3%80%91BM25%E7%AE%97%E6%B3%95/"},{"title":"L1正则化","text":"L1正则化是一种在机器学习中用于降低模型复杂度的技术。它通过向模型的损失函数添加L1范数（向量中各元素绝对值的和）的惩罚来实现。L1正则化的目的是促使模型学习到稀疏权重，即让一些特征对应的权重变为零，从而减少模型的复杂性。 对于一个线性回归模型，L1正则化的损失函数可以写成： $$ J(\\theta) = \\text{MSE}(\\theta) + \\lambda \\sum_{i=1}^{n} | \\theta_i | $$ 其中： $J(\\theta)$ 是带有L1正则化的损失函数； $\\text{MSE}(\\theta)$ 是均方误差（Mean Squared Error）损失函数，用于衡量模型的预测与实际值之间的差距； $\\lambda$ 是正则化强度，控制着正则化对总损失的影响； $\\sum_{i=1}^{n} | \\theta_i |$ 是模型权重向量 (\\theta) 中所有权重的绝对值之和。 L1正则化的效果是，一些特征对应的权重会变为零，从而达到特征选择（feature selection）的效果。这使得模型更加简单，减少了过拟合的风险，并提高了模型的泛化能力。 在深度学习中，L1正则化同样可以应用于神经网络的权重，通过控制权重的稀疏性来提高模型的泛化性能。在实践中，通常使用梯度下降等优化算法来最小化包含L1正则项的损失函数。","link":"/2023/11/13/%E7%9F%A5%E8%AF%86%E7%82%B9/%E3%80%90%E7%9F%A5%E8%AF%86%E7%82%B9%E3%80%91L1%E6%AD%A3%E5%88%99%E5%8C%96/"},{"title":"L2归一化","text":"L2归一化，也称为L2范数归一化，是一种向量归一化的方法，它将向量的每个元素除以其L2范数（Euclidean范数）。 L2范数是向量元素的平方和的平方根。对于一个向量 ($x = [x_1, x_2, …, x_n]$)，其L2范数表示为：$$ |x|_2 = \\sqrt{x_1^2 + x_2^2 + … + x_n^2} $$ L2归一化的过程是将向量的每个元素除以其L2范数，得到一个具有单位长度的向量。具体而言，对于向量 $x$，它的L2归一化 $\\hat{x}$ 计算如下： $$ \\hat{x} = \\frac{x}{|x|_2} $$ L2归一化的目标是将向量的模（长度）缩放到1，从而使得向量在空间中的表示更加规范化，便于比较和处理。在深度学习中，L2归一化有时被用作正则化的手段，可以帮助控制模型的复杂度，防止过拟合。 在机器学习和深度学习中，除了L2归一化，还有L1归一化和Lp范数归一化等不同的向量归一化方法，用于处理不同的问题和优化需求。","link":"/2023/11/13/%E7%9F%A5%E8%AF%86%E7%82%B9/%E3%80%90%E7%9F%A5%E8%AF%86%E7%82%B9%E3%80%91L2%E5%BD%92%E4%B8%80%E5%8C%96/"},{"title":"NLI","text":"NLI 是 “Natural Language Inference”（自然语言推断）的缩写。这是一种自然语言处理（NLP）任务，旨在评估一个系统对于给定的两个句子之间的关系的理解能力。通常情况下，这个任务被定义为三个类别的分类问题： 蕴涵（Entailment）： 如果一个句子可以从另一个句子中推断出来，那么这两个句子之间存在蕴涵关系。例如，如果第一个句子是 “狗在跑步”，第二个句子是 “动物在运动”，那么我们可以说第一个句子蕴含（entails）第二个句子。 矛盾（Contradiction）： 如果两个句子之间存在逻辑上的矛盾，那么它们之间存在矛盾关系。例如，如果第一个句子是 “太阳从东方升起”，第二个句子是 “太阳从西方升起”，那么这两个句子之间存在矛盾关系。 中性（Neutral）： 如果两个句子之间既不是蕴涵关系，也不是矛盾关系，那么它们之间是中性关系。例如，第一个句子是 “猫喜欢晒太阳”，第二个句子是 “天空是蓝色的”，这两个句子之间可能没有明显的蕴涵或矛盾关系。 NLI 是自然语言处理中一个重要的任务，它具有广泛的应用，包括问答系统、机器翻译、对话系统等。训练一个在 NLI 任务上表现良好的模型可以提升系统对语义关系的理解能力，从而在各种应用中提供更准确的自然语言理解。","link":"/2023/11/13/%E7%9F%A5%E8%AF%86%E7%82%B9/%E3%80%90%E7%9F%A5%E8%AF%86%E7%82%B9%E3%80%91NLI/"},{"title":"Oracle重要性采样","text":"Oracle重要性采样（Oracle Importance Sampling）是一种用于估计机器学习模型的边缘似然或边缘概率的方法。它是基于重要性采样的一种改进方法，在一些情况下可以提供更准确的估计。 在机器学习中，边缘似然或边缘概率是指在给定输入数据的情况下，对目标变量的概率分布进行建模。然而，计算边缘似然或边缘概率可能会面临挑战，特别是在高维空间或复杂模型中。 Oracle重要性采样通过引入一个“Oracle”模型来改进传统的重要性采样方法。这个Oracle模型是一个近似目标分布的模型，可以根据先验知识、专家领域知识或其他渠道获得。Oracle模型的目标是更好地近似真实的目标分布，以提供更准确的重要性权重。 在Oracle重要性采样中，首先使用一个基准模型（通常是一个简化的模型）对边缘似然或边缘概率进行采样。然后，使用Oracle模型对相同的输入数据进行采样。通过比较基准模型和Oracle模型的采样结果，可以计算出重要性权重，用于对基准模型的采样结果进行校正。 通过使用Oracle重要性采样，可以利用Oracle模型中的先验知识或专家领域知识来改进重要性采样的准确性。这在一些情况下可以提供更可靠和准确的边缘似然或边缘概率估计，从而帮助解决机器学习中的一些挑战。","link":"/2023/11/05/%E7%9F%A5%E8%AF%86%E7%82%B9/%E3%80%90%E7%9F%A5%E8%AF%86%E7%82%B9%E3%80%91Oracle%E9%87%8D%E8%A6%81%E6%80%A7%E9%87%87%E6%A0%B7/"},{"title":"Seq2Seq","text":"Seq2Seq（Sequence-to-Sequence）是一种序列到序列的神经网络模型，也被称为编码-解码模型。它主要用于处理具有不定长输入和输出序列的任务，如机器翻译、文本摘要、对话生成等。 Seq2Seq模型由两个主要组件组成：编码器（Encoder）和解码器（Decoder）。编码器将输入序列（源语言）转换为一个固定长度的向量，捕捉输入序列的语义信息。解码器则将该向量作为输入，逐步生成目标序列（目标语言）。 编码器和解码器都可以使用循环神经网络（如LSTM或GRU）来建模序列信息。编码器通过逐步读取输入序列的每个元素，并在每个时间步产生一个编码器隐藏状态。最后，编码器隐藏状态中包含了输入序列的总体表示。 解码器使用编码器的隐藏状态作为初始状态，并逐步生成目标序列的每个元素。在每个时间步，解码器接收上一个时间步的输出和当前的隐藏状态作为输入，并生成下一个输出和隐藏状态。这个过程一直持续到生成完整的目标序列为止。 Seq2Seq模型的训练过程通常使用教师强制（Teacher Forcing）方法，即将目标序列的真实值作为解码器的输入，以便更好地引导模型学习正确的序列生成。在推断阶段，解码器则使用自己的前一个输出作为下一个时间步的输入，以逐步生成预测的目标序列。 Seq2Seq模型的优点在于能够处理不定长的输入和输出序列，并且能够学习序列之间的语义关系。它在机器翻译、对话生成等任务中取得了显著的成功，并成为自然语言处理领域的重要模型之一。","link":"/2023/11/07/%E7%9F%A5%E8%AF%86%E7%82%B9/%E3%80%90%E7%9F%A5%E8%AF%86%E7%82%B9%E3%80%91Seq2Seq/"},{"title":"TF-IDF","text":"$TF$是指归一化后的词频，$IDF$是指逆文档频率。给定一个文档集合$D$，有$d_1,d_2,d_3, \\cdots ,d_n \\in D$。 文档集合总共包含$m$个词（注：一般在计算TF−IDF时会去除如“的”这一类的停用词），有$w_1,w_2,w_3,\\cdots,w_m \\in W$。我们现在以计算词$w_i$在文档$d_j$中的$TF-IDF$指为例。$TF$的计算公式为： $$TF=\\frac{freq(i,j)}{max_{len}(j)}$$ 在这里$freq(i,j)$为$w_i$在$d_j$中出现的频率，$max_{len}(j)$为$d_j$长度。 $TF$只能是描述词在文档中的频率，但假设现在有个词为”我们“，这个词可能在文档集$D$中每篇文档中都会出现，并且有较高的频率。那么这一类词就不具有很好的区分文档的能力，为了降低这种通用词的作用，引入了$IDF$。 $IDF$的表达式如下： $$IDF=log(\\frac{len(D)}{n(i)})$$在这里$len(D)$表示文档集合$D$中文档的总数，$n(i)$表示含有$w_i$这个词的文档的数量。 得到$TF$和$IDF$之后，我们将这两个值相乘得到$TF−IDF$的值： $$TF-IDF=TF \\times IDF$$ $TF$可以计算在一篇文档中词出现的频率，而$IDF$可以降低一些通用词的作用。因此对于一篇文档我们可以用文档中每个词的$TF-IDF$组成的向量来表示该文档，再根据余弦相似度这类的方法来计算文档之间的相关性。","link":"/2023/11/17/%E7%9F%A5%E8%AF%86%E7%82%B9/%E3%80%90%E7%9F%A5%E8%AF%86%E7%82%B9%E3%80%91TF-IDF%E7%AE%97%E6%B3%95/"},{"title":"TREC","text":"TREC（Text REtrieval Conference）是一个历史悠久的信息检索领域的国际会议和评测活动。它由美国国家标准与技术研究院（NIST）于1992年开始组织，旨在促进和推动信息检索技术的研究和发展。 TREC会议每年举办一次，吸引了来自学术界、工业界和政府机构的研究人员和专家参与。会议的主要目标是通过组织评测任务和共享数据集，推动信息检索算法和系统的创新与改进，并促进研究社区之间的合作与交流。 TREC评测任务通常涉及信息检索、文本分类、问答系统等相关领域。参与者根据提供的数据集和任务要求，设计和实现自己的检索算法或系统，并在评测期间对其性能进行测试和评估。最终，TREC会议会发布评测结果和报告，以促进对信息检索技术的进一步研究和发展。","link":"/2023/11/04/%E7%9F%A5%E8%AF%86%E7%82%B9/%E3%80%90%E7%9F%A5%E8%AF%86%E7%82%B9%E3%80%91TREC/"},{"title":"nDCG","text":"nDCG（Normalized Discounted Cumulative Gain）是一种用于评估信息检索和推荐系统排序质量的指标。它是对DCG指标的标准化扩展。 在信息检索和推荐系统中，排序是将相关性较高的项目（例如搜索结果或推荐项）排在前面的过程。DCG是一种度量排序质量的指标，它考虑了项目的相关性以及它们在排序列表中的位置。DCG通过对相关性进行折扣，根据项目在排序列表中的位置赋予不同的权重，然后将这些权重进行累加。 nDCG对DCG进行标准化，以便进行不同排序列表之间的比较。它通过将DCG除以在理想排序下的最大可能DCG，得到一个取值范围在0到1之间的归一化指标。理想排序是指将相关性最高的项目排在最前面的排序。 nDCG的计算方式如下： 1nDCG = DCG / IDCG 其中，DCG是折扣累计增益（Discounted Cumulative Gain），IDCG是在理想排序下的最大可能折扣累计增益（Ideal DCG）。 nDCG是一种常用的排序质量指标，广泛应用于信息检索、推荐系统和广告排序等领域。它能够反映出排序结果的相关性和顺序，从而帮助评估和比较不同排序算法或系统的性能。较高的nDCG值表示排序结果更符合用户的偏好和期望。","link":"/2023/11/10/%E7%9F%A5%E8%AF%86%E7%82%B9/%E3%80%90%E7%9F%A5%E8%AF%86%E7%82%B9%E3%80%91nDCG/"},{"title":"Zero-shot","text":"“Zero-shot” 是一种指在模型训练阶段未见过特定任务或类别的能力。在这种情况下，模型被要求执行没有直接训练样本的任务。这对于泛化到新领域或新任务非常有用。 举几个例子来说明 zero-shot 的概念： 图像分类： 假设有一个图像分类模型，经过训练可以识别猫、狗、汽车等类别。如果在测试阶段，模型被要求对一些在训练数据中从未见过的类别，比如大象、飞机进行分类，而它仍能够正确分类，那么这就是 zero-shot 图像分类。 文本情感分析： 假设有一个情感分析模型，它在训练阶段只见过积极和消极的情感类别。如果在测试阶段，模型被要求对一种新的情感类别，比如中立或焦虑进行分类，而它能够适应并进行准确的分类，那么这就是 zero-shot 情感分析。 自然语言处理（NLP）中的零样本学习： 在NLP领域，zero-shot 学习可能涉及到对一些未在训练中见过的主题或任务的理解。例如，一个文本生成模型在训练时只接触到了电影评论，但在测试时可以生成关于体育比赛的文本，而且质量仍然很高，这就是 zero-shot 学习。 Zero-shot 能力通常需要模型具有很强的泛化能力，能够推广到新的情境和任务。在深度学习中，一些预训练的模型（如 GPT、BERT）通过大规模训练来获得广泛的知识，使它们在 zero-shot 或 few-shot 任务上表现出色。","link":"/2023/11/12/%E7%9F%A5%E8%AF%86%E7%82%B9/%E3%80%90%E7%9F%A5%E8%AF%86%E7%82%B9%E3%80%91zero-shot/"},{"title":"倒排索引","text":"倒排索引（Inverted Index）是一种常用的索引结构，用于加速文本检索过程。它将文档集合中的每个单词与包含该单词的文档进行关联，从而能够高效地根据关键词进行检索。以下是一些使用倒排索引的原因： 快速定位文档：倒排索引允许通过关键词快速定位包含该关键词的文档。相比于遍历整个文档集合来查找匹配的文档，倒排索引可以通过索引结构直接定位到相关文档，极大地提高了检索效率。 减少搜索空间：倒排索引可以根据关键词的出现情况，快速缩小待搜索的文档范围。通过在索引中查找关键词，可以排除不包含关键词的文档，从而减少了需要搜索的文档数量，提高了检索速度。 支持布尔检索：倒排索引可以方便地支持布尔检索操作，如AND、OR、NOT等逻辑运算。通过对多个关键词进行逻辑组合，可以快速筛选出满足特定条件的文档集合，从而实现更精确的检索。 支持短语查询：倒排索引可以支持短语查询，即根据关键词的顺序进行检索。通过记录关键词在文档中的位置信息，可以识别并返回包含特定短语的文档，满足对短语匹配的需求。 节省存储空间：倒排索引的结构相对紧凑，可以有效地节省存储空间。通过将文档的关键词与文档ID进行关联，而不是存储完整的文档内容，可以大大减少索引的存储需求。 总结而言，倒排索引能够通过关键词快速定位文档，减少搜索空间，支持布尔检索和短语查询，并且节省存储空间。这些优势使得倒排索引成为文本检索领域中常用的索引结构，提高了检索效率和准确性。","link":"/2023/11/06/%E7%9F%A5%E8%AF%86%E7%82%B9/%E3%80%90%E7%9F%A5%E8%AF%86%E7%82%B9%E3%80%91%E5%80%92%E6%8E%92%E7%B4%A2%E5%BC%95/"},{"title":"启发式函数","text":"启发式函数（Heuristic Function）是在问题求解中用于评估可能解决方案的一种函数。它基于一些经验规则或启发性的指导，为每个可能的解决方案分配一个估计的价值或代价。 启发式函数通常用于启发式搜索算法中，这些算法通过评估候选解决方案的启发式函数值来引导搜索过程，以便更快地找到较优的解决方案。 启发式函数的设计取决于具体的问题。在某些情况下，启发式函数可能基于问题的特定知识和领域专家的经验。这些函数可以根据问题的特征和目标进行定义，以提供有关解决方案质量的估计。 在启发式搜索算法中，启发式函数可以用来选择下一步要扩展的节点，以便更有可能找到更好的解决方案。通过合理设计和利用启发式函数，可以加速搜索过程，减少搜索空间，并提高解决问题的效率。 需要注意的是，启发式函数是基于经验和启发性的指导，它并不保证找到最优解决方案。它可能在某些情况下受限于启发性的偏见或局限性，因此在应用启发式函数时需要进行权衡和评估。","link":"/2023/11/07/%E7%9F%A5%E8%AF%86%E7%82%B9/%E3%80%90%E7%9F%A5%E8%AF%86%E7%82%B9%E3%80%91%E5%90%AF%E5%8F%91%E5%BC%8F%E5%87%BD%E6%95%B0/"},{"title":"梯度范数","text":"梯度范数指的是在多元函数中，梯度向量的长度或大小。梯度是一个向量，包含了函数在每个维度上的偏导数，而梯度的范数则表示了这个向量的长度。 在机器学习和深度学习中，梯度通常指的是损失函数对于模型参数的偏导数。在训练模型的过程中，通过梯度下降等优化算法，我们尝试最小化损失函数，以更新模型的参数。梯度的方向指向损失函数上升最快的方向，而梯度的大小（即梯度范数）则表示了这个上升速度。 梯度范数越大，意味着函数在当前点的变化越快，优化算法在这个方向上的调整也就更大。梯度下降算法会朝着梯度的相反方向（即梯度的负方向）更新参数，以使损失函数尽可能减小，直至找到局部最小值或全局最小值（在凸优化问题中）。 在深度学习中，梯度范数的大小对训练过程至关重要。太大的梯度可能导致训练不稳定，甚至出现梯度爆炸的问题；而太小的梯度可能导致训练过慢或者陷入局部最优点。因此，调节学习率、使用合适的优化器和正则化技术等方法，都是为了更好地控制梯度的大小和方向，以有效地训练深度学习模型。","link":"/2023/11/03/%E7%9F%A5%E8%AF%86%E7%82%B9/%E3%80%90%E7%9F%A5%E8%AF%86%E7%82%B9%E3%80%91%E6%A2%AF%E5%BA%A6%E8%8C%83%E6%95%B0/"},{"title":"矩阵分解","text":"矩阵分解是将一个矩阵拆分成多个子矩阵或分量的过程。它是线性代数和数值计算中的一个重要技术，被广泛应用于各种领域，如数据分析、图像处理、推荐系统等。 在矩阵分解中，常见的几种分解方法包括： LU 分解：LU 分解将一个矩阵拆分为一个下三角矩阵 L 和一个上三角矩阵 U 的乘积。这种分解可以用于解线性方程组、计算矩阵的行列式和逆等。 QR 分解：QR 分解将一个矩阵拆分为一个正交矩阵 Q 和一个上三角矩阵 R 的乘积。QR 分解常用于求解最小二乘问题、计算特征值和特征向量等。 奇异值分解（SVD）：奇异值分解将一个矩阵拆分为三个矩阵的乘积，即 A = UΣV^T，其中 U 和 V 是正交矩阵，Σ 是一个对角矩阵。SVD 在降维、图像压缩、推荐系统等领域有广泛应用。 特征值分解：特征值分解将一个方阵拆分为特征向量矩阵和特征值对角矩阵的乘积。特征值分解可以用于求解线性微分方程和矩阵的幂等等。 这些矩阵分解方法可以帮助我们理解矩阵的结构和性质，简化复杂的计算问题，提取矩阵的重要信息，并在各种应用中提供高效的数值计算工具。","link":"/2023/11/06/%E7%9F%A5%E8%AF%86%E7%82%B9/%E3%80%90%E7%9F%A5%E8%AF%86%E7%82%B9%E3%80%91%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3/"},{"title":"矩阵除法","text":"在矩阵运算中，A \\ B 和 A / B 是两种不同的操作，它们分别表示矩阵的左除和右除运算。 A \\ B：表示矩阵 B 对矩阵 A 进行左除运算。这意味着通过求解线性方程组 A * X = B 来找到矩阵 X 的值。其中，A 是系数矩阵，B 是结果矩阵，X 是未知变量矩阵。左除运算可以用来求解线性方程组的解，例如在线性回归和最小二乘法中经常使用。 A / B：表示矩阵 B 对矩阵 A 进行右除运算。这意味着通过求解线性方程组 X * A = B 来找到矩阵 X 的值。其中，A 是系数矩阵，B 是结果矩阵，X 是未知变量矩阵。右除运算也可以用来求解线性方程组的解，但是相对较少使用。 需要注意的是，左除和右除运算的结果可能不同，因为矩阵的乘法不满足交换律。所以 A \\ B 和 A / B 的结果通常是不同的。","link":"/2023/11/05/%E7%9F%A5%E8%AF%86%E7%82%B9/%E3%80%90%E7%9F%A5%E8%AF%86%E7%82%B9%E3%80%91%E7%9F%A9%E9%98%B5%E9%99%A4%E6%B3%95/"},{"title":"自回归和非自回归","text":"自回归（Autoregressive，简称AR）和非自回归（Non-Autoregressive，简称NAR）是两种不同的建模方法，常用于序列生成任务，特别是自然语言处理领域。 自回归模型：自回归模型是一种基于时间顺序的建模方法，其中当前时间步的输出依赖于之前的输出。在自回归模型中，生成的序列是按照时间顺序逐步生成的，每一步都依赖于之前的生成结果。典型的自回归模型包括基于循环神经网络（RNN）的模型，如循环神经网络语言模型（RNNLM），以及基于Transformer的模型，如GPT（Generative Pre-trained Transformer）。这些模型在生成文本、机器翻译等任务上表现出色，因为它们能够利用上下文信息进行逐步生成，保持一定的连贯性和上下文一致性。 非自回归模型：非自回归模型是指生成过程中每个时间步之间相互独立，不依赖于之前的生成结果。与自回归模型不同，非自回归模型可以同时并行地生成整个序列，而不需要等待前面的结果。这种并行生成的特点使得非自回归模型在速度上具有优势，能够更快地生成序列。典型的非自回归模型包括基于生成对抗网络（GAN）的模型和基于自注意力机制的模型，如Mask-Predict和MASS（Masked Sequence to Sequence）。然而，非自回归模型往往在生成质量上存在一定的挑战，因为缺乏上下文信息的依赖关系，可能导致生成结果的不连贯性和语义不准确性。 总结而言，自回归模型在生成任务中能够保持上下文的连贯性，但生成速度较慢；非自回归模型可以快速生成序列，但在生成质量上可能存在一定的问题。选择使用哪种模型取决于具体的任务需求和平衡生成质量与生成速度的要求。","link":"/2023/11/06/%E7%9F%A5%E8%AF%86%E7%82%B9/%E3%80%90%E7%9F%A5%E8%AF%86%E7%82%B9%E3%80%91%E8%87%AA%E5%9B%9E%E5%BD%92%E5%92%8C%E9%9D%9E%E8%87%AA%E5%9B%9E%E5%BD%92/"},{"title":"自然问题","text":"自然问题（Nature Question）是指关于自然界、科学领域或人类与自然环境相关的问题。这些问题通常涉及自然现象、生物学、物理学、化学、地球科学等科学领域的知识和概念。 自然问题可以涉及各种主题，例如天文学中的宇宙起源和星系形成、生物学中的进化和生物多样性、物理学中的力、能量和运动等。这些问题可能涉及到科学研究、实验观察、理论推理和数据分析等方法来解决。 自然问题的特点是它们基于现实世界中的观察和现象，并通过科学方法和理论来解答。这些问题推动了科学的进步和发展，激发了科学家们的思考和探索。 举个例子，一些自然问题可能包括： 如何解释黑洞的形成和行为？ 为什么某些植物能够进行光合作用？ 地球上的生命是如何起源的？ 为什么水在低温下会结冰？ 如何预测地震和其他自然灾害？ 这些问题都是关于自然界和科学领域的重要课题，研究人员通过实证和理论的方法来回答这些问题，以增加我们对自然世界的理解。","link":"/2023/11/05/%E7%9F%A5%E8%AF%86%E7%82%B9/%E3%80%90%E7%9F%A5%E8%AF%86%E7%82%B9%E3%80%91%E8%87%AA%E7%84%B6%E9%97%AE%E9%A2%98(NQ)/"},{"title":"词袋模型","text":"词袋模型（Bag of Words，简称BoW）是信息检索领域中常用的一种文本表示方法。它将文本表示为一个无序的词汇项集合，忽略了文本中词汇的顺序和语法结构，仅关注每个词汇在文本中出现的频率。 以下是词袋模型的基本步骤和解释： 构建词汇表（Vocabulary）： 首先，从文本数据中提取所有不同的词汇，形成一个词汇表。这个词汇表包含了文本中出现的所有单词，去除了停用词等不重要的词汇。 向量化文本： 对于每个文本样本，将其表示为一个与词汇表等长的向量，向量的每个元素对应于词汇表中的一个词汇。向量的每个元素表示对应词汇在文本中出现的频率。 例如，如果有一个词汇表 [&quot;apple&quot;, &quot;orange&quot;, &quot;banana&quot;]，而文本 “I like apple and apple” 则可以表示为向量 [2, 0, 0]，其中第一个元素表示 “apple” 出现的次数，其他元素表示 “orange” 和 “banana” 的出现次数。 忽略词序和语法： 词袋模型忽略了文本中词汇的顺序和语法结构，只关注词汇的出现频率。这意味着具有相同词汇分布的文本在词袋表示中是相似的，而不考虑它们的词汇顺序。 稀疏表示： 由于大多数文本只包含词汇表中的一小部分词汇，词袋模型的表示通常是稀疏的，即大多数元素为零。这种表示形式有效地减少了存储和计算的复杂性。 词袋模型是一种简单而有效的文本表示方法，常用于文本分类、信息检索和自然语言处理任务。然而，它也有一些局限性，例如无法捕捉词汇之间的语义关系和上下文信息。为了解决这些问题，后续的模型，如词嵌入（Word Embeddings）和深度学习模型，被引入以更好地捕获语义信息。","link":"/2023/11/12/%E7%9F%A5%E8%AF%86%E7%82%B9/%E3%80%90%E7%9F%A5%E8%AF%86%E7%82%B9%E3%80%91%E8%AF%8D%E8%A2%8B%E6%A8%A1%E5%9E%8BBoW/"},{"title":"高斯建模","text":"高斯建模（Gaussian modeling）是一种统计建模方法，基于高斯分布（也称为正态分布）对数据进行建模和分析。高斯分布在概率统计中非常常见，因为它具有许多有用的性质和应用。 高斯建模假设数据的分布服从高斯分布。高斯分布可以由其均值（mean）和方差（variance）来完全描述。它的概率密度函数（probability density function，PDF）在一维情况下可以表示为： $$f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$$ 其中，$x$ 是随机变量的取值，$\\mu$ 是均值，$\\sigma^2$ 是方差。 高斯建模的目标是通过对数据进行观察和分析，估计数据的均值和方差，并基于高斯分布对未知数据进行预测和推断。高斯建模在许多领域中都有广泛的应用，包括统计学、机器学习、图像处理、金融领域等。 在实际应用中，高斯建模可以用于以下任务： 数据建模和分析：通过拟合高斯分布参数，对数据进行建模和分析，了解数据的分布特征和统计性质。 异常检测：通过比较观测数据与高斯分布的概率密度，识别和检测数据中的异常值或离群点。 数据生成和合成：基于已知的高斯分布参数，生成符合相同分布的合成数据，用于模拟和测试。 数据预测和推断：基于已有数据的高斯建模，对未知数据进行预测和推断，如预测某个未来时间点的观测值。 高斯建模是概率统计和机器学习中一个重要的基础方法，广泛应用于各种数据分析和模型构建的任务中。","link":"/2023/11/09/%E7%9F%A5%E8%AF%86%E7%82%B9/%E3%80%90%E7%9F%A5%E8%AF%86%E7%82%B9%E3%80%91%E9%AB%98%E6%96%AF%E5%BB%BA%E6%A8%A1/"}],"tags":[{"name":"工程项目","slug":"工程项目","link":"/tags/%E5%B7%A5%E7%A8%8B%E9%A1%B9%E7%9B%AE/"},{"name":"后端开发","slug":"后端开发","link":"/tags/%E5%90%8E%E7%AB%AF%E5%BC%80%E5%8F%91/"},{"name":"科研项目","slug":"科研项目","link":"/tags/%E7%A7%91%E7%A0%94%E9%A1%B9%E7%9B%AE/"},{"name":"计算机视觉","slug":"计算机视觉","link":"/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"},{"name":"人工智能创意赛","slug":"人工智能创意赛","link":"/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%88%9B%E6%84%8F%E8%B5%9B/"},{"name":"嵌入式","slug":"嵌入式","link":"/tags/%E5%B5%8C%E5%85%A5%E5%BC%8F/"},{"name":"互联网+","slug":"互联网","link":"/tags/%E4%BA%92%E8%81%94%E7%BD%91/"},{"name":"声学","slug":"声学","link":"/tags/%E5%A3%B0%E5%AD%A6/"},{"name":"计算机设计大赛","slug":"计算机设计大赛","link":"/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%AE%BE%E8%AE%A1%E5%A4%A7%E8%B5%9B/"},{"name":"NLP","slug":"NLP","link":"/tags/NLP/"},{"name":"信息检索","slug":"信息检索","link":"/tags/%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2/"},{"name":"数学","slug":"数学","link":"/tags/%E6%95%B0%E5%AD%A6/"},{"name":"概率论","slug":"概率论","link":"/tags/%E6%A6%82%E7%8E%87%E8%AE%BA/"},{"name":"线性代数","slug":"线性代数","link":"/tags/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/"},{"name":"算法","slug":"算法","link":"/tags/%E7%AE%97%E6%B3%95/"},{"name":"机器学习","slug":"机器学习","link":"/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"哲学","slug":"哲学","link":"/tags/%E5%93%B2%E5%AD%A6/"}],"categories":[{"name":"工程项目","slug":"工程项目","link":"/categories/%E5%B7%A5%E7%A8%8B%E9%A1%B9%E7%9B%AE/"},{"name":"科研项目","slug":"科研项目","link":"/categories/%E7%A7%91%E7%A0%94%E9%A1%B9%E7%9B%AE/"},{"name":"论文学习","slug":"论文学习","link":"/categories/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/"}],"pages":[{"title":"Achievement","text":"荣誉称号 id 日期 获奖名称 获奖等级 颁奖单位 1 2021.06 优秀志愿者 校级 哈尔滨工程大学 2 2023.03 优秀三好学生 校级 哈尔滨工程大学 3 2023.03 优秀共青团员 校级 哈尔滨工程大学 竞赛奖项国家级 id 日期 比赛名称 获奖等级 颁奖单位 指导教师 1 2023.05 美国大学生数学建模竞赛 Honorable Mention COMAP(美国数学及其应用联合会) 刘东平 2 2023.07 中国大学生计算机设计大赛 国家级二等奖 中国大学生计算机设计大赛组织委员会 卢丹、王也 省部级 id 日期 比赛名称 获奖等级 颁奖单位 指导教师 1 2022.09 东北三省数学建模联赛 省级二等奖 东北三省数学建模联赛组委会 高振滨 2 2022.11 全国大学生数学建模联赛 省级一等奖 全国大学生数学建模竞赛黑龙江赛区组委会 王淑娟 3 2023.05 中国大学生计算机设计大赛省赛 省级一等奖 中国大学生计算机设计大赛黑龙江省级赛组织委员会 卢丹、王也 4 2023.05 中国大学生计算机设计大赛省赛 省级三等奖 中国大学生计算机设计大赛黑龙江省级赛组织委员会 张泽宝 5 2023.08 “建行杯”第九届中国国际“互联网+”大学生创新创业大赛省赛 省级银奖 黑龙江省教育厅 李超 6 2023.08 “建行杯”第九届中国国际“互联网+”大学生创新创业大赛省赛 省级铜奖 黑龙江省教育厅 吴艳霞、李超 校级 id 日期 比赛名称 获奖等级 颁奖单位 指导教师 1 2022.10 “五四杯”大学生课外学术科技作品竞赛 校级二等奖 共青团哈尔滨工程大学委员会 李超 2 2022.11 “五四杯”大学生创业大赛 校级金奖 共青团哈尔滨工程大学委员会 杨东梅、徐悦竹 3 2022.11 “五四杯”大学生创业大赛 校级银奖 共青团哈尔滨工程大学委员会 李超、冯晓宁、刘刚 4 2023.06 第一届“贡橙杯”CTF竞赛 校级二等奖 哈尔滨工程大学计算机科学与技术学院 - 5 2023.11 第二届“贡橙杯”CTF竞赛 校级二等奖 哈尔滨工程大学计算机科学与技术学院 - 奖学金 id 日期 奖励名称 获奖等级 颁奖单位 1 2021.05 优秀学生奖学金 校级一等奖 哈尔滨工程大学 2 2021.10 优秀学生奖学金 校级二等奖 哈尔滨工程大学 3 2022.05 优秀学生奖学金 校级一等奖 哈尔滨工程大学 4 2022.11 优秀学生奖学金 校级二等奖 哈尔滨工程大学 5 2023.05 优秀学生奖学金 校级二等奖 哈尔滨工程大学 6 2023.11 优秀学生奖学金 校级二等奖 哈尔滨工程大学","link":"/achievement/index.html"},{"title":"Links","text":"同窗好友 Junhao Chen (Tsinghua University, Shenzhen) Xiaojun Ye (Zhejiang University) Zihan Xu (Shanghai Jiao Tong University, Shanghai) Yuhao Xue (Tongji University, Shanghai) Zhicheng Li (University of Science and Technology of China, Suzhou) Fang Wu (Xiamen University, Xiamen) 我的母校 Harbin Engineering University Northeastern University (Shenyang, China) 其他组织 HEU 开放原子开源社团 HEU 开放原子社区论坛 HEU 网络安全社团","link":"/links/index.html"},{"title":"Profile","text":"学术背景 时间 学校 专业 2017.09 - 2020.06 黑龙江省伊春市第一中学 理科(物化生) 2020.09 - 2024.07(在读) 哈尔滨工程大学 计算机科学与技术 2024.09 - 2027.07 东北大学(沈阳, 中国) 计算机技术 学习成绩 时间 计算方法 成绩 排名 前六学期 必修课加权成绩(教务办) 89.58 33 / 290 (11.38%) 前六学期 推免综合成绩(学工办) 91.58 22 / 290 (7.59%) 等级认证 考试 成绩 时间 CET-4 554 2020.12 CET-6 459 2021.12 CSP 170 2021.06 科研项目 时间 项目 方向 2022.10 - 2022.12 海洋鹰眼——舰船识别与海况观测系统 计算机视觉 2022.10 - 2022.12 基于计算机视觉的盲人视觉辅助智能眼镜 计算机视觉 2022.12 - 2023.05 基于音频特诊分析的动态鸟类识别分析 声学 2022.09 - 2023.01 基于深度学习的多功能图像处理和社交平台 计算机视觉 2023.05 - 2023.06 基于同元MWorks统一建模语言的在线仿真建模平台 嵌入式 2022.10 - 2023.04 哈尔滨工程大学语料库智能检索系统 后端开发 主修课程 专业课 成绩 选修课 成绩 数学课 成绩 编译系统设计 100 计算机逻辑设计综合实验 98 线性代数与解析几何 97 计算思维 95 计算机图形学 96 工科数学分析 96 计算机组成原理 95 面向对象程序设计 95 复变函数 90 信息管理系统实践 92 软件工程项目实践 95 计算机硬件综合课程设计 92 ACM程序设计 95 数据结构 91 ios程序设计 95 电路基础 91 模型机设计 95 计算机网络 90 数据库原理 94 实习经历 时间 单位 工作 2023.05 - 2023.06 中国联合网络通信公司(哈尔滨) 网络攻击监测 2023.07 - 2023.08 Oracle甲骨文华育兴业(哈尔滨) 后端开发 学生任职 时间 学生组织 工作 2021.10 - 2022.06 计算机科学与技术学院宣传中心 摄影部部员 2021.10 - 2022.06 计算机科学与技术学院团委 宣传部部员","link":"/profile/index.html"}]}