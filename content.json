{"posts":[{"title":"ANN检索","text":"ANN（Approximate Nearest Neighbors）检索是一种用于寻找给定查询点附近近似最近邻的技术。在大规模数据集中，寻找最近邻（nearest neighbors）是一个计算密集型任务，尤其是当数据集的维度较高时。ANN检索的目标是以更高的效率找到近似的最近邻，而不一定要找到确切的最近邻。 ANN检索常用于各种机器学习、数据挖掘和信息检索任务中。特别是在大规模数据集中，ANN检索技术可以大幅减少搜索时间，从而提高算法的效率。 ANN检索的方法有很多种，其中包括但不限于： 树结构方法：例如KD树、Ball 树、VP 树、M 树等，这些方法通过构建树形结构来快速搜索近似最近邻。 哈希方法：例如局部敏感哈希（LSH），LSH使用哈希函数将数据点映射到桶（buckets），从而加速最近邻的搜索。 近似搜索算法：例如最近邻算法（FLANN），这些算法采用近似的方法来快速查找最近邻。 这些技术可以在不牺牲太多精度的情况下，在大型数据集中实现高效的最近邻搜索。ANN检索通常用于提供近似的解决方案，对于许多应用而言，高效的近似结果已经足够满足需求，并且能大大减少搜索时间和计算资源的消耗。","link":"/2023/11/03/%E7%9F%A5%E8%AF%86%E7%82%B9/%E3%80%90%E7%9F%A5%E8%AF%86%E7%82%B9%E3%80%91ANN%E6%A3%80%E7%B4%A2/"},{"title":"BM25算法","text":"$BM25$算法通常用来做搜索相关性评分的，也是$ES$中的搜索算法，通常用来计算$query$和文本集合$D$中每篇文本之间的相关性。 我们用$Q$表示$query$，在这里$Q$一般是一个句子。在这里我们要对$Q$进行语素解析（一般是分词），在这里以分词为例，我们对$Q$进行分词，得到$q_1,q_2,\\cdots,q_t$这样一个词序列。给定文本$d \\in D$，现在以计算$Q$和$d$之间的分数（相关性），其表达式如下：$$Score(Q,d)=\\sum\\limits^t_{i=1}w_i\\times R(q_i,d)$$上面式子中$w_i$表示$q_i$的权重，$R(q_i,d)$为$q_i$和$d$的相关性，$Score(Q,d)$就是每个语素$q_i$和$d$的相关性的加权和。 $w_i$的计算方法有很多，一般是用$IDF$来表示的，但这里的$IDF$计算和上面的有所不同，具体的表达式如下：$$w_i=IDF(q_i)=log\\frac{N-n(q_i)+0.5}{n(qi)+0.5}$$上面式子中$N$表示文本集合中文本的总数量，$n(q_i)$表示包含$q_i$这个词的文本的数量，$0.5$主要是做平滑处理。 $R(q_i,d)$的计算公式如下： $$R(q_i,d)=\\frac{f_i \\times (k_1+1)}{f_i+K}\\times \\frac{qf_i\\times(k_2+1)}{qf_i+k_2}$$其中 $$K=k_1\\times(1−b+b\\times \\frac{dl}{avgdl})$$ 上面式子中$f_i$为$q_i$在文本$d$中出现的频率，$qf_i$为$q_i$在$Q$中出现的频率，$k_1,k_2,b$都是可调节的参数，$dl,avgdl$分别为文本$d$的长度和文本集$D$中所有文本的平均长度。 一般$qf_i=1$，取$k_2=0$，则可以去除后一项，将上面式子改写成： $$R(q_i,d)=\\frac{f_i\\times(k_1+1)}{f_i+K}$$ 通常设置$k_1=2,b=0.75$。参数$b$的作用主要是调节文本长度对相关性的影响。","link":"/2023/11/01/%E7%9F%A5%E8%AF%86%E7%82%B9/%E3%80%90%E7%9F%A5%E8%AF%86%E7%82%B9%E3%80%91BM25%E7%AE%97%E6%B3%95/"},{"title":"嵌入坍缩","text":"“Embedding collapse”（嵌入坍缩）是指在训练神经网络时，由于模型的复杂性或者数据分布的问题，导致嵌入空间中的不同输入被映射到相似的嵌入向量。这种情况会使得模型在处理不同输入时难以区分它们，因为它们被映射到了相似的表示。 嵌入坍缩通常是不希望出现的，因为模型需要在嵌入空间中保留输入数据的差异，以便正确地学习和泛化。嵌入坍缩可能导致模型在训练和测试时表现不佳，因为模型难以捕捉输入之间的微小差异。 以下是一些可能导致嵌入坍缩的原因： 数据不平衡： 如果训练数据中某些类别的样本数量远远超过其他类别，模型可能更容易将它们映射到相似的嵌入向量，而忽略了较少出现的类别。 过度拟合： 当模型过度拟合训练数据时，它可能会学到一些特定的模式，而不是泛化到更广泛的情况。这可能导致嵌入坍缩，使得相似的输入被映射到相似的表示。 模型复杂性： 过于复杂的模型可能会倾向于将输入映射到一个较小的嵌入空间，导致嵌入坍缩。这可能发生在具有大量参数的深度神经网络中。 为了缓解嵌入坍缩的问题，可以尝试以下方法： 正则化： 添加正则化项，如L1或L2正则化，以防止模型学习过于复杂的表示。 数据增强： 通过对训练数据进行变换或增强，引入更多的差异性，帮助模型学到更鲁棒的表示。 平衡数据： 确保训练数据中的不同类别样本数量相对平衡，以防止某些类别的过度表示。 监控训练过程： 使用监控工具和可视化方法来检查模型在嵌入空间中的学习情况，及时发现和解决嵌入坍缩问题。","link":"/2023/11/13/%E7%9F%A5%E8%AF%86%E7%82%B9/%E3%80%90%E7%9F%A5%E8%AF%86%E7%82%B9%E3%80%91Embedding%20collapse/"},{"title":"GLUE","text":"GLUE和SUPER-GLUE是用于衡量自然语言处理模型表现的两个基准测试体系: GLUE(General Language Understanding Evaluation): 由谷歌 AI 实验室于2018年提出。 包括9个不同类型的NLP任务,如句子推理、naming等。 旨在评估模型在各种常见NLP任务上的泛化能力。 SUPER-GLUE: 是GLUE基准测试的升级版,由同一团队于2019年发布。 增加和改进了一些GLUE任务,加入了更难更复杂的新任务。 任务包括:想象性问答、多项选择、修辞关系等难点任务。 对模型的理解和推理能力要求更高。 两个基准测试的主要区别: SUPER-GLUE的任务设置更难,对模型的要求更高。 可以更全面和细致地检测模型在不同程度的NLP任务中的表现。 成为评估新一代更强大NLP模型的主流标准,如BERT、GPT-3等。 因此,在最新的研究中,模型表现都以其在GLUE和SUPER-GLUE上的效果进行报告,成为NLP进步的关键指标之一。 GLUE（General Language Understanding Evaluation）是一个用于评估自然语言处理（NLP）模型性能的基准测试集合。GLUE基准测试涵盖了多个任务，包括文本分类、语义相似度、自然语言推断等。下面是GLUE基准测试中包括的九个任务的详细解释： CoLA（Corpus of Linguistic Acceptability）: 任务： 判断给定的句子是否在语法和语义上是可接受的。 示例： “He is playing the piano.”（可接受） vs. “Him play piano.”（不可接受） SST-2（Stanford Sentiment Treebank）: 任务： 对给定的句子进行情感分类，判断情感是正面、负面还是中性的。 示例： “I love this product!”（正面） vs. “This is the worst movie ever.”（负面） MRPC（Microsoft Research Paraphrase Corpus）: 任务： 判断给定的两个句子是否是语义上相似的。 示例： “The cat is on the mat.” vs. “The cat is lying on the mat.”（相似） STS-B（Semantic Textual Similarity Benchmark）: 任务： 对给定的两个句子进行语义相似度评分。 示例： “A man is playing a large flute.” vs. “A man is playing a flute.”（相似度得分） QQP（Quora Question Pairs）: 任务： 判断给定的两个问题是否是语义上相似的。 示例： “How can I be a good geologist?” vs. “What should I do to be a great geologist?”（相似） MNLI（MultiNLI）: 任务： 自然语言推断任务，给定一个前提句子和一个假设句子，判断假设在给定前提下是蕴含、矛盾还是中立关系。 示例： Premise: “The cat is sitting on the windowsill.”， Hypothesis: “The cat is outside.”（矛盾） QNLI（Question Natural Language Inference）: 任务： 是MNLI任务的变体，专注于问题和短文本之间的推断关系。 RTE（Recognizing Textual Entailment）: 任务： 自然语言推断任务，判断给定的两个文本之间是否存在蕴涵关系。 示例： Text: “A soccer game with multiple males playing.”， Hypothesis: “Some men are playing a sport.”（蕴涵） 以上是GLUE基准测试中包括的九个任务，每个任务都有一组训练和测试数据，用于评估模型在不同自然语言处理任务上的性能。这些任务旨在涵盖多样的自然语言理解方面，从而全面评估模型的通用性。","link":"/2023/11/13/%E7%9F%A5%E8%AF%86%E7%82%B9/%E3%80%90%E7%9F%A5%E8%AF%86%E7%82%B9%E3%80%91GLUE/"},{"title":"L1正则化","text":"L1正则化是一种在机器学习中用于降低模型复杂度的技术。它通过向模型的损失函数添加L1范数（向量中各元素绝对值的和）的惩罚来实现。L1正则化的目的是促使模型学习到稀疏权重，即让一些特征对应的权重变为零，从而减少模型的复杂性。 对于一个线性回归模型，L1正则化的损失函数可以写成： $$ J(\\theta) = \\text{MSE}(\\theta) + \\lambda \\sum_{i=1}^{n} | \\theta_i | $$ 其中： $J(\\theta)$ 是带有L1正则化的损失函数； $\\text{MSE}(\\theta)$ 是均方误差（Mean Squared Error）损失函数，用于衡量模型的预测与实际值之间的差距； $\\lambda$ 是正则化强度，控制着正则化对总损失的影响； $\\sum_{i=1}^{n} | \\theta_i |$ 是模型权重向量 (\\theta) 中所有权重的绝对值之和。 L1正则化的效果是，一些特征对应的权重会变为零，从而达到特征选择（feature selection）的效果。这使得模型更加简单，减少了过拟合的风险，并提高了模型的泛化能力。 在深度学习中，L1正则化同样可以应用于神经网络的权重，通过控制权重的稀疏性来提高模型的泛化性能。在实践中，通常使用梯度下降等优化算法来最小化包含L1正则项的损失函数。","link":"/2023/11/13/%E7%9F%A5%E8%AF%86%E7%82%B9/%E3%80%90%E7%9F%A5%E8%AF%86%E7%82%B9%E3%80%91L1%E6%AD%A3%E5%88%99%E5%8C%96/"},{"title":"L2归一化","text":"L2归一化，也称为L2范数归一化，是一种向量归一化的方法，它将向量的每个元素除以其L2范数（Euclidean范数）。 L2范数是向量元素的平方和的平方根。对于一个向量 ($x = [x_1, x_2, …, x_n]$)，其L2范数表示为：$$ |x|_2 = \\sqrt{x_1^2 + x_2^2 + … + x_n^2} $$ L2归一化的过程是将向量的每个元素除以其L2范数，得到一个具有单位长度的向量。具体而言，对于向量 $x$，它的L2归一化 $\\hat{x}$ 计算如下： $$ \\hat{x} = \\frac{x}{|x|_2} $$ L2归一化的目标是将向量的模（长度）缩放到1，从而使得向量在空间中的表示更加规范化，便于比较和处理。在深度学习中，L2归一化有时被用作正则化的手段，可以帮助控制模型的复杂度，防止过拟合。 在机器学习和深度学习中，除了L2归一化，还有L1归一化和Lp范数归一化等不同的向量归一化方法，用于处理不同的问题和优化需求。","link":"/2023/11/13/%E7%9F%A5%E8%AF%86%E7%82%B9/%E3%80%90%E7%9F%A5%E8%AF%86%E7%82%B9%E3%80%91L2%E5%BD%92%E4%B8%80%E5%8C%96/"},{"title":"Oracle重要性采样","text":"Oracle重要性采样（Oracle Importance Sampling）是一种用于估计机器学习模型的边缘似然或边缘概率的方法。它是基于重要性采样的一种改进方法，在一些情况下可以提供更准确的估计。 在机器学习中，边缘似然或边缘概率是指在给定输入数据的情况下，对目标变量的概率分布进行建模。然而，计算边缘似然或边缘概率可能会面临挑战，特别是在高维空间或复杂模型中。 Oracle重要性采样通过引入一个“Oracle”模型来改进传统的重要性采样方法。这个Oracle模型是一个近似目标分布的模型，可以根据先验知识、专家领域知识或其他渠道获得。Oracle模型的目标是更好地近似真实的目标分布，以提供更准确的重要性权重。 在Oracle重要性采样中，首先使用一个基准模型（通常是一个简化的模型）对边缘似然或边缘概率进行采样。然后，使用Oracle模型对相同的输入数据进行采样。通过比较基准模型和Oracle模型的采样结果，可以计算出重要性权重，用于对基准模型的采样结果进行校正。 通过使用Oracle重要性采样，可以利用Oracle模型中的先验知识或专家领域知识来改进重要性采样的准确性。这在一些情况下可以提供更可靠和准确的边缘似然或边缘概率估计，从而帮助解决机器学习中的一些挑战。","link":"/2023/11/05/%E7%9F%A5%E8%AF%86%E7%82%B9/%E3%80%90%E7%9F%A5%E8%AF%86%E7%82%B9%E3%80%91Oracle%E9%87%8D%E8%A6%81%E6%80%A7%E9%87%87%E6%A0%B7/"},{"title":"NLI","text":"NLI 是 “Natural Language Inference”（自然语言推断）的缩写。这是一种自然语言处理（NLP）任务，旨在评估一个系统对于给定的两个句子之间的关系的理解能力。通常情况下，这个任务被定义为三个类别的分类问题： 蕴涵（Entailment）： 如果一个句子可以从另一个句子中推断出来，那么这两个句子之间存在蕴涵关系。例如，如果第一个句子是 “狗在跑步”，第二个句子是 “动物在运动”，那么我们可以说第一个句子蕴含（entails）第二个句子。 矛盾（Contradiction）： 如果两个句子之间存在逻辑上的矛盾，那么它们之间存在矛盾关系。例如，如果第一个句子是 “太阳从东方升起”，第二个句子是 “太阳从西方升起”，那么这两个句子之间存在矛盾关系。 中性（Neutral）： 如果两个句子之间既不是蕴涵关系，也不是矛盾关系，那么它们之间是中性关系。例如，第一个句子是 “猫喜欢晒太阳”，第二个句子是 “天空是蓝色的”，这两个句子之间可能没有明显的蕴涵或矛盾关系。 NLI 是自然语言处理中一个重要的任务，它具有广泛的应用，包括问答系统、机器翻译、对话系统等。训练一个在 NLI 任务上表现良好的模型可以提升系统对语义关系的理解能力，从而在各种应用中提供更准确的自然语言理解。","link":"/2023/11/13/%E7%9F%A5%E8%AF%86%E7%82%B9/%E3%80%90%E7%9F%A5%E8%AF%86%E7%82%B9%E3%80%91NLI/"},{"title":"TF-IDF","text":"$TF$是指归一化后的词频，$IDF$是指逆文档频率。给定一个文档集合$D$，有$d_1,d_2,d_3, \\cdots ,d_n \\in D$。 文档集合总共包含$m$个词（注：一般在计算TF−IDF时会去除如“的”这一类的停用词），有$w_1,w_2,w_3,\\cdots,w_m \\in W$。我们现在以计算词$w_i$在文档$d_j$中的$TF-IDF$指为例。$TF$的计算公式为： $$TF=\\frac{freq(i,j)}{max_{len}(j)}$$ 在这里$freq(i,j)$为$w_i$在$d_j$中出现的频率，$max_{len}(j)$为$d_j$长度。 $TF$只能是描述词在文档中的频率，但假设现在有个词为”我们“，这个词可能在文档集$D$中每篇文档中都会出现，并且有较高的频率。那么这一类词就不具有很好的区分文档的能力，为了降低这种通用词的作用，引入了$IDF$。 $IDF$的表达式如下： $$IDF=log(\\frac{len(D)}{n(i)})$$在这里$len(D)$表示文档集合$D$中文档的总数，$n(i)$表示含有$w_i$这个词的文档的数量。 得到$TF$和$IDF$之后，我们将这两个值相乘得到$TF−IDF$的值： $$TF-IDF=TF \\times IDF$$ $TF$可以计算在一篇文档中词出现的频率，而$IDF$可以降低一些通用词的作用。因此对于一篇文档我们可以用文档中每个词的$TF-IDF$组成的向量来表示该文档，再根据余弦相似度这类的方法来计算文档之间的相关性。","link":"/2023/11/17/%E7%9F%A5%E8%AF%86%E7%82%B9/%E3%80%90%E7%9F%A5%E8%AF%86%E7%82%B9%E3%80%91TF-IDF%E7%AE%97%E6%B3%95/"},{"title":"TREC","text":"TREC（Text REtrieval Conference）是一个历史悠久的信息检索领域的国际会议和评测活动。它由美国国家标准与技术研究院（NIST）于1992年开始组织，旨在促进和推动信息检索技术的研究和发展。 TREC会议每年举办一次，吸引了来自学术界、工业界和政府机构的研究人员和专家参与。会议的主要目标是通过组织评测任务和共享数据集，推动信息检索算法和系统的创新与改进，并促进研究社区之间的合作与交流。 TREC评测任务通常涉及信息检索、文本分类、问答系统等相关领域。参与者根据提供的数据集和任务要求，设计和实现自己的检索算法或系统，并在评测期间对其性能进行测试和评估。最终，TREC会议会发布评测结果和报告，以促进对信息检索技术的进一步研究和发展。","link":"/2023/11/04/%E7%9F%A5%E8%AF%86%E7%82%B9/%E3%80%90%E7%9F%A5%E8%AF%86%E7%82%B9%E3%80%91TREC/"},{"title":"Seq2Seq","text":"Seq2Seq（Sequence-to-Sequence）是一种序列到序列的神经网络模型，也被称为编码-解码模型。它主要用于处理具有不定长输入和输出序列的任务，如机器翻译、文本摘要、对话生成等。 Seq2Seq模型由两个主要组件组成：编码器（Encoder）和解码器（Decoder）。编码器将输入序列（源语言）转换为一个固定长度的向量，捕捉输入序列的语义信息。解码器则将该向量作为输入，逐步生成目标序列（目标语言）。 编码器和解码器都可以使用循环神经网络（如LSTM或GRU）来建模序列信息。编码器通过逐步读取输入序列的每个元素，并在每个时间步产生一个编码器隐藏状态。最后，编码器隐藏状态中包含了输入序列的总体表示。 解码器使用编码器的隐藏状态作为初始状态，并逐步生成目标序列的每个元素。在每个时间步，解码器接收上一个时间步的输出和当前的隐藏状态作为输入，并生成下一个输出和隐藏状态。这个过程一直持续到生成完整的目标序列为止。 Seq2Seq模型的训练过程通常使用教师强制（Teacher Forcing）方法，即将目标序列的真实值作为解码器的输入，以便更好地引导模型学习正确的序列生成。在推断阶段，解码器则使用自己的前一个输出作为下一个时间步的输入，以逐步生成预测的目标序列。 Seq2Seq模型的优点在于能够处理不定长的输入和输出序列，并且能够学习序列之间的语义关系。它在机器翻译、对话生成等任务中取得了显著的成功，并成为自然语言处理领域的重要模型之一。","link":"/2023/11/07/%E7%9F%A5%E8%AF%86%E7%82%B9/%E3%80%90%E7%9F%A5%E8%AF%86%E7%82%B9%E3%80%91Seq2Seq/"},{"title":"nDCG","text":"nDCG（Normalized Discounted Cumulative Gain）是一种用于评估信息检索和推荐系统排序质量的指标。它是对DCG指标的标准化扩展。 在信息检索和推荐系统中，排序是将相关性较高的项目（例如搜索结果或推荐项）排在前面的过程。DCG是一种度量排序质量的指标，它考虑了项目的相关性以及它们在排序列表中的位置。DCG通过对相关性进行折扣，根据项目在排序列表中的位置赋予不同的权重，然后将这些权重进行累加。 nDCG对DCG进行标准化，以便进行不同排序列表之间的比较。它通过将DCG除以在理想排序下的最大可能DCG，得到一个取值范围在0到1之间的归一化指标。理想排序是指将相关性最高的项目排在最前面的排序。 nDCG的计算方式如下： 1nDCG = DCG / IDCG 其中，DCG是折扣累计增益（Discounted Cumulative Gain），IDCG是在理想排序下的最大可能折扣累计增益（Ideal DCG）。 nDCG是一种常用的排序质量指标，广泛应用于信息检索、推荐系统和广告排序等领域。它能够反映出排序结果的相关性和顺序，从而帮助评估和比较不同排序算法或系统的性能。较高的nDCG值表示排序结果更符合用户的偏好和期望。","link":"/2023/11/10/%E7%9F%A5%E8%AF%86%E7%82%B9/%E3%80%90%E7%9F%A5%E8%AF%86%E7%82%B9%E3%80%91nDCG/"},{"title":"Zero-shot","text":"“Zero-shot” 是一种指在模型训练阶段未见过特定任务或类别的能力。在这种情况下，模型被要求执行没有直接训练样本的任务。这对于泛化到新领域或新任务非常有用。 举几个例子来说明 zero-shot 的概念： 图像分类： 假设有一个图像分类模型，经过训练可以识别猫、狗、汽车等类别。如果在测试阶段，模型被要求对一些在训练数据中从未见过的类别，比如大象、飞机进行分类，而它仍能够正确分类，那么这就是 zero-shot 图像分类。 文本情感分析： 假设有一个情感分析模型，它在训练阶段只见过积极和消极的情感类别。如果在测试阶段，模型被要求对一种新的情感类别，比如中立或焦虑进行分类，而它能够适应并进行准确的分类，那么这就是 zero-shot 情感分析。 自然语言处理（NLP）中的零样本学习： 在NLP领域，zero-shot 学习可能涉及到对一些未在训练中见过的主题或任务的理解。例如，一个文本生成模型在训练时只接触到了电影评论，但在测试时可以生成关于体育比赛的文本，而且质量仍然很高，这就是 zero-shot 学习。 Zero-shot 能力通常需要模型具有很强的泛化能力，能够推广到新的情境和任务。在深度学习中，一些预训练的模型（如 GPT、BERT）通过大规模训练来获得广泛的知识，使它们在 zero-shot 或 few-shot 任务上表现出色。","link":"/2023/11/12/%E7%9F%A5%E8%AF%86%E7%82%B9/%E3%80%90%E7%9F%A5%E8%AF%86%E7%82%B9%E3%80%91zero-shot/"},{"title":"倒排索引","text":"倒排索引（Inverted Index）是一种常用的索引结构，用于加速文本检索过程。它将文档集合中的每个单词与包含该单词的文档进行关联，从而能够高效地根据关键词进行检索。以下是一些使用倒排索引的原因： 快速定位文档：倒排索引允许通过关键词快速定位包含该关键词的文档。相比于遍历整个文档集合来查找匹配的文档，倒排索引可以通过索引结构直接定位到相关文档，极大地提高了检索效率。 减少搜索空间：倒排索引可以根据关键词的出现情况，快速缩小待搜索的文档范围。通过在索引中查找关键词，可以排除不包含关键词的文档，从而减少了需要搜索的文档数量，提高了检索速度。 支持布尔检索：倒排索引可以方便地支持布尔检索操作，如AND、OR、NOT等逻辑运算。通过对多个关键词进行逻辑组合，可以快速筛选出满足特定条件的文档集合，从而实现更精确的检索。 支持短语查询：倒排索引可以支持短语查询，即根据关键词的顺序进行检索。通过记录关键词在文档中的位置信息，可以识别并返回包含特定短语的文档，满足对短语匹配的需求。 节省存储空间：倒排索引的结构相对紧凑，可以有效地节省存储空间。通过将文档的关键词与文档ID进行关联，而不是存储完整的文档内容，可以大大减少索引的存储需求。 总结而言，倒排索引能够通过关键词快速定位文档，减少搜索空间，支持布尔检索和短语查询，并且节省存储空间。这些优势使得倒排索引成为文本检索领域中常用的索引结构，提高了检索效率和准确性。","link":"/2023/11/06/%E7%9F%A5%E8%AF%86%E7%82%B9/%E3%80%90%E7%9F%A5%E8%AF%86%E7%82%B9%E3%80%91%E5%80%92%E6%8E%92%E7%B4%A2%E5%BC%95/"},{"title":"梯度范数","text":"梯度范数指的是在多元函数中，梯度向量的长度或大小。梯度是一个向量，包含了函数在每个维度上的偏导数，而梯度的范数则表示了这个向量的长度。 在机器学习和深度学习中，梯度通常指的是损失函数对于模型参数的偏导数。在训练模型的过程中，通过梯度下降等优化算法，我们尝试最小化损失函数，以更新模型的参数。梯度的方向指向损失函数上升最快的方向，而梯度的大小（即梯度范数）则表示了这个上升速度。 梯度范数越大，意味着函数在当前点的变化越快，优化算法在这个方向上的调整也就更大。梯度下降算法会朝着梯度的相反方向（即梯度的负方向）更新参数，以使损失函数尽可能减小，直至找到局部最小值或全局最小值（在凸优化问题中）。 在深度学习中，梯度范数的大小对训练过程至关重要。太大的梯度可能导致训练不稳定，甚至出现梯度爆炸的问题；而太小的梯度可能导致训练过慢或者陷入局部最优点。因此，调节学习率、使用合适的优化器和正则化技术等方法，都是为了更好地控制梯度的大小和方向，以有效地训练深度学习模型。","link":"/2023/11/03/%E7%9F%A5%E8%AF%86%E7%82%B9/%E3%80%90%E7%9F%A5%E8%AF%86%E7%82%B9%E3%80%91%E6%A2%AF%E5%BA%A6%E8%8C%83%E6%95%B0/"},{"title":"矩阵分解","text":"矩阵分解是将一个矩阵拆分成多个子矩阵或分量的过程。它是线性代数和数值计算中的一个重要技术，被广泛应用于各种领域，如数据分析、图像处理、推荐系统等。 在矩阵分解中，常见的几种分解方法包括： LU 分解：LU 分解将一个矩阵拆分为一个下三角矩阵 L 和一个上三角矩阵 U 的乘积。这种分解可以用于解线性方程组、计算矩阵的行列式和逆等。 QR 分解：QR 分解将一个矩阵拆分为一个正交矩阵 Q 和一个上三角矩阵 R 的乘积。QR 分解常用于求解最小二乘问题、计算特征值和特征向量等。 奇异值分解（SVD）：奇异值分解将一个矩阵拆分为三个矩阵的乘积，即 A = UΣV^T，其中 U 和 V 是正交矩阵，Σ 是一个对角矩阵。SVD 在降维、图像压缩、推荐系统等领域有广泛应用。 特征值分解：特征值分解将一个方阵拆分为特征向量矩阵和特征值对角矩阵的乘积。特征值分解可以用于求解线性微分方程和矩阵的幂等等。 这些矩阵分解方法可以帮助我们理解矩阵的结构和性质，简化复杂的计算问题，提取矩阵的重要信息，并在各种应用中提供高效的数值计算工具。","link":"/2023/11/06/%E7%9F%A5%E8%AF%86%E7%82%B9/%E3%80%90%E7%9F%A5%E8%AF%86%E7%82%B9%E3%80%91%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3/"},{"title":"自回归和非自回归","text":"自回归（Autoregressive，简称AR）和非自回归（Non-Autoregressive，简称NAR）是两种不同的建模方法，常用于序列生成任务，特别是自然语言处理领域。 自回归模型：自回归模型是一种基于时间顺序的建模方法，其中当前时间步的输出依赖于之前的输出。在自回归模型中，生成的序列是按照时间顺序逐步生成的，每一步都依赖于之前的生成结果。典型的自回归模型包括基于循环神经网络（RNN）的模型，如循环神经网络语言模型（RNNLM），以及基于Transformer的模型，如GPT（Generative Pre-trained Transformer）。这些模型在生成文本、机器翻译等任务上表现出色，因为它们能够利用上下文信息进行逐步生成，保持一定的连贯性和上下文一致性。 非自回归模型：非自回归模型是指生成过程中每个时间步之间相互独立，不依赖于之前的生成结果。与自回归模型不同，非自回归模型可以同时并行地生成整个序列，而不需要等待前面的结果。这种并行生成的特点使得非自回归模型在速度上具有优势，能够更快地生成序列。典型的非自回归模型包括基于生成对抗网络（GAN）的模型和基于自注意力机制的模型，如Mask-Predict和MASS（Masked Sequence to Sequence）。然而，非自回归模型往往在生成质量上存在一定的挑战，因为缺乏上下文信息的依赖关系，可能导致生成结果的不连贯性和语义不准确性。 总结而言，自回归模型在生成任务中能够保持上下文的连贯性，但生成速度较慢；非自回归模型可以快速生成序列，但在生成质量上可能存在一定的问题。选择使用哪种模型取决于具体的任务需求和平衡生成质量与生成速度的要求。","link":"/2023/11/06/%E7%9F%A5%E8%AF%86%E7%82%B9/%E3%80%90%E7%9F%A5%E8%AF%86%E7%82%B9%E3%80%91%E8%87%AA%E5%9B%9E%E5%BD%92%E5%92%8C%E9%9D%9E%E8%87%AA%E5%9B%9E%E5%BD%92/"},{"title":"启发式函数","text":"启发式函数（Heuristic Function）是在问题求解中用于评估可能解决方案的一种函数。它基于一些经验规则或启发性的指导，为每个可能的解决方案分配一个估计的价值或代价。 启发式函数通常用于启发式搜索算法中，这些算法通过评估候选解决方案的启发式函数值来引导搜索过程，以便更快地找到较优的解决方案。 启发式函数的设计取决于具体的问题。在某些情况下，启发式函数可能基于问题的特定知识和领域专家的经验。这些函数可以根据问题的特征和目标进行定义，以提供有关解决方案质量的估计。 在启发式搜索算法中，启发式函数可以用来选择下一步要扩展的节点，以便更有可能找到更好的解决方案。通过合理设计和利用启发式函数，可以加速搜索过程，减少搜索空间，并提高解决问题的效率。 需要注意的是，启发式函数是基于经验和启发性的指导，它并不保证找到最优解决方案。它可能在某些情况下受限于启发性的偏见或局限性，因此在应用启发式函数时需要进行权衡和评估。","link":"/2023/11/07/%E7%9F%A5%E8%AF%86%E7%82%B9/%E3%80%90%E7%9F%A5%E8%AF%86%E7%82%B9%E3%80%91%E5%90%AF%E5%8F%91%E5%BC%8F%E5%87%BD%E6%95%B0/"},{"title":"矩阵除法","text":"在矩阵运算中，A \\ B 和 A / B 是两种不同的操作，它们分别表示矩阵的左除和右除运算。 A \\ B：表示矩阵 B 对矩阵 A 进行左除运算。这意味着通过求解线性方程组 A * X = B 来找到矩阵 X 的值。其中，A 是系数矩阵，B 是结果矩阵，X 是未知变量矩阵。左除运算可以用来求解线性方程组的解，例如在线性回归和最小二乘法中经常使用。 A / B：表示矩阵 B 对矩阵 A 进行右除运算。这意味着通过求解线性方程组 X * A = B 来找到矩阵 X 的值。其中，A 是系数矩阵，B 是结果矩阵，X 是未知变量矩阵。右除运算也可以用来求解线性方程组的解，但是相对较少使用。 需要注意的是，左除和右除运算的结果可能不同，因为矩阵的乘法不满足交换律。所以 A \\ B 和 A / B 的结果通常是不同的。","link":"/2023/11/05/%E7%9F%A5%E8%AF%86%E7%82%B9/%E3%80%90%E7%9F%A5%E8%AF%86%E7%82%B9%E3%80%91%E7%9F%A9%E9%98%B5%E9%99%A4%E6%B3%95/"},{"title":"自然问题","text":"自然问题（Nature Question）是指关于自然界、科学领域或人类与自然环境相关的问题。这些问题通常涉及自然现象、生物学、物理学、化学、地球科学等科学领域的知识和概念。 自然问题可以涉及各种主题，例如天文学中的宇宙起源和星系形成、生物学中的进化和生物多样性、物理学中的力、能量和运动等。这些问题可能涉及到科学研究、实验观察、理论推理和数据分析等方法来解决。 自然问题的特点是它们基于现实世界中的观察和现象，并通过科学方法和理论来解答。这些问题推动了科学的进步和发展，激发了科学家们的思考和探索。 举个例子，一些自然问题可能包括： 如何解释黑洞的形成和行为？ 为什么某些植物能够进行光合作用？ 地球上的生命是如何起源的？ 为什么水在低温下会结冰？ 如何预测地震和其他自然灾害？ 这些问题都是关于自然界和科学领域的重要课题，研究人员通过实证和理论的方法来回答这些问题，以增加我们对自然世界的理解。","link":"/2023/11/05/%E7%9F%A5%E8%AF%86%E7%82%B9/%E3%80%90%E7%9F%A5%E8%AF%86%E7%82%B9%E3%80%91%E8%87%AA%E7%84%B6%E9%97%AE%E9%A2%98(NQ)/"},{"title":"词袋模型","text":"词袋模型（Bag of Words，简称BoW）是信息检索领域中常用的一种文本表示方法。它将文本表示为一个无序的词汇项集合，忽略了文本中词汇的顺序和语法结构，仅关注每个词汇在文本中出现的频率。 以下是词袋模型的基本步骤和解释： 构建词汇表（Vocabulary）： 首先，从文本数据中提取所有不同的词汇，形成一个词汇表。这个词汇表包含了文本中出现的所有单词，去除了停用词等不重要的词汇。 向量化文本： 对于每个文本样本，将其表示为一个与词汇表等长的向量，向量的每个元素对应于词汇表中的一个词汇。向量的每个元素表示对应词汇在文本中出现的频率。 例如，如果有一个词汇表 [&quot;apple&quot;, &quot;orange&quot;, &quot;banana&quot;]，而文本 “I like apple and apple” 则可以表示为向量 [2, 0, 0]，其中第一个元素表示 “apple” 出现的次数，其他元素表示 “orange” 和 “banana” 的出现次数。 忽略词序和语法： 词袋模型忽略了文本中词汇的顺序和语法结构，只关注词汇的出现频率。这意味着具有相同词汇分布的文本在词袋表示中是相似的，而不考虑它们的词汇顺序。 稀疏表示： 由于大多数文本只包含词汇表中的一小部分词汇，词袋模型的表示通常是稀疏的，即大多数元素为零。这种表示形式有效地减少了存储和计算的复杂性。 词袋模型是一种简单而有效的文本表示方法，常用于文本分类、信息检索和自然语言处理任务。然而，它也有一些局限性，例如无法捕捉词汇之间的语义关系和上下文信息。为了解决这些问题，后续的模型，如词嵌入（Word Embeddings）和深度学习模型，被引入以更好地捕获语义信息。","link":"/2023/11/12/%E7%9F%A5%E8%AF%86%E7%82%B9/%E3%80%90%E7%9F%A5%E8%AF%86%E7%82%B9%E3%80%91%E8%AF%8D%E8%A2%8B%E6%A8%A1%E5%9E%8BBoW/"},{"title":"高斯建模","text":"高斯建模（Gaussian modeling）是一种统计建模方法，基于高斯分布（也称为正态分布）对数据进行建模和分析。高斯分布在概率统计中非常常见，因为它具有许多有用的性质和应用。 高斯建模假设数据的分布服从高斯分布。高斯分布可以由其均值（mean）和方差（variance）来完全描述。它的概率密度函数（probability density function，PDF）在一维情况下可以表示为： $$f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$$ 其中，$x$ 是随机变量的取值，$\\mu$ 是均值，$\\sigma^2$ 是方差。 高斯建模的目标是通过对数据进行观察和分析，估计数据的均值和方差，并基于高斯分布对未知数据进行预测和推断。高斯建模在许多领域中都有广泛的应用，包括统计学、机器学习、图像处理、金融领域等。 在实际应用中，高斯建模可以用于以下任务： 数据建模和分析：通过拟合高斯分布参数，对数据进行建模和分析，了解数据的分布特征和统计性质。 异常检测：通过比较观测数据与高斯分布的概率密度，识别和检测数据中的异常值或离群点。 数据生成和合成：基于已知的高斯分布参数，生成符合相同分布的合成数据，用于模拟和测试。 数据预测和推断：基于已有数据的高斯建模，对未知数据进行预测和推断，如预测某个未来时间点的观测值。 高斯建模是概率统计和机器学习中一个重要的基础方法，广泛应用于各种数据分析和模型构建的任务中。","link":"/2023/11/09/%E7%9F%A5%E8%AF%86%E7%82%B9/%E3%80%90%E7%9F%A5%E8%AF%86%E7%82%B9%E3%80%91%E9%AB%98%E6%96%AF%E5%BB%BA%E6%A8%A1/"},{"title":"使用Vscode调试Linux内核","text":"本文转载来源: 使用Vscode调试Linux内核 上一篇博客我们在虚拟机中编译了Linux内核，并且使用Qemu和gdb进行调试，但是gdb的指令我还不熟练，还是想用vscode来调试，这样也更加方便 Vscode插件安装remote-ssh 安装完成后右边工具栏会多出一个功能 按F1呼出对话框，输入remote-ssh，选择open ssh configuration file。 选择第一个配置文件 123Host 自定义连接名称 HostName 服务器IP地址 User 用户名 C/C++安装C/C++插件 依次点击【运行】-&gt;【打开配置】，将以下配置复制到launch.json中。 该代码不需要更改，直接粘贴 123456789101112131415161718192021222324{ // 使用 IntelliSense 了解相关属性。 // 悬停以查看现有属性的描述。 // 欲了解更多信息，请访问: https://go.microsoft.com/fwlink/?linkid=830387 &quot;version&quot;: &quot;0.2.0&quot;, &quot;configurations&quot;: [ { &quot;name&quot;: &quot;kernel-debug&quot;, //随便起名 &quot;type&quot;: &quot;cppdbg&quot;, &quot;request&quot;: &quot;launch&quot;, &quot;miDebuggerServerAddress&quot;: &quot;127.0.0.1:1234&quot;, //远端调试地址，1234为qemu的监视端口 &quot;program&quot;: &quot;${workspaceFolder}/vmlinux&quot;, //当前目录下的vmlinux &quot;args&quot;: [], &quot;stopAtEntry&quot;: false, &quot;cwd&quot;: &quot;${workspaceFolder}&quot;, &quot;environment&quot;: [], &quot;externalConsole&quot;: false, &quot;logging&quot;: { &quot;engineLogging&quot;: false }, &quot;MIMode&quot;: &quot;gdb&quot;, } ]} Vscode调试在虚拟机中启动qemu在Linux内核文件夹下运行此命令 1qemu-system-x86_64 -kernel ./arch/x86/boot/bzImage -initrd ../initramfs.cpio.gz -append &quot;nokaslr console=ttyS0&quot; -s -S -nographic 打断点打开init/main.c，我打了如下的断点 调试 然后在vscode中就可以看到调试结果了 代码中标红的问题代码标红是缺少compile_commands.json文件 我在B站上学习的时候，是跟着这位up主来的，我的解决方案如下： 在终端键入命令 1./scripts/clang-tools/gen_compile_commands.py 在源码目录下就生成了compile_commands.json文件 在vscode中：ctrl+shipt+p选择C/C++：Edit Coonfigurations, 在c_cpp_properties.json 1234567891011121314151617{ &quot;configurations&quot;: [ { &quot;name&quot;: &quot;Linux&quot;, &quot;includePath&quot;: [ &quot;${workspaceFolder}/**&quot; ], &quot;defines&quot;: [], &quot;compilerPath&quot;: &quot;/usr/bin/gcc&quot;, &quot;cStandard&quot;: &quot;c17&quot;, &quot;cppStandard&quot;: &quot;gnu++17&quot;, &quot;intelliSenseMode&quot;: &quot;linux-gcc-x64&quot;, &quot;compileCommands&quot;: &quot;${workspaceFolder}/compile_commands.json&quot; } ], &quot;version&quot;: 4} 此后，main.c中的代码就不标红了","link":"/2023/11/19/%E6%8A%80%E6%9C%AF%E5%AD%A6%E4%B9%A0/%E4%BD%BF%E7%94%A8Vscode%E8%B0%83%E8%AF%95Linux%E5%86%85%E6%A0%B8/"},{"title":"使用虚拟机进行基于qemu和gdb的Linux内核调试","text":"本文转载来源: 使用虚拟机进行基于qemu和gdb的Linux内核调试 虚拟机配置至少分8个核心（不然编译速度很慢，亲测） 磁盘大小分50G（编译后的内核大小有20多个G！） 打开SSH虚拟机中安装的是ubuntu22.04版本，默认没有安装和启用SSH服务 更新软件源 1sudo apt update &amp;&amp; sudo apt upgrade -y 安装SSH(OpenSSH) 1sudo apt install openssh-server -y 启动SSH服务 1sudo systemctl enable --now ssh 检查是否启动成功 1sudo systemctl status ssh 内核编译准备工作1234567891011121314# 安装相关依赖sudo apt-get install libncurses5-dev libssl-dev bison flex libelf-dev gcc g++ make openssl libc6-dev# 安装gdb，这里使用apt安装（多次尝试的结果）sudo apt-get install gdbgdb --version # gdb版本为12.1#这里选择清华源，国内速度会快很多wget https://mirrors.tuna.tsinghua.edu.cn/kernel/v5.x/linux-5.14.tar.gz# 解压tar -xvf linux-5.14.tar.gz#配置编译选项make menuconfig 然后会在此文件夹下生成 ./config文件 进入该文件，并做以下2处修改 1vim ./.config 安装dwarves软件包（编译报错得出结论） 1sudo apt-get install dwarves BTF报错解决 如果仅仅只进行了上边的配置，会报如下错误（我个人是这样） 12345678910 KSYMS .tmp_vmlinux.kallsyms1.S AS .tmp_vmlinux.kallsyms1.S LD .tmp_vmlinux.kallsyms2 KSYMS .tmp_vmlinux.kallsyms2.S AS .tmp_vmlinux.kallsyms2.S LD vmlinux BTFIDS vmlinuxFAILED: load BTF from vmlinux: Invalid argumentmake: *** [Makefile:1187: vmlinux] Error 255make: *** Deleting file 'vmlinux' 查阅资料后，有三种解决方案：https://devkernel.io/posts/pahole-error/ 我使用的是第二种方法，对pahole软件包进行降级 : 查看pahole的版本，是1.25 1pahole --version 查看pahole的所有可用安装版本 1sudo apt-cache policy pahole 截图如下 我们发现只有两个版本，因此只能降级为 1.22-8 1sudo apt install pahole=1.22-8 编译1make -j8 #8个线程并行编译， 然后可能会弹出一个选择（1，2，3），直接选择1即可。等待一段时间，30分钟左右 是否成功编译完成后，目录下会生成以下,那么就编译成功了 ./vmLinux ./arch/x86/boot/bzImage 其中vmLinux为GDB所需的调试Map文件，bzImage为大内核文件 安装Qemuqemu是一款完全软件模拟(Binary translation)的虚拟化软件，在虚拟化的实现中性能相对较差。但利用它在测试环境中gdb调试Linux内核代码，是熟悉Linux内核代码的一个好方法。 12#安装qemusudo apt-get install qemu 安装编译busybox安装busybox的目的是：借助BusyBox构建极简initramfs，提供基本的用户态可执行程序。 1234wget https://busybox.net/downloads/busybox-1.36.1.tar.bz2 # 去官网找最新版tar -xvf busybox-1.36.1.tar.bz2cd busybox-1.36.1/make menuconfig 在编译busybox之前，我们需要对其进行设置，执行make menuconfig，如下 这里一定要选择静态编译，编译好的可执行文件busybox不依赖动态链接库，可以独立运行，方便构建initramfs。 12make -j 8make &amp;&amp; make install # 安装完成后生成的相关文件会在 _install 目录下 构建initramfs根文件系统1234567891011# 在busybox压缩包的下载目录下,创建的该文件夹，该文件夹下有# wufang@wufang:~/linux_kernel/kernel_compile$ ls# busybox-1.36.1 busybox-1.36.1.tar.bz2 linux-5.14 linux-5.14.tar.gzmkdir initramfscd initramfscp ../busybox-1.29.0/_install/* -rf ./ #将_install文件夹下的所有文件复制到initramfs文件夹下mkdir dev proc syssudo cp -a /dev/{null,console,tty,tty1,tty2,tty3,tty4} dev/rm -f linuxrcvim init 添加如下代码 1234567#!/bin/busybox shecho &quot;{==DBG==} INIT SCRIPT&quot;mount -t proc none /procmount -t sysfs none /sysecho -e &quot;{==DBG==} Boot took $(cut -d' ' -f1 /proc/uptime) seconds&quot;exec /sbin/init 123chmod a+x init 修改文件权限# 完成后，initrams下有如下文件# bin dev init proc sbin sys usr 打包initramfs 1234find . -print0 | cpio --null -ov --format=newc | gzip -9 &gt; ../initramfs.cpio.gz# 此时在busybox压缩包的下载目录下，有如下文件# busybox-1.36.1 initramfs linux-5.14# busybox-1.36.1.tar.bz2 initramfs.cpio.gz linux-5.14.tar.gz 启动Qemu调试内核上述完成之后，就可以启动Qemu来调试内核了,启动代码如下（是一个指令） 1qemu-system-x86_64 -kernel ./arch/x86/boot/bzImage -initrd ../initramfs.cpio.gz -append &quot;nokaslr console=ttyS0&quot; -s -S -nographic qemu-system-x86_64：指定是x86，64位; -kernel ./arch/x86/boot/bzImage：指定启用的内核镜像； -initrd ../initramfs.cpio.gz：指定启动的内存文件系统 -append &quot;nokaslr console=ttyS0&quot; ：附加参数，其中 nokaslr 参数必须添加进来，防止内核起始地址随机化，这样会导致 gdb 断点不能命中； -s ：监听在 gdb 1234 端口； -S ：表示启动后就挂起，等待 gdb 连接； -nographic：不启动图形界面 开启另一个命令行窗口，输入gdb，即可开启调试 1234(gdb) target remote localhost:1234 #连接qemu监听的端口(gdb) break start_kernel #在start_kernel打断点(gdb) break rest_init #在res_init打断点(gdb) c #运行到断点处","link":"/2023/11/18/%E6%8A%80%E6%9C%AF%E5%AD%A6%E4%B9%A0/%E4%BD%BF%E7%94%A8%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%BF%9B%E8%A1%8C%E5%9F%BA%E4%BA%8Evscode%E7%9A%84Linux%E5%86%85%E6%A0%B8%E8%B0%83%E8%AF%95/"},{"title":"线性预热","text":"在AI领域中，线性预热通常指的是在训练神经网络模型时逐渐增加学习率或其他训练参数，以在训练开始阶段更好地引导模型的学习过程。这是一种优化训练过程的策略，可以帮助模型更快地收敛到良好的解决方案。 在神经网络训练中，学习率是一个关键的超参数，它决定了在每次更新模型权重时调整的步长大小。线性预热的想法是，在训练开始时，使用一个相对较小的学习率，然后逐渐增加学习率，使得模型能够更容易地找到全局最优解或更好的局部最优解。 具体来说，线性预热的过程可以描述为以下步骤： 初始学习率设置： 初始阶段使用一个较小的学习率，通常是在零附近。 逐渐增加学习率： 在训练的初始几个epoch中，逐渐增加学习率，可以是线性增加的方式，也可以是其他函数形式。这个过程通常在模型开始学习之前的几个epoch中完成。 常规训练： 一旦线性预热阶段完成，模型将以较高的学习率进行正常训练，这有助于更快地调整权重并学习模型的有效表示。 线性预热的优势在于它可以帮助模型在训练的早期阶段更好地适应数据，防止由于初始的不稳定性而导致的训练困难。这种策略在许多深度学习任务中都有助于加速模型的收敛并提高最终性能。","link":"/2023/11/20/%E7%9F%A5%E8%AF%86%E7%82%B9/%E3%80%90%E7%9F%A5%E8%AF%86%E7%82%B9%E3%80%91%E7%BA%BF%E6%80%A7%E9%A2%84%E7%83%AD/"},{"title":"哈尔滨工程大学——语料库智能检索系统","text":"For：哈尔滨工程大学国际合作教育学院 校内访问地址：哈尔滨工程大学语料库智能检索系统","link":"/2023/05/01/%E9%A1%B9%E7%9B%AE%E7%BB%8F%E5%8E%86/HEU%20Corpus%20Web/"},{"title":"MoHub平台支持工科教学","text":"From: 工业知识模型互联平台-MoHub 原文链接：哈工程李超老师-计算机逻辑设计综合实验课程","link":"/2023/06/10/%E9%A1%B9%E7%9B%AE%E7%BB%8F%E5%8E%86/MoHub%E5%B9%B3%E5%8F%B0%E6%94%AF%E6%8C%81%E5%B7%A5%E7%A7%91%E6%95%99%E5%AD%A6/"},{"title":"基于深度学习的多功能图像处理和社交平台","text":"Contribution：完成基于CNN网络将输入的内容图像与风格图像的目标特征提取，实现画风迁移功能；基于NoGAN生成对抗网络模型进行生成器的预训练与评价，实现黑白图像色彩填充；负责服务端处理任务调度和网页端开发 Award：人工智能创意赛东北赛区二等奖","link":"/2022/10/15/%E9%A1%B9%E7%9B%AE%E7%BB%8F%E5%8E%86/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%A4%9A%E5%8A%9F%E8%83%BD%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E5%92%8C%E7%A4%BE%E4%BA%A4%E5%B9%B3%E5%8F%B0/"},{"title":"基于音频特诊分析的动态鸟类识别分析","text":"Background：旨在提供轻量化的鸟类音频识别与分类工具，帮助森林工作者保护生物多样性 Contribution：使用Kaggle Bird CLEF 2023比赛中的数据集，基于Mel频谱特征提取和EfficientNet-B4模型实现对鸟类音频的自动分类 Award：中国大学生计算机设计大赛黑龙江赛区省级三等奖","link":"/2023/03/20/%E9%A1%B9%E7%9B%AE%E7%BB%8F%E5%8E%86/%E5%9F%BA%E4%BA%8E%E9%9F%B3%E9%A2%91%E7%89%B9%E8%AF%8A%E5%88%86%E6%9E%90%E7%9A%84%E5%8A%A8%E6%80%81%E9%B8%9F%E7%B1%BB%E8%AF%86%E5%88%AB%E5%88%86%E6%9E%90/"},{"title":"基于计算机视觉的盲人视觉辅助智能眼镜","text":"Background：旨在提升盲人和视障人士的生活质量，超越传统导盲辅助工具 Contribution：使用COCO 2017 数据集，基于YOLOv3模型完成目标检测和移动端导盲App设计与开发 Award：“建行杯”第九届中国国际“互联网+”大学生创新创业大赛省级银奖","link":"/2023/07/15/%E9%A1%B9%E7%9B%AE%E7%BB%8F%E5%8E%86/%E5%9F%BA%E4%BA%8E%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%9A%84%E7%9B%B2%E4%BA%BA%E8%A7%86%E8%A7%89%E8%BE%85%E5%8A%A9%E6%99%BA%E8%83%BD%E7%9C%BC%E9%95%9C/"},{"title":"海洋鹰眼——舰船识别与海况观测系统","text":"Background：旨在准确有效的海上船舶识别技术以提高船舶航行安全，是船舶智能化发展的关键技术 Contribution：负责数据标注和强化数据样本工作，使用YOLOv5模型完成目标检测与船只定位，检测准确率达到67.26% Award：“建行杯”第九届中国国际“互联网+”大学生创新创业大赛省级铜奖","link":"/2023/07/15/%E9%A1%B9%E7%9B%AE%E7%BB%8F%E5%8E%86/%E6%B5%B7%E6%B4%8B%E9%B9%B0%E7%9C%BC%E2%80%94%E2%80%94%E8%88%B0%E8%88%B9%E8%AF%86%E5%88%AB%E4%B8%8E%E6%B5%B7%E5%86%B5%E8%A7%82%E6%B5%8B%E7%B3%BB%E7%BB%9F/"},{"title":"tensor的创建","text":"本文转载自【Pytorch笔记】1. tensor的创建 参考视频：深度之眼官方账号：01-02-张量简介与创建 torch.tensor()12345b = torch.tensor(data, dtype=None, device=None, requires_grad=False, pin_memory=False) data：创建的tensor的数据来源，可以是list或numpydtype：数据类型，默认与data一致device：所选设备，cuda/cpurequires_grad：是否需要梯度pin_memory：是否存在锁页内存（与转换效率有关，通常设置为False） 12345678import numpy as npimport torcharr = np.ones((3, 3))print(&quot;ndarray的数据类型：&quot;, arr.dtype)t = torch.tensor(arr)# t = torch.tensor(arr, device='cuda')print(t) 输出： 1234ndarray的数据类型： float64tensor([[1., 1., 1.], [1., 1., 1.], [1., 1., 1.]], dtype=torch.float64) torch.from_numpy(ndarray)功能：从numpy创建tensor注意：从torch.from_numpy创建的tensor与原ndarray共享内存，当修改其中一个的数据，另一个也会被改动。 12345678import numpy as npimport torcharr = np.array([[1, 2, 3], [4, 5, 6]])t = torch.from_numpy(arr)print(t)arr[0, 0] = 7print(t) 输出： 1234tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.int32)tensor([[7, 2, 3], [4, 5, 6]], dtype=torch.int32) torch.zeros()功能：根据给定size创建全0的tensor 123456torch.zeros(*size, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) *size：创建的tensor的尺寸out：输出到哪个tensor当中dtype：创建的tensor中内容的类型layout：tensor在内存中的分布方式，目前支持torch.strided和torch.sparse_coodevice：所在设备，gpu/cpurequires_grad：是否需要梯度 123456import torcht = torch.zeros((3, 3)) #不使用out参数创建tt = torch.tensor([2.])torch.zeros((3, 3), out=tt) #使用out参数输出到已有tensorprint(t, '\\n', tt) 输出： 123456tensor([[0., 0., 0.], [0., 0., 0.], [0., 0., 0.]]) tensor([[0., 0., 0.], [0., 0., 0.], [0., 0., 0.]]) 有一点需要注意，如果像这样使用： 12t = torch.tensor([2.])p = torch.zeros((3, 3), out=t) 那么t和p会指向同一个地址。 torch.zeros_like()功能：根据input的形状创建全0的tensor。 12345torch.zeros_like(input, dtype=None, layout=None, device=None, requires_grad=False) input：创建新的tensor所用的形状的基准，生成的tensor和input的形状相同。注：input只能为tensor，不能是np.array。其余参数和torch.zeros()相同。 123456import numpy as npimport torchdata = torch.tensor(np.array([[1, 2], [3, 4]]))t = torch.zeros_like(data)print(t) 输出： 12tensor([[0, 0], [0, 0]], dtype=torch.int32) torch.ones()功能：根据给定size创建全1的tensor，与torch.zeros()基本一样。 123456torch.ones(*size, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) torch.ones_like()功能：根据input的形状创建全1的tensor，与torch.zeros_like()基本一样。 12345torch.ones_like(input, dtype=None, layout=None, device=None, requires_grad=False) torch.full()功能：根据给定size创建tensor，元素全部赋值为fill_value。 1234567torch.full(size, fill_value, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) fill_value：传入的参数，full()将创建好的tensor全部赋值为fill_value。 12345import numpy as npimport torcht = torch.full(size=(3, 3), fill_value=4)print(t) 输出： 123tensor([[4, 4, 4], [4, 4, 4], [4, 4, 4]]) torch.arange()功能：创建等差的一维tensor。 12345678torch.arange(start=0, end, step=1, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) 范围是[start, end)，左闭右开的，step为步长。其他的参数与上面的函数一样。 12345import numpy as npimport torcht = torch.arange(start=3, end=6, step=1)print(t) 输出： 1tensor([3, 4, 5]) torch.linspace()功能：创建均分的1维tensor。linspace: Linear space，类比下面的logspace。 12345678torch.linspace(start=0, end, steps=100, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) 范围是[start, end]，是闭区间（和arange不同）。steps为生成的tensor的长度。 12345import numpy as npimport torcht = torch.linspace(start=2, end=10, steps=4)print(t) 输出： 1tensor([ 2.0000, 4.6667, 7.3333, 10.0000]) torch.logspace()功能：创建对数均分的1维tensor。logspace: Log space，类比上面的linspace。 123456789torch.logspace(start, end, steps=100, base=10.0, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) start、end、steps与linspace相同。base指对数函数的底，默认为10。和linspace不同的是，我们通过start, end, steps枚举出来的数列，是tensor的内容外面要套一个$log_{base}$。反过来讲，我们枚举的就是以base为底的那些幂数。可以参考下面的代码： 12345import numpy as npimport torcht = torch.logspace(start=1, end=3, steps=3, base=10.0)print(t) 输出： 1tensor([ 10., 100., 1000.]) torch.eye()功能：创建单位对角矩阵，是2维tensor。默认为方阵。 1234567torch.eye(n, m=None, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) n、m：矩阵行数、列数。通常我们只需要设置n。 1234567import numpy as npimport torcht = torch.eye(n=3)p = torch.eye(n=3, m=4)print(t)print(p) 输出： 123456tensor([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]])tensor([[1., 0., 0., 0.], [0., 1., 0., 0.], [0., 0., 1., 0.]]) torch.normal()功能：根据给定的均值$\\mu$和标准差$\\sigma$，通过$N(\\mu,\\sigma)$生成对应尺寸的随机数（具体如何对应尺寸，下面会有说明）。 123torch.normal(mean, std, out=None) mean：均值；std：标准差。均值和标准差的类型，既可以是float，也可以是tensor。此时出现了4种情况，先看示例代码。 1234567891011import numpy as npimport torcht1 = torch.normal(mean=2, std=3, size=(2, 3))print(&quot;t1:\\n&quot;, t1)t2 = torch.normal(mean=torch.arange(1., 3.), std=torch.arange(3., 5.))print(&quot;t2:\\n&quot;, t2)t3 = torch.normal(mean=1., std=torch.arange(1., 5.))print(&quot;t3:\\n&quot;, t3)t4 = torch.normal(mean=torch.arange(1., 4.), std=2.)print(&quot;t4:\\n&quot;, t4) 输出： 123456789t1: tensor([[-0.1495, 0.2061, 3.0486], [ 6.1257, 1.6023, 1.1515]])t2: tensor([-5.4967, 7.4201])t3: tensor([ 3.3218, 0.7347, -3.6644, 3.4812])t4: tensor([2.2250, 1.1026, 0.9171]) mean和std都是float见示例代码中t1。此时我们必须要再加一个参数size，表示我们要生成的tensor的尺寸。生成的t1中每一个元素都是由$N(mean,std)$生成的一个随机数。 mean和std都是tensor见示例代码中t2。经过尝试，需要mean和std的shape相同，这样生成的tensor的对应位置就是由mean和std中对应位置的均值和标准差随机出来的数。 mean是float，std是tensor见示例代码中t3。生成的tensor和std的shape相同，对应的位置是由mean、std中的对应位置的标准差随机出来的数。 mean是tensor，std是float见示例代码中t4。生成的tensor和mean的shape相同，对应的位置是由mean中的对应位置的均值、std随机出来的数。 torch.randn()功能：生成标准正态分布$N(0,1)$。 123456torch.randn(*size, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) 12345import numpy as npimport torcht = torch.randn((2, 3))print(t) 输出： 12tensor([[ 0.2370, -1.4351, -0.0624], [ 0.7974, 1.2915, -1.0052]]) torch.randn_like()功能：生成与给定tensor同样尺寸的标准正态分布tensor。（类比torch.zeros_like()和torch.ones_like()） 123456import numpy as npimport torchtmp = torch.ones((2, 3))t = torch.randn_like(tmp)print(t) 输出： 12tensor([[-0.3384, -0.8061, 0.7020], [ 0.1602, 0.6525, -0.6860]]) torch.rand()、torch.rand_like()功能：在区间[0,1)上生成均匀分布，示例略。 torch.randint()、torch.randint_like()功能：在区间[low, high)生成整数均匀分布。 torch.randperm()功能：生成从0到n-1的随机排列，n是张量的长度。 123456torch.randperm(n, out=None, dtype=torch.int64, layout=torch.strided, device=None, requires_grad=False) 12345import numpy as npimport torcht = torch.randperm(5)print(t) 输出： 1tensor([3, 2, 1, 0, 4]) torch.bernoulli()功能：以input为概率，生成伯努利分布（0-1分布，两点分布） 1234torch.bernoulli(input, *, generator=None, ont=None) input为概率值，为tensor。 1234567import numpy as npimport torchp = torch.rand((3, 3))t = torch.bernoulli(p)print(&quot;p:\\n&quot;, p)print(&quot;t:\\n&quot;, t) 输出： 12345678p: tensor([[0.6881, 0.7921, 0.4212], [0.6857, 0.4809, 0.4009], [0.2400, 0.5160, 0.1303]])t: tensor([[1., 1., 1.], [1., 1., 0.], [0., 0., 0.]])","link":"/2023/09/01/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/%E3%80%90Pytorch%E7%AC%94%E8%AE%B0%E3%80%911.%20tensor%E7%9A%84%E5%88%9B%E5%BB%BA/"},{"title":"张量操作","text":"本文转载自【Pytorch笔记】2.张量操作 参考视频：深度之眼官方账号 - 01-03-mp4-张量操作与线性回归 torch.cat()功能：将两个tensor按照给定的维度进行拼接。 123torch.cat(tensors, dim=0, out=None) tensors：包含一坨tensor的list或tuple；dim：指定拼接的维度；out：输出到哪个tensor中。 1234567import numpy as npimport torcht1 = torch.ones((4, 4))t2 = torch.zeros((4, 3))t3 = torch.cat((t1, t2), dim=1)print(t3) 输出： 1234tensor([[1., 1., 1., 1., 0., 0., 0.], [1., 1., 1., 1., 0., 0., 0.], [1., 1., 1., 1., 0., 0., 0.], [1., 1., 1., 1., 0., 0., 0.]]) torch.stack()功能：将多个tensor按照指定的维度进行堆叠（stack），创建一个新的张量。 123torch.stack(tensors, dim=0, out=None) 1234567import numpy as npimport torcht1 = torch.ones((2, 2))t2 = torch.zeros((2, 2))t3 = torch.stack((t1, t2), dim=2)print(t3) 输出： 12345tensor([[[1., 0.], [1., 0.]], [[1., 0.], [1., 0.]]]) 两个tensor只能在dim维度上不同，其他维度的大小必须相同。 torch.chunk()功能：将一个tensor在给定维度上进行均等切分，若不能均等切分，最后一份切片的大小小于其他切片。 123torch.chunk(input, chunks, dim=0) input：被切分的tensor；chunks：切分份数；dim：在dim维度上进行切分 123456import numpy as npimport torcht1 = torch.ones((4, 2))t_list = torch.chunk(t1, chunks=2, dim=0)print(t_list) 输出： 123(tensor([[1., 1.], [1., 1.]]), tensor([[1., 1.], [1., 1.]])) torch.split()功能：将tensor按照给定的长度进行切分。 123torch.split(tensor, split_size_or_sections, dim=0) split_size_or_sections：如果是int，则切出的长度全为这个值；如果是列表，则按列表中的数作为尺寸来切。dim：在dim维度上进行切分。 123456import numpy as npimport torcht1 = torch.ones((4, 2))t_list = torch.split(t1, (1, 2, 1), dim=0)print(t_list) 输出： 12(tensor([[1., 1.]]), tensor([[1., 1.], [1., 1.]]), tensor([[1., 1.]])) torch.index_select()功能：在维度dim上，按index索引数据，返回依据index索引拼接的tensor。 1234torch.index_select(input, dim, index, out=None) input：待搜索数据的tensor；dim：在维度dim上的维度；index：包含所有索引需求的tensor。 1234567import numpy as npimport torcht1 = torch.tensor([[2, 3], [4, 5], [6, 7]])t2 = torch.tensor([0, 2])t_out = torch.index_select(t1, dim=0, index=t2)print(t_out) 输出： 12tensor([[2, 3], [6, 7]]) torch.masked_select()功能：在维度dim上，按mask索引数据，返回依据mask索引拼接的tensor。 123torch.masked_select(input, mask, out=None) input：待搜索数据的tensor；mask：在维度dim上需要索引的位置为True，不需要索引的位置为False，形成一个全为布尔量的tensor。该tensor与input同形状。 1234567import numpy as npimport torcht1 = torch.tensor([[2, 3], [4, 5], [6, 7]])t2 = torch.tensor([[True, False],[True, True], [False, True]])t_out = torch.masked_select(t1, mask=t2)print(t_out) 输出 1tensor([2, 4, 5, 7]) torch.reshape()功能：变换张量形状。（当张量在内存中连续时，reshape后的tensor将存储在原来的地址内） 12torch.reshape(input, shape) input：待变换形状的tensor；shape：变换后的形状。（形状中至多一位可以填为-1，由于原tensor中的元素数量是固定的，-1的位置应该填写的数会自动算出） 123456import numpy as npimport torcht1 = torch.tensor([[2, 3, 4], [5, 6, 7]])t2 = torch.reshape(t1, (3, 2))print(t2) 输出： 123tensor([[2, 3], [4, 5], [6, 7]]) torch.transpose()功能：交换张量的两个维度。 123torch.transpose(input, dim0, dim1) dim0、dim1：被交换的两个维度。 123456import numpy as npimport torcht = torch.tensor([[2, 3, 4], [5, 6, 7]])t1 = torch.transpose(t, 0, 1)print(t1) 输出： 123tensor([[2, 5], [3, 6], [4, 7]]) torch.t()功能：仅限于二维tensor的维度交换。（二维tensor中，等价于torch.transpose(input, 0, 1)） 1torch.t(input) input：待进行维度交换的tensor。 123456import numpy as npimport torcht = torch.tensor([[2, 3, 4], [5, 6, 7]])t1 = torch.t(t)print(t1) 输出： 123tensor([[2, 5], [3, 6], [4, 7]]) torch.squeeze()功能：压缩维度为1的轴。 123torch.squeeze(input, dim=None, out=None) dim：指定维度进行压缩，这个维度的长度必须为1。 123456import numpy as npimport torcht = torch.tensor([[2], [3], [4], [5], [6]])t1 = torch.squeeze(t)print(t1) 输出： 1tensor([2, 3, 4, 5, 6]) torch.unsqueeze()功能：依据维度dim扩展维度。 123torch.unsqueeze(input, dim, out=None) dim：指定维度dim进行扩展，不同于torch.squeeze()的默认为0，这里必须要指明dim的数值。 123456import numpy as npimport torcht = torch.tensor([2, 3, 4, 5, 6])t1 = torch.unsqueeze(t, dim=0)print(t1) 输出： 1tensor([[2, 3, 4, 5, 6]])","link":"/2023/09/17/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/%E3%80%90Pytorch%E7%AC%94%E8%AE%B0%E3%80%912.%E5%BC%A0%E9%87%8F%E6%93%8D%E4%BD%9C/"},{"title":"数学运算","text":"本文转载自【Pytorch笔记】3.数学运算 参考视频：深度之眼官方账号 - 01-03-mp4-张量操作与线性回归 torch.add()功能：逐元素计算input+alpha×other。 1234torch.add(input, alpha=1, other, out=None) input：tensor；alpha：other的系数，是个实数；other：和input同样形状的tensor。 123456import torcht1 = torch.tensor([[2, 3], [4, 5]])t2 = torch.tensor([[1, 1], [2, 2]])t = torch.add(t1, alpha=2, other=t2)print(t) 输出： 12tensor([[4, 5], [8, 9]]) torch.sub()功能：逐元素计算input-alpha×other。 1234torch.sub(input, alpha=1, other, out=None) input：tensor；alpha：other的系数，是个实数；other：和input同样形状的tensor。 123456import torcht1 = torch.tensor([[2, 3], [4, 5]])t2 = torch.tensor([[1, 1], [2, 2]])t = torch.add(t1, alpha=2, other=t2)print(t) 输出： 12tensor([[0, 1], [0, 1]]) torch.mul()功能：逐元素计算$out_i=input_i \\times other_i$ 12torch.mul(input, other) input：tensor；other：和input同样尺寸的tensor。other支持广播，即可以只向other传入一个数，torch利用广播机制变成同样尺寸的tensor。 123456import torcht1 = torch.tensor([[9, 12], [15, 18]])t2 = torch.tensor([[3, 3], [2, 2]])t = torch.mul(t1, other=t2)print(t) 输出： 12tensor([[27, 36], [30, 36]]) torch.div()功能：逐元素计算$out_i=\\frac{input_i}{other}$ 12torch.div(input, other) input：tensor；other：和input同样尺寸的、元素不能为0的tensor。other支持广播，即可以只向other传入一个数，torch利用广播机制变成同样尺寸的tensor。 123456import torcht1 = torch.tensor([[9, 12], [4, 6]])t2 = torch.tensor([[3, 3], [2, 2]])t = torch.div(t1, other=t2)print(t) 输出： 12tensor([[3., 4.], [2., 3.]]) torch.addcmul()功能：逐元素计算$out_i=input_i+value \\times tensor1_i \\times tensor2_i$ 12345torch.addcmul(input, value=1, tensor1, tensor2, out=None) input：输入的tensor；value：见公式，实数；tensor1：和input相同形状的tensor，见公式；tensor2：和input相同形状的tensor，见公式。 1234567import torcht1 = torch.tensor([[2., 3.], [4., 5.]])t2 = torch.tensor([[4., 6.], [8., 10.]])t3 = torch.tensor([[2., 2.], [2., 2.]])t = torch.addcmul(t1, value=2, tensor1=t2, tensor2=t3)print(t) 输出： 12tensor([[18., 27.], [36., 45.]]) torch.addcdiv()功能：逐元素计算$out_i=input_i+value \\times\\frac{tensor1_i}{tensor2_i}$。 12345torch.addcdiv(input, value=1, tensor1, tensor2, out=None) input：输入的tensor；value：见公式，实数；tensor1：和input相同形状的tensor，见公式；tensor2：和input相同形状但是元素中不能出现0的tensor，见公式。注：input、tensor1、tensor2的内容需要是浮点型。如果使用整数会报如下错误： RuntimeError: Integer division with addcdiv is no longer supported, and in a future release addcdiv will perform a true division of tensor1 and tensor2. The historic addcdiv behavior can be implemented as (input + value * torch.trunc(tensor1 / tensor2)).to(input.dtype) for integer inputs and as (input + value * tensor1 / tensor2) for float inputs. The future addcdiv behavior is just the latter implementation: (input + value * tensor1 / tensor2), for all dtypes. 1234567import torcht1 = torch.tensor([[2., 3.], [4., 5.]])t2 = torch.tensor([[4., 6.], [8., 10.]])t3 = torch.tensor([[2., 2.], [2., 2.]])t = torch.addcdiv(t1, value=2, tensor1=t2, tensor2=t3)print(t) 输出： 12tensor([[ 6., 9.], [12., 15.]]) torch.log()功能：逐元素求解$out_i=log_e(input_i)$。 12torch.log(input, out=None) input：待求解的tensor。 12345import torcht1 = torch.tensor([[9., -12.], [15., 18.]])t = torch.log(t1)print(t) 输出： 12tensor([[2.1972, nan], [2.7081, 2.8904]]) torch.log10()功能：逐元素求解$out_i=log_{10}(input_i)$。 12torch.log10(input, out=None) input：待求解的tensor。 12345import torcht1 = torch.tensor([[9., -12.], [15., 18.]])t = torch.log10(t1)print(t) 输出： 12tensor([[0.9542, nan], [1.1761, 1.2553]]) torch.log2()功能：逐元素求解$out_i=log_2(input_i)$。 12torch.log2(input, out=None) input：待求解的tensor。 12345import torcht1 = torch.tensor([[8., -12.], [16., 18.]])t = torch.log2(t1)print(t) 输出： 12tensor([[3.0000, nan], [4.0000, 4.1699]]) torch.exp()功能：逐元素求解$out_i=e^{input_i}$。 12torch.exp(input, out=None) input：待求解的tensor。 123456import mathimport torcht1 = torch.tensor([[-2., 0.], [1., math.log(2.)]])t = torch.exp(t1)print(t) 输出： 12tensor([[0.1353, 1.0000], [2.7183, 2.0000]]) torch.pow()功能：逐元素求解$out_i=x_i^{exponent_i}$ 123torch.pow(input, exponent, out=None) input：待求解的tensor。exponent：与input相同形状的tensor。如果exponent是一个数，torch会广播成一个和input相同形状的tensor。 12345678import torcht1 = torch.tensor([[1., 2.], [3., 4.]])t2 = torch.tensor([[3., 2.], [4., 2.]])t3 = torch.pow(t1, 2.)t4 = torch.pow(t1, t2)print(t3)print(t4) 输出： 1234tensor([[ 1., 4.], [ 9., 16.]])tensor([[ 1., 4.], [81., 16.]]) tensor.abs()功能：逐元素取绝对值，$out_i=|input_i|$。 12torch.abs(input, out=None) input：待求解的tensor。 12345import torcht1 = torch.tensor([[1., -2.], [-3., 4.]])t = torch.abs(t1)print(t) 输出： 12tensor([[1., 2.], [3., 4.]]) tensor.acos()功能：逐元素求解$out_i=cos^{-1}(input_i)$。 12torch.acos(input, out=None) input：待求解的tensor。 123456import torcht1 = torch.randn(4)print(t1)t = torch.acos(t1)print(t) 输出： 12tensor([ 0.5100, 0.1678, -0.0250, 0.3119])tensor([1.0357, 1.4022, 1.5958, 1.2536]) torch.cosh()功能：逐元素求解$out_i=cosh(input_i)$注：$cosh(x)=\\frac{e^x+e^{-x}}{2}$ 12torch.cosh(input, out=None) input：待求解的tensor。 123456789import torcht1 = torch.randn(4)print(t1)t = torch.cosh(t1)print(t)torch.cosh(input, out=None) 输出： 12tensor([-0.3447, -0.2875, -0.2717, -1.3635])tensor([1.0600, 1.0416, 1.0371, 2.0828]) torch.cos()功能：逐元素求解$out_i=cos(input_i)$ 12torch.cos(input, out=None) input：待求解的tensor。 123456789import torcht1 = torch.randn(4)print(t1)t = torch.cos(t1)print(t)torch.cosh(input, out=None) 输出： 12tensor([-0.6443, -0.8991, 1.2432, -0.3162])tensor([0.7995, 0.6223, 0.3218, 0.9504]) torch.asin()功能：逐元素求解$out_i=sin^{-1}(input_i)$ 12torch.asin(input, out=None) input：待求解的tensor。 123456import torcht1 = torch.randn(4)print(t1)t = torch.asin(t1)print(t) 输出： 12tensor([-0.7372, -0.0238, -1.8213, -0.0912])tensor([-0.8289, -0.0238, nan, -0.0913]) torch.atan()功能：逐元素求解$out_i=tan^{-1}(input_i)$ 12torch.atan(input, out=None) input：待求解的tensor。 123456import torcht1 = torch.randn(4)print(t1)t = torch.atan(t1)print(t) 输出： 12tensor([ 0.3620, -0.6551, 1.0304, 2.1545])tensor([ 0.3474, -0.5799, 0.8003, 1.1362]) torch.atan2()功能：逐元素求解$out_i=tan^{-1}(\\frac{input_i}{other_i})$ 123torch.atan(input, other, out=None) input：待求解的tensor。 12345678import torcht1 = torch.randn(4)print(t1)t2 = torch.randn(4)print(t2)t = torch.atan2(t1, t2)print(t) 输出： 123tensor([ 1.9372, 0.7993, -1.4123, 0.4260])tensor([-1.5106, 1.2147, -1.4479, 0.1674])tensor([ 2.2331, 0.5820, -2.3686, 1.1963])","link":"/2023/09/27/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/%E3%80%90Pytorch%E7%AC%94%E8%AE%B0%E3%80%913.%E6%95%B0%E5%AD%A6%E8%BF%90%E7%AE%97/"},{"title":"梯度计算","text":"本文转载自【Pytorch笔记】4.梯度计算 参考视频：深度之眼官方账号 - 01-04-mp4-计算图与动态图机制 前置知识：计算图可以参考我的笔记：【学习笔记】计算机视觉与深度学习(2.全连接神经网络) 计算图以这棵计算图为例。这个计算图中，叶子节点为x和w。 123456789101112131415import torchw = torch.tensor([1.], requires_grad=True)x = torch.tensor([2.], requires_grad=True)a = torch.add(w, x)b = torch.add(w, 1)y = torch.mul(a, b)# 调用backward()方法，开始反向求梯度y.backward()print(w.grad)print(&quot;is_leaf:\\n&quot;, w.is_leaf, x.is_leaf, a.is_leaf, b.is_leaf, y.is_leaf)print(&quot;gradient:\\n&quot;, w.grad, x.grad, a.grad, b.grad, y.grad) 输出： 12345tensor([5.])is_leaf: True True False False Falsegradient: tensor([5.]) tensor([2.]) None None None 由此可见，非叶子节点在最后不会被保留梯度。这是出于节省空间的需要而这样设计的。实际的计算图会非常大，如果每个节点都保留梯度，会占用非常大的存储空间，而这些节点的梯度对于我们学习并没有什么帮助。 如果非要看他们的梯度，可以这样操作：在a = torch.add(w, x)的后面加上一句a.retain_grad()，这样a的梯度就会被存储起来。输出会变成： 12345tensor([5.])is_leaf: True True False False Falsegradient: tensor([5.]) tensor([2.]) tensor([2.]) None None 对于节点，还可以看这些节点进行的运算。grad_fn，gradient function的缩写，表示这个节点的tensor是什么运算产生的。加一句： 1print(&quot;gradient function:\\n&quot;, w.grad_fn, '\\n', x.grad_fn, '\\n', a.grad_fn, '\\n', b.grad_fn, '\\n', y.grad_fn) 会输出 123456gradient function: None None &lt;AddBackward0 object at 0x000001B1DA3651C0&gt; &lt;AddBackward0 object at 0x000001B1DA3651F0&gt; &lt;MulBackward0 object at 0x000001B1DA3515B0&gt; retain_graph12345678910111213import torchw = torch.tensor([1.], requires_grad=True)x = torch.tensor([2.], requires_grad=True)a = torch.add(w, x)a.retain_grad()b = torch.add(w, 1)y = torch.mul(a, b)# 调用backward()方法，开始反向求梯度y.backward()y.backward() 连续两次调用backward()方法，会报这样的错误： RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward. 原因是我们进行第一次backward()后，计算图就被自动释放掉了，进行第二次backward()时，没有计算图可以计算梯度，于是报错。 解决方案：backward内部添加一个参数：retain_graph=True，意思是计算完梯度后保留计算图。 123# 调用backward()方法，开始反向求梯度y.backward(retain_graph=True)y.backward() 这样就不会报错了。 gradient当计算图末部的节点有1个以上时，有时我们会希望他们之间的梯度有一个权重关系。这时就会用上gradient。 1234567891011121314151617181920import torchw = torch.tensor([1.], requires_grad=True)x = torch.tensor([2.], requires_grad=True)a = torch.add(w, x)b = torch.add(w, 1)# 不难看出，y0和y1是两个互不干扰的末部节点y0 = torch.mul(a, b)y1 = torch.add(a, b)# 将两个末部节点打包起来loss = torch.cat([y0, y1], dim=0)grad_tensors = torch.tensor([1., 2.])# 将grad_tensors中的内容作为权重，变成y0+2y1loss.backward(gradient=grad_tensors)print(w.grad) 输出 1tensor([9.]) 如果把grad_tensors改成： 1grad_tensors = torch.tensor([1., 3.]) 输出变成： 1tensor([11.]) torch.autograd.grad()除了加减乘除法，我们还可以对torch进行求导操作。求的是$\\frac{d(outputs)}{d(inputs)}$。 12345torch.autograd.grad(outputs, inputs, grad_outputs=None, retain_graph=None, create_graph=False) outputs和inputs已在上述定义中给出；grad_outputs：多梯度权重；retain_graph：保留计算图；create_graph：创建计算图。 12345678910111213import torch# y = x ** 2x = torch.tensor([3.], requires_grad=True)y = torch.pow(x, 2)# grad_1 = dy / dx = 2x = 6grad_1 = torch.autograd.grad(y, x, create_graph=True)print(grad_1)# grad_2 = d(dy / dx) / dx = 2grad_2 = torch.autograd.grad(grad_1, x)print(grad_2) 输出 12(tensor([6.], grad_fn=&lt;MulBackward0&gt;),)(tensor([2.]),) autograd注意事项1.梯度不会自动清零123456789101112import torchw = torch.tensor([1.], requires_grad=True)x = torch.tensor([2.], requires_grad=True)for i in range(4): a = torch.add(w, x) b = torch.mul(w, x) y = torch.mul(a, b) y.backward() print(&quot;w's grad: &quot;, w.grad) # w.grad.zero_() 输出： 1234w's grad: tensor([8.])w's grad: tensor([16.])w's grad: tensor([24.])w's grad: tensor([32.]) 由此可以看出，在不加上注释掉的那一行时，梯度在w处是不断累积的。而如果我们把print后面的那句w.grad.zero_()加上，输出就会变成： 1234w's grad: tensor([8.])w's grad: tensor([8.])w's grad: tensor([8.])w's grad: tensor([8.]) w.grad.zero_()的意思就是把w处积累的梯度清零。 2.依赖于叶子节点的节点，requires_grad默认为True可以从上面的代码中发现，我们只有在定义w和x两个tensor时，设置requires_grad为True。这个参数在定义tensor时默认为False。后面我们的a、b、y都没有设置这个参数。 如果我们定义w和x的时候不加上requires_grad=True，那么y.backward()这一步就会报错，因为我们的预设，这两个tensor不需要梯度，于是就无法求梯度。而w和x是我们计算图上的叶子节点，所以必须加上requires_grad=True。 而后面通过w和x延伸定义出的a、b、y，由于依赖的w、x的requires_grad是True，那么a、b、y的这个参数也被默认设置为了True，不需要我们手动添加。 3.叶子节点不可执行in-place操作计算图上叶子节点处的tensor不能进行原地修改。 什么是in-place操作？123t = torch.tensor([1., 2.])t.add_(3.)print(t) 输出 1tensor([4., 5.]) torch.Tensor.add_就是torch.add的in-place版本。所谓in-place，就是在tensor上进行原地修改。大部分的torch.tensor的运算，名字后面加一个下划线，就变成inplace操作了。 再比如求绝对值： 123t = torch.tensor([-1., -2.])t.abs_()print(t) 输出 1tensor([1., 2.]) 知道什么是in-place操作后，我们尝试一下在requires_grad=True的叶子节点上原地修改，代码如下： 123456789101112import torchw = torch.tensor([1.], requires_grad=True)x = torch.tensor([2.], requires_grad=True)a = torch.add(w, x)b = torch.mul(w, x)y = torch.mul(a, b)w.add_(1)y.backward() 报错信息： RuntimeError: a leaf Variable that requires grad is being used in an in-place operation.","link":"/2023/10/03/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/%E3%80%90Pytorch%E7%AC%94%E8%AE%B0%E3%80%914.%E6%A2%AF%E5%BA%A6%E8%AE%A1%E7%AE%97/"},{"title":"DataLoader、Dataset、自定义Dataset","text":"本文转载自【Pytorch笔记】5.DataLoader、Dataset、自定义Dataset 参考视频：深度之眼官方账号 - 02-01 Dataloader与Dataset.mp4 torch.utils.data.DataLoader功能：构建可迭代的数据装载器。 123456789101112data.DataLoader(dataset, batch_size=1, shuffle=False, sampler=None, batch_sampler=None, num_workers=0, collate_fn=None, pin_memory=False, drop_last=False, timeout=0, worker_init_fn=None, multiprocessing_context=None) dataset：Dataset类，决定数据从哪里读取；batch_size：批大小；shuffle：每个epoch是否乱序；sampler：定义从数据集中抽取数据的策略，指明sampler时不能指明shuffle。batch_sampler：和sampler相似，但是一次返回一个批大小的下标。和batch_size、shuffle 、sampler、drop_last互斥；num_workers：使用该DataLoader加载数据的子进程的数量。如果该值为0，意味着只有主进程使用该DataLoader加载数据。collate_fn：合并样本列表，形成一个小型tensor批次。在使用map-style dataset批量加载时使用。pin_memory：True时，DataLoader在返回tensor数据时会先将这些数据复制到device/CUDA的pinned memory中。drop_last：当样本数不能被batch_size整除时，是否舍弃最后一批数据。timeout：如果是正数，则表示获取批次的超时值。要求设置为非负数。worker_init_fn：如果不是None，那么这个函数会调用所有带有worker_id的worker的子进程。 Epoch：所有训练样本都已输入到模型中，称为一个Epoch。Iteration：一批样本输入到模型中，称为一个Iteration。BatchSize：批大小，决定一个Epoch有多少个Iteration。 torch.utils.data.Dataset功能：Dataset抽象类，所有自定义的Dataset需要继承他，并且复写__getitem__()。 __getitem__()：接收一个索引，返回一个样本。 torch.utils.data.TensorDataset由tensor封装的数据集，传入一些在第1维长度相同的tensor，形成dataset。 DataLoader如何使用？123456789101112131415161718192021222324import torchfrom torch.utils import datadef data_gen(num_examples): X = torch.normal(0, 1, (num_examples, 2)) y = torch.normal(0, 1, (num_examples, 1)) return X, ydef load_array(data_arrays, batch_size, need_shuffle=True): dataset = data.TensorDataset(*data_arrays) return data.DataLoader(dataset, batch_size, shuffle=need_shuffle)data_A, data_B = data_gen(30)# 定义一个数据迭代器data_iterdata_iter = load_array((data_A, data_B), 10, False)# 使用for循环，iter每次会指向下一组数据# 每组数据返回我们封装进去的那些tensor，封装几个就返回几个for X, y in iter(data_iter): print(&quot;X\\n&quot;, X) print(&quot;y\\n&quot;, y) 输出： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566X tensor([[-0.2338, -0.4072], [ 1.6080, -0.0880], [-1.0243, -0.6245], [ 0.6012, 1.9620], [ 1.1876, 0.9539], [-0.5972, 0.9251], [-0.4918, -0.1340], [ 2.3297, 0.1833], [-0.8487, 0.3370], [-0.0600, 2.3769]])y tensor([[ 1.0454], [ 0.5992], [ 2.0075], [ 0.2727], [-1.6845], [ 0.0845], [ 1.0992], [ 0.5103], [-0.6727], [-0.1900]])X tensor([[-0.1419, -1.5535], [-1.6436, 0.8680], [-0.4432, -0.7703], [ 0.3822, 0.4675], [ 0.4000, 1.3471], [ 0.9776, 2.0103], [ 0.1298, 2.7382], [ 0.2664, -0.6223], [-1.0774, 0.0734], [-0.1904, -1.3299]])y tensor([[-0.5979], [-0.5432], [ 0.2951], [ 0.2811], [-0.5997], [ 0.8073], [ 1.4356], [ 1.1555], [-0.3368], [-0.0626]])X tensor([[-1.4326, -0.3407], [-1.1878, -1.5619], [ 0.3498, 1.5307], [-0.8174, 0.6017], [ 0.8076, 0.8295], [ 2.6239, 1.1669], [-1.2598, 1.4309], [ 0.3365, 0.1765], [-0.4472, -0.6882], [ 0.6732, -0.0742]])y tensor([[ 0.5114], [ 1.0669], [-1.5565], [ 0.4512], [ 3.2071], [ 0.4752], [-1.5981], [ 0.0035], [-0.2723], [-1.3634]]) 自定义Dataset12345678910111213141516171819202122232425262728293031323334353637383940414243import torchfrom torch.utils import data# 自定义的Dataset# 每一条数据包含X的一行数据(1x2的tensor)和y的一行数据(1x1的tensor)class MyTensorDataset(data.Dataset): def __init__(self, tensor_list): # 初始化超类 (data.Dataset) super(MyTensorDataset, self).__init__() self.data = tensor_list def __getitem__(self, index): # 这个函数必须重写 # 返回dataset中下标编号为index的数据 sample = {'X': self.data[0][index], 'y': self.data[1][index]} return sample def __len__(self): # 返回这个dataset中的数据个数 return len(self.data[0])# 随机生成数据def data_gen(num_examples): X = torch.normal(0, 1, (num_examples, 2)) y = torch.normal(0, 1, (num_examples, 1)) return X, y# 使用自己的Datasetdef load_array(data_arrays, batch_size, need_shuffle=True): X, y = data_arrays dataset = MyTensorDataset((X, y)) return data.DataLoader(dataset, batch_size, shuffle=need_shuffle)data_A, data_B = data_gen(30)data_iter = load_array((data_A, data_B), 10, False)for sample in iter(data_iter): print(&quot;sample:\\n&quot;, sample) 输出： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960sample: {'X': tensor([[ 0.2149, 0.6216], [ 0.4691, 0.1862], [-1.7705, -1.5983], [-0.0196, -0.5903], [-0.1313, 0.3206], [ 0.1898, -0.2575], [ 1.5934, -0.4720], [-0.8343, -0.2181], [ 1.6159, -0.5473], [-1.2662, -0.0218]]), 'y': tensor([[ 0.8886], [ 0.1653], [ 0.8054], [-0.0725], [-0.4806], [-0.4661], [-0.4040], [ 0.6192], [-0.2522], [ 1.4091]])}sample: {'X': tensor([[ 0.6441, -0.5759], [-0.7285, -1.0021], [ 0.1250, -0.2333], [ 0.3196, 0.7762], [ 0.1429, 0.4667], [ 1.0751, -0.4867], [ 0.1664, 0.3489], [ 0.1616, -0.1998], [ 0.6707, -0.4678], [-1.7778, -2.4658]]), 'y': tensor([[-0.2342], [ 0.1402], [ 0.5768], [ 1.7898], [-0.6802], [-2.3584], [ 0.7048], [ 0.1848], [ 0.1225], [ 0.5535]])}sample: {'X': tensor([[ 0.1694, 0.2863], [ 0.3062, -0.7494], [ 0.6844, 1.9278], [ 0.9141, 0.3842], [-1.2314, -1.4933], [ 0.1568, -1.4182], [ 1.7723, -0.4890], [ 0.5734, 1.0614], [ 0.9536, -0.2866], [ 0.2510, 0.3375]]), 'y': tensor([[ 0.9802], [-0.5557], [ 0.7763], [ 1.1688], [-1.0067], [-0.4044], [-0.2745], [-0.3661], [-0.6058], [ 0.6905]])}","link":"/2023/10/04/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/%E3%80%90Pytorch%E7%AC%94%E8%AE%B0%E3%80%915.DataLoader%E3%80%81Dataset%E3%80%81%E8%87%AA%E5%AE%9A%E4%B9%89Dataset/"},{"title":"Transforms","text":"本文转载自【Pytorch笔记】6.Transforms pytorch官方文档 - transforms transforms需要使用计算机视觉工具包：torchvision。torchvision.transforms：常用的图像预处理方法；torchvision.datasets：常用数据集的dataset实现，如MNIST、CIFAR-10、ImageNet等；torchvision.model：常用的模型预训练，如AlexNet、VGG、ResNet、GoogleNet等。 torchvision.transforms常用的图像预处理方法，包括数据中心化、数据标准化、缩放、裁剪、旋转、翻转、填充、噪声添加、灰度变换、线性变换、仿射变换、亮度变换、饱和度变换、对比度变换等。 Geometry - 几何变换torchvision.transforms.Resize()功能：将给定图像缩放成指定的尺寸。 1trans = transforms.Resize((32, 32)) 意思就是把样本图像缩放成32x32的。如果里面的参数给的不是上面这样的(H, W)的格式，而是一个数的话，那么令原图像的较短边与该数匹配，按比例缩放。 torchvision.transforms.RandomCrop()功能：将给定图像按照指定的尺寸随机裁剪。 1trans = transforms.RandomCrop(size=10, padding=4) size指裁剪大小。如果size=10，就是裁出10x10的图片；如果size=(15, 10)，就是裁出15x10的图片。padding指填充大小。如果padding=4，就是上下左右填充都是4；如果padding=(4,2)，就是左右填充为4，上下填充为2；如果padding=(4,3,2,1)，那么左填充为4，上填充为3，右填充为2，下填充为1。 torchvision.transforms.CenterCrop()功能：将给定图像按照指定的尺寸在中心裁剪。 1trans = transforms.CenterCrop(size=4) size指裁剪大小。如果size=4，就是裁出4x4的图片；如果size=(4, 3)，就是裁出4x3的图片。 torchvision.transforms.FiveCrop()功能：将给定图像按照指定的尺寸在中心、四个角裁剪，返回五个裁剪出来的图。 1trans = transforms.FiveCrop(size=4) size指裁剪大小。如果size=4，就是裁出4x4的图片；如果size=(4, 3)，就是裁出4x3的图片。 torchvision.transforms.TenCrop()功能：将给定图像按照指定的尺寸在中心、四个角裁剪，并分别进行翻转，返回十个裁剪/翻转出来的图。 1trans = transforms.TenCrop(size=4, vertical_flip=False) size指裁剪大小。如果size=4，就是裁出4x4的图片；如果size=(4, 3)，就是裁出4x3的图片。vertical_flip为True，则翻转采用垂直翻转；False，则翻转采用水平翻转。 torchvision.transforms.Pad()功能：将给定图像按照给定值进行边界填充。 1trans = transforms.Pad(padding=4, fill=0, padding_mode='constant') padding指填充大小。如果padding=4，就是上下左右填充都是4；如果padding=(4,2)，就是左右填充为4，上下填充为2；如果padding=(4,3,2,1)，那么左填充为4，上填充为3，右填充为2，下填充为1。fill指填充内容，该参数当且仅当后面的padding_mode为'constant'时有效，默认值为0。如果是一个长度为3的tuple，则分别用于填充图像的R,G,B层。用于tensor时仅支持一个数，用于PIL图像时仅支持一个数或一个长度为3的tuple。padding_mode指填充方式，有以下取值：'constant'：用一个常值填充，填充的内容用fill参数传入。'edge'：边缘填充，使用图像最靠边界的元素的数值进行填充。'reflect'：镜像填充，以最外侧的元素为对称轴，将图像的内容对称填充进去。如原图像为[1,2,3,4]、padding=2时，填充后变成[3,2,1,2,3,4,3,2]。'symmetric'：对称填充，以边界为对称轴，将图像的内容对称填充进去。如原图像为[1,2,3,4]、padding=2时，填充后变成[2,1,1,2,3,4,4,3]。 torchvision.transforms.RandomRotation()功能：将给定图像按照指定角度范围进行随机旋转。 12345trans = transforms.RandomRotation(degrees=(40, 90), interpolation=InterpolationMode.NEAREST, expand=False, center=None, fill=0) degrees指角度范围，如果degree=40，那么旋转角度的范围为(-40,40)；interpolation指插值方式，有两种取值：InterpolationMode.NEAREST：就近插值、InterpolationMode.BILINEAR：双线性插值；expand指是否扩充输出图像的尺寸，如果为True，则输出图像会根据旋转后的结果使尺寸变大，如果为False，则输出图像的形状不变。center指旋转中心，默认为None，以图片左上角为旋转中心。fill指旋转后空白的地方的填充内容。 torchvision.transforms.RandomPerspective()功能：将给定的图像按照给定参数进行透视变换（透视变换效果见下图） 1234trans = transforms.RandomPerspective(distortion_scale=0.5, p=0.5, interpolation=InterpolationMode.BILINEAR, fill=0) distoration_scale指扭曲程度，范围是[0,1]，默认为0.5；p指被变换概率，默认为0.5；interpolation指插值方式，有两种取值：InterpolationMode.NEAREST：就近插值、InterpolationMode.BILINEAR：双线性插值；fill指旋转后空白的地方的填充内容。 torchvision.transforms.RandomHorizontalFlip()功能：以指定概率水平翻转图像。 1trans = transforms.RandomHorizontalFlip(p=0.5) p指被变换概率，默认为0.5； torchvision.transforms.RandomVerticalFlip()功能：以指定概率垂直翻转图像。 1trans = transforms.RandomVerticalFlip(p=0.5) p指被变换概率，默认为0.5； Color - 色彩变换torchvision.transforms.ColorJitter()功能：随机改变图像的亮度、对比度、饱和度和色调。 1trans = transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5) brightness指亮度：如果是一个float，则亮度从[max(0, 1-brightness), 1+brightness]中均匀抽取，如果是一个长度为2的tuple：float(min, max)，则亮度从[min, max]中均匀抽取；contrast指对比度：如果是一个float，则对比度从[max(0, 1-contrast), 1+contrast]中均匀抽取，如果是一个长度为2的tuple：float(min, max)，则对比度从[min, max]中均匀抽取；saturation指饱和度：如果是一个float，则饱和度从[max(0, 1-saturation), 1+saturation]中均匀抽取，如果是一个长度为2的tuple：float(min, max)，则饱和度从[min, max]中均匀抽取；hue指色调：如果是一个float，则色调从[-hue, hue]中均匀抽取，其中$0\\leq hue \\leq 0.5$，如果是一个长度为2的tuple：float(min, max)，则色调从[min, max]中均匀抽取，其中$-0.5\\leq min\\leq max\\leq 0.5$； torchvision.transforms.Grayscale()功能：将图像转为灰度图像。 1trans = transforms.Grayscale(num_output_channels=1) num_output_channels指输出图像的通道数，取值只能是1或3。如果为1：返回的图像只有一个通道；如果为3：返回的图像有3个通道，且r=g=b。 torchvision.transforms.RandomGrayscale()功能：以指定概率将图像转为灰度图像。 1trans = transforms.RandomGrayscale(p=0.5) p为转变图像的概率。 torchvision.transforms.GaussianBlur()功能：使用随机的高斯模糊对图像进行模糊处理。 1trans = transforms.GaussianBlur(kernel_size=2, sigma=(0.1, 2.0)) kernel_size指高斯核的大小；sigma指用于创建内核进行模糊处理的标准偏差，如果是一个float则sigma固定；如果是长度为2的float元组，令其为float(min, max)，并在[min, max]范围内均匀随机选取sigma。 torchvision.transforms.RandomInvert()功能：以指定概率替换图片的颜色。 1trans = transforms.RandomInvert(p=0.5) p为改变颜色的概率。 transforms.RandomPosterize()功能：以指定概率将图片海报化（减少通道中的比特数）。 1trans = transforms.RandomPosterize(bits=4, p=0.5) bits指每个通道保持的比特数，取值为0-8；p为转化图像的概率。 transforms.RandomSolarize()功能：以指定概率将图片高曝（Solarize该怎么翻译呢？）。 1trans = transforms.RandomSolarize(threshold=128, p=0.5) threshold指高曝的阈值。所谓高曝，就是将像素值在[0, threshold]映射到[threshold, 255]上。p为转化图像的概率。 Composition - 变换组合torchvision.transforms.Compose()功能：将多个transforms变换封装成一个组合。 12345transforms.Compose([ transforms.CenterCrop(10), transforms.PILToTensor(), transforms.ConvertImageDtype(torch.float),]) 参数就是包含很多transforms变换的list。 torchvision.transforms.RandomApply()功能：以给定概率将transforms列表中的变换打乱。 12345transforms.RandonApply([ transforms.CenterCrop(10), transforms.PILToTensor(), transforms.ConvertImageDtype(torch.float),], p=3) 第一项可以是transforms的sequence，也可以是torch.nn.ModuleList；第二项p是打乱的概率。 Miscellaneous - 杂项torchvision.transforms.Normalize()功能：对得到的tensor进行减均值除标准差处理。 1trans = transforms.Normalize(mean=0, std=1, inplace=True) mean指均值；std指标准差；inplace指是否替换原tensor，True为替换，False为不替换。 torchvision.transforms.RandomErasing()功能：对得到的图像，以一定概率随机一个矩形区域进行擦除。 12345trans = transforms.RandomErasing(p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False) p指给定的概率；scale指擦除区域面积占原图面积的比例范围；ratio指擦除区域长宽比的范围；value指擦除后填充的值，如果是一个int，那么全部用这个int来填充；如果是长度为3的int，则分别用于填充R、G、B三个通道；如果value='random'，则随机填充。inplace指是否替换原tensor，True为替换，False为不替换。 Conversion - 格式转换torchvision.transforms.ToTensor()功能：将PIL图像或ndarray转换成tensor并将值映射到[0,1]之间。更具体地，将一个PIL图像或ndarray（形状是$H\\times W\\times C$，元素范围在[0,255])转化成tensor（形状是$C\\times H\\times W$，元素范围在[0,1]）。前提是PIL图像属于L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK, 1中的一种模式、ndarray的dtype=uint8。其他情况下返回的tensor不会被映射到[0,1]之间。 1trans = transforms.ToTensor() torchvision.transforms.PILToTensor()功能：将$H\\times W\\times C$的PIL图像转换为$C\\times H\\times W$的tensor。 1trans = transforms.PILToTensor()","link":"/2023/10/06/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/%E3%80%90Pytorch%E7%AC%94%E8%AE%B0%E3%80%916.Transforms/"},{"title":"torch.nn (Convolution Layers)","text":"本文转载自【Pytorch笔记】7.torch.nn (Convolution Layers) 我们常用torch.nn来封装网络，torch.nn为我们封装好了很多神经网络中不同的层，如卷积层、池化层、归一化层等。我们会把这些层像是串成一个牛肉串一样串起来，形成网络。 先从最简单的，都有哪些层开始学起。 Convolution Layers - 卷积层torch.nn.Conv1d()1维卷积层。 1234567891011torch.nn.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None) in_channels：输入tensor的通道数；out_channels：输出tensor的通道数；kernel_size：卷积核的大小；stride：步长；padding：输入tensor的边界填充尺寸；dilation：卷积核之间的间距（下面这个图为dilation=2），默认为1； groups：从输入通道到输出通道的阻塞连接数。in_channel和out_channel需要能被groups整除。更具体地：groups=1时所有输入均与所有输出进行卷积，groups=2时该操作相当于并排设置两个卷积层，每卷积层看到一半的输入通道，产生一半的输出通道，然后将两个卷积层连接起来。groups=in_channel时输入的每个通道都和相应的卷积核进行卷积；bias：是否添加可学习的偏差值，True为添加，False为不添加。padding_mode：填充模式，有以下取值：zeros（这个是默认值）、reflect、replicate、circular。 1234567891011import torchimport torch.nn as nnm = nn.Conv1d(in_channels=16, out_channels=33, kernel_size=3, stride=2)# input: 批大小为20，每个数据通道为16，size=50input = torch.randn(20, 16, 50)output = m(input)print(output.size()) 输出 12# output: 批大小为20，每个数据通道为33，size=24torch.Size([20, 33, 24]) torch.nn.Conv2d()2维卷积层。 1234567891011torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None) 参数与Conv1d()基本一样，不再赘述。 12345678910import torchimport torch.nn as nnm = nn.Conv2d(in_channels=2, out_channels=3, kernel_size=3, stride=2)input = torch.randn(20, 2, 5, 6)output = m(input)print(output.size()) 输出 1torch.Size([20, 3, 2, 2]) torch.nn.Conv3d()3维卷积层。 1234567891011torch.nn.Conv3d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None) 参数与Conv1d()基本一样，不再赘述。 12345678910import torchimport torch.nn as nnm = nn.Conv3d(in_channels=2, out_channels=3, kernel_size=3, stride=2)input = torch.randn(20, 2, 4, 5, 6)output = m(input)print(output.size()) 输出 1torch.Size([20, 3, 1, 2, 2]) torch.nn.ConvTranspose1d()1维转置卷积层。 123456789101112torch.nn.ConvTranspose1d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1, padding_mode='zeros', device=None, dtype=None) 参数与Conv1d()基本一样，不再赘述。唯一不同的是output_padding，与padding不同的是，output_padding是输出tensor的每一个边，外面填充的层数。（padding是输入tensor的每个边填充的层数） 12345678910import torchimport torch.nn as nnm = nn.ConvTranspose1d(in_channels=2, out_channels=3, kernel_size=3, stride=1)input = torch.randn(20, 2, 2)output = m(input)print(output.size()) 输出 1torch.Size([20, 3, 4]) torch.nn.ConvTranspose2d()2维转置卷积层。 123456789101112torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1, padding_mode='zeros', device=None, dtype=None) 参数与Conv1d()基本一样，不再赘述。 12345678910import torchimport torch.nn as nnm = nn.ConvTranspose2d(in_channels=2, out_channels=3, kernel_size=3, stride=1)input = torch.randn(20, 2, 2, 2)output = m(input)print(output.size()) 输出 1torch.Size([20, 3, 4, 4]) torch.nn.ConvTranspose3d()3维转置卷积层。 123456789101112torch.nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1, padding_mode='zeros', device=None, dtype=None) 参数与Conv1d()基本一样，不再赘述。 12345678910import torchimport torch.nn as nnm = nn.ConvTranspose3d(in_channels=2, out_channels=3, kernel_size=3, stride=1)input = torch.randn(20, 2, 2, 2, 2)output = m(input)print(output.size()) 输出 1torch.Size([20, 3, 4, 4, 4]) torch.nn.LazyConv1d()1维延迟初始化卷积层，当in_channel不确定时可使用这个层。关于延迟初始化，大家可以参考这篇文章，我认为讲的很好：俱往矣… - 延迟初始化——【torch学习笔记】 12345678910torch.nn.LazyConv1d(out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None) LazyConv1d没有in_channel参数。这不代表这个层没有输入的通道，而是在调用时自动适配，并进行初始化。引用文章中的一段代码，改成LazyConv1d，讲述使用方法。 12345678910111213141516import torchimport torch.nn as nnnet = nn.Sequential( nn.LazyConv1d(256, 2), nn.ReLU(), nn.Linear(9, 10))print(net)[net[i].state_dict() for i in range(len(net))]low = torch.finfo(torch.float32).min / 10high = torch.finfo(torch.float32).max / 10X = torch.zeros([2, 20, 10], dtype=torch.float32).uniform_(low, high)net(X)print(net) 输出 12345678910Sequential( (0): LazyConv1d(0, 256, kernel_size=(2,), stride=(1,)) (1): ReLU() (2): Linear(in_features=9, out_features=10, bias=True))Sequential( (0): Conv1d(20, 256, kernel_size=(2,), stride=(1,)) (1): ReLU() (2): Linear(in_features=9, out_features=10, bias=True)) 可以看出，未进行初始化时，in_features=0。只有传入参数使用网络后才会根据输入进行初始化。 torch.nn.LazyConv2d()2维延迟初始化卷积层。 12345678910torch.nn.LazyConv2d(out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None) torch.nn.LazyConv3d()3维延迟初始化卷积层。 12345678910torch.nn.LazyConv3d(out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None) torch.nn.LazyConvTranspose1d()1维延迟初始化转置卷积层。 1234567891011torch.nn.LazyConvTranspose1d(out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1, padding_mode='zeros', device=None, dtype=None) torch.nn.LazyConvTranspose2d()2维延迟初始化转置卷积层。 1234567891011torch.nn.LazyConvTranspose2d(out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1, padding_mode='zeros', device=None, dtype=None) torch.nn.LazyConvTranspose3d()3维延迟初始化转置卷积层。 1234567891011torch.nn.LazyConvTranspose3d(out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1, padding_mode='zeros', device=None, dtype=None) torch.nn.Unfold()从一个批次的输入张量中提取出滑动的局部区域块。 1234torch.nn.Unfold(kernel_size, dilation=1, padding=0, stride=1) kernel_size：滑动块的大小；dilation：卷积核之间的间距(torch.nn.Conv1d中有图示)；padding：输入tensor的边界填充尺寸；stride：滑块滑动的步长。 这里的输入必须是4维的tensor，否则会报这样的错误： 1NotImplementedError: Input Error: Only 4D input Tensors are supported (got 2D) 示例 123456789101112import torchfrom torch import nnt = torch.tensor([[[[1., 2., 3., 4.], [5., 6., 7., 8.], [9., 10., 11., 12.], [13., 14., 15., 16.],]]])unfold = nn.Unfold(kernel_size=(2, 2), dilation=1, padding=0, stride=1)output = unfold(t)print(output) 输出 1234tensor([[[ 1., 2., 3., 5., 6., 7., 9., 10., 11.], [ 2., 3., 4., 6., 7., 8., 10., 11., 12.], [ 5., 6., 7., 9., 10., 11., 13., 14., 15.], [ 6., 7., 8., 10., 11., 12., 14., 15., 16.]]]) torch.nn.Fold()Unfold()的逆操作。当Unfold()时出现滑块有重复覆盖时会导致结果和原来不一样。因为Fold()的过程中对于同一个位置的元素进行加法处理。 12345torch.nn.Fold(output_size, kernel_size, dilation=1, padding=0, stride=1) 下面是Unfold()和Fold()结合的代码，Unfold()部分和上面代码相同。 1234567891011121314import torchfrom torch import nnt = torch.tensor([[[[1., 2., 3., 4.], [5., 6., 7., 8.], [9., 10., 11., 12.], [13., 14., 15., 16.]]]])unfold = nn.Unfold(kernel_size=(2, 2), dilation=1, padding=0, stride=1)output = unfold(t)print(output)fold = nn.Fold(output_size=(4, 4), kernel_size=(2, 2))out = fold(output)print(out) 输出 12345678tensor([[[ 1., 2., 3., 5., 6., 7., 9., 10., 11.], [ 2., 3., 4., 6., 7., 8., 10., 11., 12.], [ 5., 6., 7., 9., 10., 11., 13., 14., 15.], [ 6., 7., 8., 10., 11., 12., 14., 15., 16.]]])tensor([[[[ 1., 4., 6., 4.], [10., 24., 28., 16.], [18., 40., 44., 24.], [13., 28., 30., 16.]]]])","link":"/2023/11/15/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/%E3%80%90Pytorch%E7%AC%94%E8%AE%B0%E3%80%917.torch.nn%20(Convolution%20Layers)/"}],"tags":[{"name":"工程项目","slug":"工程项目","link":"/tags/%E5%B7%A5%E7%A8%8B%E9%A1%B9%E7%9B%AE/"},{"name":"后端开发","slug":"后端开发","link":"/tags/%E5%90%8E%E7%AB%AF%E5%BC%80%E5%8F%91/"},{"name":"嵌入式","slug":"嵌入式","link":"/tags/%E5%B5%8C%E5%85%A5%E5%BC%8F/"},{"name":"科研项目","slug":"科研项目","link":"/tags/%E7%A7%91%E7%A0%94%E9%A1%B9%E7%9B%AE/"},{"name":"计算机视觉","slug":"计算机视觉","link":"/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"},{"name":"互联网+","slug":"互联网","link":"/tags/%E4%BA%92%E8%81%94%E7%BD%91/"},{"name":"声学","slug":"声学","link":"/tags/%E5%A3%B0%E5%AD%A6/"},{"name":"计算机设计大赛","slug":"计算机设计大赛","link":"/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%AE%BE%E8%AE%A1%E5%A4%A7%E8%B5%9B/"},{"name":"人工智能创意赛","slug":"人工智能创意赛","link":"/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%88%9B%E6%84%8F%E8%B5%9B/"},{"name":"NLP","slug":"NLP","link":"/tags/NLP/"},{"name":"信息检索","slug":"信息检索","link":"/tags/%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2/"},{"name":"数学","slug":"数学","link":"/tags/%E6%95%B0%E5%AD%A6/"},{"name":"机器学习","slug":"机器学习","link":"/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"线性代数","slug":"线性代数","link":"/tags/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/"},{"name":"算法","slug":"算法","link":"/tags/%E7%AE%97%E6%B3%95/"},{"name":"哲学","slug":"哲学","link":"/tags/%E5%93%B2%E5%AD%A6/"},{"name":"概率论","slug":"概率论","link":"/tags/%E6%A6%82%E7%8E%87%E8%AE%BA/"},{"name":"Linux","slug":"Linux","link":"/tags/Linux/"},{"name":"C&#x2F;C++","slug":"C-C","link":"/tags/C-C/"},{"name":"PyTorch","slug":"PyTorch","link":"/tags/PyTorch/"}],"categories":[{"name":"工程项目","slug":"工程项目","link":"/categories/%E5%B7%A5%E7%A8%8B%E9%A1%B9%E7%9B%AE/"},{"name":"科研项目","slug":"科研项目","link":"/categories/%E7%A7%91%E7%A0%94%E9%A1%B9%E7%9B%AE/"},{"name":"论文学习","slug":"论文学习","link":"/categories/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/"},{"name":"技术学习","slug":"技术学习","link":"/categories/%E6%8A%80%E6%9C%AF%E5%AD%A6%E4%B9%A0/"}],"pages":[{"title":"Achievement","text":"荣誉称号 id 日期 获奖名称 获奖等级 颁奖单位 1 2021.06 优秀志愿者 校级 哈尔滨工程大学 2 2023.03 优秀三好学生 校级 哈尔滨工程大学 3 2023.03 优秀共青团员 校级 哈尔滨工程大学 竞赛奖项国家级 id 日期 比赛名称 获奖等级 颁奖单位 指导教师 1 2023.05 美国大学生数学建模竞赛 Honorable Mention COMAP(美国数学及其应用联合会) 刘东平 2 2023.07 中国大学生计算机设计大赛 国家级二等奖 中国大学生计算机设计大赛组织委员会 卢丹、王也 省部级 id 日期 比赛名称 获奖等级 颁奖单位 指导教师 1 2022.09 东北三省数学建模联赛 省级二等奖 东北三省数学建模联赛组委会 高振滨 2 2022.11 全国大学生数学建模联赛 省级一等奖 全国大学生数学建模竞赛黑龙江赛区组委会 王淑娟 3 2023.05 中国大学生计算机设计大赛省赛 省级一等奖 中国大学生计算机设计大赛黑龙江省级赛组织委员会 卢丹、王也 4 2023.05 中国大学生计算机设计大赛省赛 省级三等奖 中国大学生计算机设计大赛黑龙江省级赛组织委员会 张泽宝 5 2023.08 “建行杯”第九届中国国际“互联网+”大学生创新创业大赛省赛 省级银奖 黑龙江省教育厅 李超 6 2023.08 “建行杯”第九届中国国际“互联网+”大学生创新创业大赛省赛 省级铜奖 黑龙江省教育厅 吴艳霞、李超 校级 id 日期 比赛名称 获奖等级 颁奖单位 指导教师 1 2022.10 “五四杯”大学生课外学术科技作品竞赛 校级二等奖 共青团哈尔滨工程大学委员会 李超 2 2022.11 “五四杯”大学生创业大赛 校级金奖 共青团哈尔滨工程大学委员会 杨东梅、徐悦竹 3 2022.11 “五四杯”大学生创业大赛 校级银奖 共青团哈尔滨工程大学委员会 李超、冯晓宁、刘刚 4 2023.06 第一届“贡橙杯”CTF竞赛 校级二等奖 哈尔滨工程大学计算机科学与技术学院 - 5 2023.11 第二届“贡橙杯”CTF竞赛 校级二等奖 哈尔滨工程大学计算机科学与技术学院 - 奖学金 id 日期 奖励名称 获奖等级 颁奖单位 1 2021.05 优秀学生奖学金 校级一等奖 哈尔滨工程大学 2 2021.10 优秀学生奖学金 校级二等奖 哈尔滨工程大学 3 2022.05 优秀学生奖学金 校级一等奖 哈尔滨工程大学 4 2022.11 优秀学生奖学金 校级二等奖 哈尔滨工程大学 5 2023.05 优秀学生奖学金 校级二等奖 哈尔滨工程大学 6 2023.11 优秀学生奖学金 校级二等奖 哈尔滨工程大学","link":"/achievement/index.html"},{"title":"Profile","text":"学术背景 时间 学校 专业 2017.09 - 2020.06 黑龙江省伊春市第一中学 理科(物化生) 2020.09 - 2024.07(在读) 哈尔滨工程大学 计算机科学与技术 2024.09 - 2027.07 东北大学(沈阳, 中国) NLP, Information Retrieval 学习成绩 时间 计算方法 成绩 排名 前六学期 必修课加权成绩(教务办) 89.58 33 / 290 (11.38%) 前六学期 推免综合成绩(学工办) 91.58 22 / 290 (7.59%) 等级认证 考试 成绩 时间 CET-4 554 2020.12 CET-6 459 2021.12 CSP 170 2021.06 科研项目 时间 项目 方向 2022.10 - 2022.12 海洋鹰眼——舰船识别与海况观测系统 计算机视觉 2022.10 - 2022.12 基于计算机视觉的盲人视觉辅助智能眼镜 计算机视觉 2022.12 - 2023.05 基于音频特诊分析的动态鸟类识别分析 声学 2022.09 - 2023.01 基于深度学习的多功能图像处理和社交平台 计算机视觉 2023.05 - 2023.06 基于同元MWorks统一建模语言的在线仿真建模平台 嵌入式 2022.10 - 2023.04 哈尔滨工程大学语料库智能检索系统 后端开发 主修课程 专业课 成绩 选修课 成绩 数学课 成绩 编译系统设计 100 计算机逻辑设计综合实验 98 线性代数与解析几何 97 计算思维 95 计算机图形学 96 工科数学分析 96 计算机组成原理 95 面向对象程序设计 95 复变函数 90 信息管理系统实践 92 软件工程项目实践 95 计算机硬件综合课程设计 92 ACM程序设计 95 数据结构 91 ios程序设计 95 电路基础 91 模型机设计 95 计算机网络 90 数据库原理 94 实习经历 时间 单位 工作 2023.05 - 2023.06 中国联合网络通信公司(哈尔滨) 网络攻击监测 2023.07 - 2023.08 Oracle甲骨文华育兴业(哈尔滨) 后端开发 学生任职 时间 学生组织 工作 2021.10 - 2022.06 计算机科学与技术学院宣传中心 摄影部部员 2021.10 - 2022.06 计算机科学与技术学院团委 宣传部部员","link":"/profile/index.html"},{"title":"Links","text":"同窗好友 Junhao Chen (Tsinghua University, Shenzhen) Xiaojun Ye (Zhejiang University) Zihan Xu (Shanghai Jiao Tong University, Shanghai) Yuhao Xue (Tongji University, Shanghai) Zhicheng Li (University of Science and Technology of China, Suzhou) Fang Wu (Xiamen University, Xiamen) 我的母校 Harbin Engineering University Northeastern University (Shenyang, China) 其他组织 HEU 开放原子开源社团 HEU 开放原子社区论坛 HEU 网络安全社团","link":"/links/index.html"}]}