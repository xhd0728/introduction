---
title: 自回归和非自回归
tag:
  - 机器学习
category:
  - 论文学习
lang: zh-CN
abbrlink: cfd5b824
date: 2023-11-06 00:00:00
---

自回归（Autoregressive，简称AR）和非自回归（Non-Autoregressive，简称NAR）是两种不同的建模方法，常用于序列生成任务，特别是自然语言处理领域。
<!--more-->
1. 自回归模型：
自回归模型是一种基于时间顺序的建模方法，其中当前时间步的输出依赖于之前的输出。在自回归模型中，生成的序列是按照时间顺序逐步生成的，每一步都依赖于之前的生成结果。典型的自回归模型包括基于循环神经网络（RNN）的模型，如循环神经网络语言模型（RNNLM），以及基于Transformer的模型，如GPT（Generative Pre-trained Transformer）。这些模型在生成文本、机器翻译等任务上表现出色，因为它们能够利用上下文信息进行逐步生成，保持一定的连贯性和上下文一致性。

2. 非自回归模型：
非自回归模型是指生成过程中每个时间步之间相互独立，不依赖于之前的生成结果。与自回归模型不同，非自回归模型可以同时并行地生成整个序列，而不需要等待前面的结果。这种并行生成的特点使得非自回归模型在速度上具有优势，能够更快地生成序列。典型的非自回归模型包括基于生成对抗网络（GAN）的模型和基于自注意力机制的模型，如Mask-Predict和MASS（Masked Sequence to Sequence）。然而，非自回归模型往往在生成质量上存在一定的挑战，因为缺乏上下文信息的依赖关系，可能导致生成结果的不连贯性和语义不准确性。

总结而言，自回归模型在生成任务中能够保持上下文的连贯性，但生成速度较慢；非自回归模型可以快速生成序列，但在生成质量上可能存在一定的问题。选择使用哪种模型取决于具体的任务需求和平衡生成质量与生成速度的要求。