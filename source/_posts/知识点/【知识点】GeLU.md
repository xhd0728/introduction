---
title: GeLU
tag:
  - 机器学习
  - 数学
category:
  - 论文学习
lang: zh-CN
abbrlink: '2e561321'
date: 2023-11-20 00:00:00
---

GeLU（Gaussian Error Linear Unit）是一种激活函数，通常用于神经网络的隐藏层。它被设计用来克服一些传统激活函数（如ReLU）的一些限制，尤其是对于包含负值的输入数据。

<!--more-->

GeLU的定义是：

$\text{GeLU}(x) = 0.5x \left(1 + \tanh\left(\sqrt{\frac{2}{\pi}}\left(x + 0.044715x^3\right)\right)\right)$

其中，$\tanh$ 是双曲正切函数。

GeLU的性质包括：

1. **平滑性：** GeLU是平滑的函数，这意味着在整个实数域内都是可导的。这与一些其他激活函数，如ReLU，相比是一个优势，因为可导性对于训练过程中的梯度计算是重要的。

2. **近似线性：** 当 $x$ 的值远离零点时，GeLU的形状接近于线性，这有助于减缓激活函数的饱和效应，使得网络更容易训练。

3. **零均值：** 对于足够大的输入 $x$，GeLU的输出趋近于零均值。这对一些神经网络架构可能是有用的。

GeLU的名字来源于它的形状类似于高斯误差函数（Gaussian Error Function）。GeLU被广泛应用于一些神经网络架构，尤其是在语言模型和自然语言处理任务中，但并非适用于所有情况。在实践中，研究人员会根据任务和模型的特性选择合适的激活函数。