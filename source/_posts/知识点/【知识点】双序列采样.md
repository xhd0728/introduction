---
title: 双序列采样
tag:
  - NLP
  - 信息检索
category:
  - 论文学习
lang: zh-CN
abbrlink: b3b84358
date: 2023-11-20 00:00:00
---

BERT（Bidirectional Encoder Representations from Transformers）模型在预训练时采用了双序列（或双句子）采样的方法，这是为了帮助模型学习句子之间的关系和上下文信息。

<!--more-->

在BERT的预训练过程中，模型的输入是一对句子，通常是一个文本中相邻的两个句子。这一对句子会被连接成一个输入序列，并使用特殊的分隔符标记（[SEP]）进行分隔。在原始BERT的预训练任务中，有一个叫做"Next Sentence Prediction"（NSP）的任务，目标是判断两个句子是否是相邻的。

双序列采样的步骤如下：

1. **选择一篇文档：** 从语料库中选择一篇文档。

2. **选择一个句子：** 随机选择文档中的一个句子作为第一个句子（A）。

3. **选择下一个句子：** 有50%的概率选择与第一个句子相邻的句子作为第二个句子（B），有50%的概率从语料库中随机选择一个不与第一个句子相邻的句子。

4. **构建输入序列：** 将句子A和句子B连接成一个输入序列，并在它们之间插入一个特殊的分隔符标记 [SEP]。

5. **添加分类标签：** 在输入序列的开头添加一个特殊的分类标签（[CLS]），用于预测下游任务的输出。

这样，模型在预训练中不仅需要理解单个句子的上下文信息，还需要理解两个句子之间的关系。这有助于提高模型对文本理解的能力，使其能够更好地捕捉句子之间的语义关系。需要注意的是，一些后续的模型，如RoBERTa，已经移除了NSP任务，而专注于更好地优化单个句子的表示。