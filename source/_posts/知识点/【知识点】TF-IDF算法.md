---
title: TF-IDF
tag:
  - NLP
  - 信息检索
category:
  - 论文学习
lang: zh-CN
abbrlink: d7d7e9a3
date: 2023-11-17 00:00:00
---

$TF$是指归一化后的词频，$IDF$是指逆文档频率。给定一个文档集合$D$，有$d_1,d_2,d_3, \cdots ,d_n \in D$。
<!--more-->
文档集合总共包含$m$个词（**注：一般在计算TF−IDF时会去除如“的”这一类的停用词**），有$w_1,w_2,w_3,\cdots,w_m \in W$。我们现在以计算词$w_i$在文档$d_j$中的$TF-IDF$指为例。$TF$的计算公式为：

$$TF=\frac{freq(i,j)}{max_{len}(j)}$$

在这里$freq(i,j)$为$w_i$在$d_j$中出现的频率，$max_{len}(j)$为$d_j$长度。

$TF$只能是描述词在文档中的频率，但假设现在有个词为”我们“，这个词可能在文档集$D$中每篇文档中都会出现，并且有较高的频率。那么这一类词就不具有很好的区分文档的能力，为了降低这种通用词的作用，引入了$IDF$。

$IDF$的表达式如下：

$$IDF=log(\frac{len(D)}{n(i)})$$
在这里$len(D)$表示文档集合$D$中文档的总数，$n(i)$表示含有$w_i$这个词的文档的数量。

得到$TF$和$IDF$之后，我们将这两个值相乘得到$TF−IDF$的值：

$$TF-IDF=TF \times IDF$$

$TF$可以计算在一篇文档中词出现的频率，而$IDF$可以降低一些通用词的作用。因此对于一篇文档我们可以用文档中每个词的$TF-IDF$组成的向量来表示该文档，再根据余弦相似度这类的方法来计算文档之间的相关性。